{"meta":{"title":"Blog","subtitle":"个人博客,记录成长历程","description":"个人博客,记录成长历程","author":"LRJ","url":"https://lurongjiang.github.io","root":"/"},"pages":[{"title":"categories","date":"2020-02-19T08:11:20.000Z","updated":"2020-02-19T08:12:50.443Z","comments":true,"path":"categories/index.html","permalink":"https://lurongjiang.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-02-19T08:11:40.000Z","updated":"2020-02-19T08:13:13.730Z","comments":true,"path":"tags/index.html","permalink":"https://lurongjiang.github.io/tags/index.html","excerpt":"","text":""},{"title":"about","date":"2020-02-19T08:14:30.000Z","updated":"2020-02-19T10:04:40.479Z","comments":true,"path":"about/index.html","permalink":"https://lurongjiang.github.io/about/index.html","excerpt":"","text":"关于我 姓名：LRJ家乡：贵州现居：北京GitHub: qq11221015@sina.comsina: qq11221015@sina.comQQ: 1817975066"}],"posts":[{"title":"Gradle的安装和使用","slug":"Gradle的安装和使用","date":"2020-01-15T04:45:14.000Z","updated":"2020-01-16T08:14:14.000Z","comments":true,"path":"2020/01/15/Gradle的安装和使用/","link":"","permalink":"https://lurongjiang.github.io/2020/01/15/Gradle%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","excerpt":"和Maven类似的项目依赖管理和构建工具Gradle的简单教程","text":"Gradle的安装和使用 下载 可以去Gradle官网下载最新的稳定版本,目前是6.2,我自己下的4.8.1 https://downloads.gradle-dn.com/distributions/gradle-4.8.1-bin.zip 安装 Gradle的安装和Maven类似,就简单的解压,配置环境变量到Path就ok了 配置完成之后,打开cmd查看一下是否配置好了 看到正确输出了gradle的版本就说明配置好了 IDEA配置Gradle Gradle和Maven这一点不同,Gradle无需再IDEA中进行配置操作,本地仓库地址的配置可以再IDEA中配置 这个我用的是环境变量来配置的,IDEA会自动识别,只需要在环境变量中新建一个GRADLE_USER_HOME变量指向自己的本地仓库地址就可以了 Gradle初体验 新建Gradle工程 选择Gradle和JDK 填写项目的GAV 填写项目的GAV坐标,点Finished Gradle的目录 Gradle的目录结构和Maven类似 src就是source目录 src/main放代码目录 src/main/java 放java代码目录 src/main/resouces放资源文件 src/test是测试目录 src/test/java 放测试的java代码目录 src/test/resouces放测试的资源文件 Groovy编程 打开Groovy Console Tools-&gt;Groovy Console HelloWorld 凡事先HelloWorld一下 Groovy语法 Groovy可以省略最末尾的分号 Groovy可以省略小括号 这两个特性可以看出,Groovy的书写更加自由,随意 定义变量 def groovy会根据数据自动推断类型 定义集合 定义Map 闭包 闭包就是一段代码块,在Gradle中主要是把闭包当参数使用 带参数的闭包 Gradle配置文件 build.gradle 构建项目配置 优先本地加载 在repositories中指定先从本地加载","categories":[{"name":"Gradle","slug":"Gradle","permalink":"https://lurongjiang.github.io/categories/Gradle/"}],"tags":[{"name":"Gradle","slug":"Gradle","permalink":"https://lurongjiang.github.io/tags/Gradle/"}]},{"title":"Spring解析xml成BeanDefinition的过程","slug":"Spring Bean的解析过程","date":"2019-05-05T02:24:42.000Z","updated":"2019-05-19T08:32:14.000Z","comments":true,"path":"2019/05/05/Spring Bean的解析过程/","link":"","permalink":"https://lurongjiang.github.io/2019/05/05/Spring%20Bean%E7%9A%84%E8%A7%A3%E6%9E%90%E8%BF%87%E7%A8%8B/","excerpt":"Spring解析xml成BeanDefinition的过程。","text":"Spring Bean的解析过程 xml文件的读取 从我们的入口开始 123456@Testpublic void testXml() &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\"); Student student = context.getBean(Student.class); System.out.println(student.getUsername());&#125; 先进入ClassPathXmlApplicationContext的构造器 123public ClassPathXmlApplicationContext(String configLocation) throws BeansException &#123; this(new String[] &#123;configLocation&#125;, true, null);&#125; 继续调用另一个构造器 12345678910public ClassPathXmlApplicationContext( String[] configLocations, boolean refresh, @Nullable ApplicationContext parent) throws BeansException &#123; super(parent); //创建解析器，解析configLocations setConfigLocations(configLocations); if (refresh) &#123; refresh(); &#125;&#125; 这个refresh()方法是核心方法,点进去 123456789101112131415161718192021222324252627public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; //为容器初始化做准备，重要程度：0 // Prepare this context for refreshing. prepareRefresh(); /* 重要程度：5 1、创建BeanFactory对象 * 2、xml解析 * 传统标签解析：bean、import等 * 自定义标签解析 如：&lt;context:component-scan base-package=\"com.xiangxue.jack\"/&gt; * 自定义标签解析流程： * a、根据当前解析标签的头信息找到对应的namespaceUri * b、加载spring所以jar中的spring.handlers文件。并建立映射关系 * c、根据namespaceUri从映射关系中找到对应的实现了NamespaceHandler接口的类 * d、调用类的init方法，init方法是注册了各种自定义标签的解析类 * e、根据namespaceUri找到对应的解析类，然后调用paser方法完成标签解析 * * 3、把解析出来的xml标签封装成BeanDefinition对象 * */ // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); .... &#125;&#125; 继续看obtainFreshBeanFactory()方法 1234567891011121314151617181920212223protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; /** * 核心方法，必须读，重要程度：5 * 这里使用了模板设计模式,Spring使用最多的设计模式,父类定义了模板,子类具体实现 * 其实反过来看,AbstractApplicationContext的refresh()同样也是定义了模板,onFresh()方法交给子类去实现 * 例如SpringBoot中嵌入式Tomcat启动就是覆写了onFresh()方法 * @see AbstractApplicationContext#onRefresh() * */ refreshBeanFactory(); return getBeanFactory();&#125; /** * 因为ClassPathXmlApplicationContext是AbstractRefreshableApplicationContext的子类 * 所以跳转到AbstractRefreshableApplicationContext * @see AbstractRefreshableApplicationContext#refreshBeanFactory() * @throws BeansException if initialization of the bean factory failed * @throws IllegalStateException if already initialized and multiple refresh * attempts are not supported * &lt;p&gt; * */protected abstract void refreshBeanFactory() throws BeansException, IllegalStateException; 跳转到AbstractRefreshableApplicationContext的refreshBeanFactory() 12345678910111213141516171819202122232425262728293031323334protected final void refreshBeanFactory() throws BeansException &#123; //如果BeanFactory不为空，则清除BeanFactory和里面的实例 if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; //创建DefaultListableBeanFactory //BeanFactory实例工厂,不管什么实例都可以从BeanFactory获取到 DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); //设置是否可以循环依赖 allowCircularReferences //是否允许使用相同名称重新注册不同的bean实现. customizeBeanFactory(beanFactory); /** * 解析xml，并把xml中的标签封装成BeanDefinition对象 * 因为ClassPathXmlApplication是继承自 AbstractXmlApplicationContext * 所以进入AbstractXmlApplicationContext,又是一个模板 * @see AbstractXmlApplicationContext#loadBeanDefinitions(org.springframework.beans.factory.support.DefaultListableBeanFactory) */ loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(\"I/O error parsing bean definition source for \" + getDisplayName(), ex); &#125;&#125;protected abstract void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException; 继续看AbstractXmlApplicationContext的loadBeanDefinitions(…) xml的解析交给了XmlBeanDefinitionReader来解析 1234567891011protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // Create a new XmlBeanDefinitionReader for the given BeanFactory. //创建xml的解析器，这里是一个委托模式 //xml的解析工作,委托给XmlBeanDefinitionReader来解析 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory);.... //主要看这个方法 重要程度 5 loadBeanDefinitions(beanDefinitionReader);&#125; 继续看loadBeanDefinitions(…) 123456789101112protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; Resource[] configResources = getConfigResources(); if (configResources != null) &#123; reader.loadBeanDefinitions(configResources); &#125; //获取需要加载的xml配置文件 String[] configLocations = getConfigLocations(); if (configLocations != null) &#123; //委托给reader来解析xml reader.loadBeanDefinitions(configLocations); &#125;&#125; 点进去 12345678910111213141516171819202122232425262728293031323334353637@Overridepublic int loadBeanDefinitions(String location) throws BeanDefinitionStoreException &#123; return loadBeanDefinitions(location, null);&#125;public int loadBeanDefinitions(String location, @Nullable Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; ResourceLoader resourceLoader = getResourceLoader(); if (resourceLoader == null) &#123; throw new BeanDefinitionStoreException( \"Cannot load bean definitions from location [\" + location + \"]: no ResourceLoader available\"); &#125; if (resourceLoader instanceof ResourcePatternResolver) &#123; // Resource pattern matching available. try &#123; //把字符串类型的xml文件路径，形如：classpath*:user/**/*-context.xml,转换成Resource对象类型，其实就是用流 //的方式加载配置文件，然后封装成Resource对象，不重要，可以不看 Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); //主要看这个方法 ** 重要程度 5 int count = loadBeanDefinitions(resources); .... return count; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( \"Could not resolve bean definition resource pattern [\" + location + \"]\", ex); &#125; &#125; else &#123; // Can only load single resources by absolute URL. Resource resource = resourceLoader.getResource(location); int count = loadBeanDefinitions(resource);.... return count; &#125;&#125; 把xml读出来之后封装成了Resource对象,开始解析Resource 123456789101112@Overridepublic int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; Assert.notNull(resources, \"Resource array must not be null\"); int count = 0; for (Resource resource : resources) &#123; //模板设计模式，调用到子类中的方法 //又是一个模板,因为委托给了XmlBeanDefinitionReader /**@see org.springframework.beans.factory.xml.XmlBeanDefinitionReader#loadBeanDefinitions(org.springframework.core.io.Resource)*/ count += loadBeanDefinitions(resource); &#125; return count;&#125; xml读出来之后,又把Resource封装成带编码的对象,委托给XmlBeanDefinitionReader进行解析 123456@Overridepublic int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; //EncodedResource带编码的对Resource对象的封装 //把资源流对象又做了编码的封装 return loadBeanDefinitions(new EncodedResource(resource));&#125; 再看如何解析Resource的 1234567891011121314151617181920212223242526272829303132333435363738public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; ... Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) &#123; currentResources = new HashSet&lt;&gt;(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); &#125; if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( \"Detected cyclic loading of \" + encodedResource + \" - check your import definitions!\"); &#125; try &#123; //获取Resource对象中的xml文件流对象 InputStream inputStream = encodedResource.getResource().getInputStream(); try &#123; //InputSource是jdk中的sax xml文件解析对象 InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; //主要看这个方法 ** 重要程度 5 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; finally &#123; inputStream.close(); &#125; &#125; catch (IOException ex) &#123; ... &#125; finally &#123; currentResources.remove(encodedResource); if (currentResources.isEmpty()) &#123; this.resourcesCurrentlyBeingLoaded.remove(); &#125; &#125;&#125; 把InputStream流对象从Resouce中读出来,封装成InputSource对象 1234567891011121314151617protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; //把inputSource 封装成Document文件对象，这是jdk的API Document doc = doLoadDocument(inputSource, resource); //主要看这个方法，根据解析出来的document对象，拿到里面的标签元素封装成BeanDefinition int count = registerBeanDefinitions(doc, resource); if (logger.isDebugEnabled()) &#123; logger.debug(\"Loaded \" + count + \" bean definitions from \" + resource); &#125; return count; &#125; catch (BeanDefinitionStoreException ex) &#123; ....&#125; 把流对象InputSource使用SAX进行解析成Document对象,对Document对象进行解析 123456789public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; //xml解析成Document之后,又将Document委托给BeanDefinitionDocumentReader来解析Document成BeanDefinition //又来一记委托模式，BeanDefinitionDocumentReader委托这个类进行document的解析 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); int countBefore = getRegistry().getBeanDefinitionCount(); //主要看这个方法，createReaderContext(resource) XmlReaderContext上下文，封装了XmlBeanDefinitionReader对象 documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore;&#125; Document委托给BeanDefinitionDocumentReader来解析Document成BeanDefinition 123456789101112131415161718192021222324void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) throws BeanDefinitionStoreException;@Overridepublic void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; this.readerContext = readerContext; //主要看这个方法，把root节点传进去 doRegisterBeanDefinitions(doc.getDocumentElement());&#125;protected void doRegisterBeanDefinitions(Element root) &#123; BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent);... //又是模板,冗余设计,空实现 preProcessXml(root); //主要看这个方法，标签具体解析过程 parseBeanDefinitions(root, this.delegate); postProcessXml(root); this.delegate = parent;&#125; 把Document的根传进去,开始解析Document 12345678910111213141516171819202122232425protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; //获取根节点下的所有子节点 //遍历所有子节点,依次解析 NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; //默认标签解析,import,alias,bean,beans parseDefaultElement(ele, delegate); &#125; else &#123; //自定义标签解析,委托给BeanDefinitionParserDelegate来解析 //context:component-scan等,使用了namespaceUri delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125;&#125; 标签的解析分为默认标签(包括import,alias,bean,beans)和自定义标签(如context:componet-scan,mvc:annotation-drive等,这类带前缀的标签需要namespaceUri来指定实现,使用了SPI思想) 默认标签的解析 先看默认标签的解析 123456789101112131415161718private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; //import标签解析 重要程度 1 ，可看可不看 if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; importBeanDefinitionResource(ele); &#125; //alias标签解析 别名标签 重要程度 1 ，可看可不看 else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; processAliasRegistration(ele); &#125; //bean标签，重要程度 5，必须看 else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; processBeanDefinition(ele, delegate); &#125; else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; // recurse doRegisterBeanDefinitions(ele); &#125;&#125; 核心方法processBeanDefinition(…) 123456789101112131415161718192021protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; //重点看这个方法，重要程度 5 ，解析document，封装成BeanDefinition BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; //该方法功能不重要，设计模式重点看一下，装饰者设计模式，加上SPI设计思想 bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; //完成document到BeanDefinition对象转换后，对BeanDefinition对象进行缓存注册 // Register the final decorated instance. BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(\"Failed to register bean definition with name '\" + bdHolder.getBeanName() + \"'\", ele, ex); &#125; // Send registration event. getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 继续看BeanDefinitionParserDelegate是如何解析的parseBeanDefinitionElement(ele) 123public BeanDefinitionHolder parseBeanDefinitionElement(Element ele) &#123; return parseBeanDefinitionElement(ele, null);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) &#123; String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;&gt;(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; beanName = aliases.remove(0); ... &#125; //检查beanName是否重复 if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; //继续点进去看 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); // Register an alias for the plain bean class name, if still possible, // if the generator returned the class name plus a suffix. // This is expected for Spring 1.2/2.0 backwards compatibility. String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(\"Neither XML 'id' nor 'name' specified - \" + \"using generated bean name [\" + beanName + \"]\"); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; return null;&#125; 继续点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, @Nullable BeanDefinition containingBean) &#123; this.parseState.push(new BeanEntry(beanName)); String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE); &#125; try &#123; //创建GenericBeanDefinition对象,设置parent和className AbstractBeanDefinition bd = createBeanDefinition(className, parent); //解析bean标签的属性，并把解析出来的属性设置到BeanDefinition对象中 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); //解析bean中的meta标签 parseMetaElements(ele, bd); //解析bean中的lookup-method标签 重要程度：2，可看可不看 parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); //解析bean中的replaced-method标签 重要程度：2，可看可不看 parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); //解析bean中的constructor-arg标签 重要程度：2，可看可不看 parseConstructorArgElements(ele, bd); //解析bean中的property标签 重要程度：2，可看可不看 parsePropertyElements(ele, bd); //可以不看，用不到 parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; &#125; catch (ClassNotFoundException ex) &#123; ... &#125; finally &#123; this.parseState.pop(); &#125; return null;&#125; 先创建BeanDefinition的封装GenericBeanDefinition 属性解析 解析每个节点的属性 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public AbstractBeanDefinition parseBeanDefinitionAttributes(Element ele, String beanName, @Nullable BeanDefinition containingBean, AbstractBeanDefinition bd) &#123; //如果有singleton属性,先提示一下,建议使用scope属性 if (ele.hasAttribute(SINGLETON_ATTRIBUTE)) &#123; error(\"Old 1.x 'singleton' attribute in use - upgrade to 'scope' declaration\", ele); &#125; else if (ele.hasAttribute(SCOPE_ATTRIBUTE)) &#123; //如果有scope属性,设置scope bd.setScope(ele.getAttribute(SCOPE_ATTRIBUTE)); &#125; else if (containingBean != null) &#123; // Take default from containing bean in case of an inner bean definition. bd.setScope(containingBean.getScope()); &#125; //设置abstract属性,不实例化,子类需要parent标签引用,父类提供了公共的属性,子类不需要写那么多了 if (ele.hasAttribute(ABSTRACT_ATTRIBUTE)) &#123; bd.setAbstract(TRUE_VALUE.equals(ele.getAttribute(ABSTRACT_ATTRIBUTE))); &#125; //设置lazy-init属性 String lazyInit = ele.getAttribute(LAZY_INIT_ATTRIBUTE); if (DEFAULT_VALUE.equals(lazyInit)) &#123; lazyInit = this.defaults.getLazyInit(); &#125; bd.setLazyInit(TRUE_VALUE.equals(lazyInit)); //设置autowired属性 String autowire = ele.getAttribute(AUTOWIRE_ATTRIBUTE); bd.setAutowireMode(getAutowireMode(autowire)); //设置depends-on属性 if (ele.hasAttribute(DEPENDS_ON_ATTRIBUTE)) &#123; String dependsOn = ele.getAttribute(DEPENDS_ON_ATTRIBUTE); bd.setDependsOn(StringUtils.tokenizeToStringArray(dependsOn, MULTI_VALUE_ATTRIBUTE_DELIMITERS)); &#125; //设置autowired-candidate String autowireCandidate = ele.getAttribute(AUTOWIRE_CANDIDATE_ATTRIBUTE); if (\"\".equals(autowireCandidate) || DEFAULT_VALUE.equals(autowireCandidate)) &#123; String candidatePattern = this.defaults.getAutowireCandidates(); if (candidatePattern != null) &#123; String[] patterns = StringUtils.commaDelimitedListToStringArray(candidatePattern); bd.setAutowireCandidate(PatternMatchUtils.simpleMatch(patterns, beanName)); &#125; &#125; else &#123; bd.setAutowireCandidate(TRUE_VALUE.equals(autowireCandidate)); &#125; //设置primary if (ele.hasAttribute(PRIMARY_ATTRIBUTE)) &#123; bd.setPrimary(TRUE_VALUE.equals(ele.getAttribute(PRIMARY_ATTRIBUTE))); &#125; //设置init-method if (ele.hasAttribute(INIT_METHOD_ATTRIBUTE)) &#123; String initMethodName = ele.getAttribute(INIT_METHOD_ATTRIBUTE); bd.setInitMethodName(initMethodName); &#125; else if (this.defaults.getInitMethod() != null) &#123; bd.setInitMethodName(this.defaults.getInitMethod()); bd.setEnforceInitMethod(false); &#125; //设置destroy-method if (ele.hasAttribute(DESTROY_METHOD_ATTRIBUTE)) &#123; String destroyMethodName = ele.getAttribute(DESTROY_METHOD_ATTRIBUTE); bd.setDestroyMethodName(destroyMethodName); &#125; else if (this.defaults.getDestroyMethod() != null) &#123; bd.setDestroyMethodName(this.defaults.getDestroyMethod()); bd.setEnforceDestroyMethod(false); &#125; //设置factory-method,指定生成实例的工厂方法 if (ele.hasAttribute(FACTORY_METHOD_ATTRIBUTE)) &#123; bd.setFactoryMethodName(ele.getAttribute(FACTORY_METHOD_ATTRIBUTE)); &#125; //设置factory-bean属性,指定生成实例的工厂,这个需要配合factory-method使用 if (ele.hasAttribute(FACTORY_BEAN_ATTRIBUTE)) &#123; bd.setFactoryBeanName(ele.getAttribute(FACTORY_BEAN_ATTRIBUTE)); &#125; return bd;&#125; parseBeanDefinitionAttributes(..)方法主要是解析Node的属性并设置了BeanDefinition的一些属性 meta标签解析 再看meta标签的解析,其实就是把bean标签的meta属性的key,value读取出来,设置到BeanDefinition,没啥用,一个标识而已 1234567891011121314public void parseMetaElements(Element ele, BeanMetadataAttributeAccessor attributeAccessor) &#123; NodeList nl = ele.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, META_ELEMENT)) &#123; Element metaElement = (Element) node; String key = metaElement.getAttribute(KEY_ATTRIBUTE); String value = metaElement.getAttribute(VALUE_ATTRIBUTE); BeanMetadataAttribute attribute = new BeanMetadataAttribute(key, value); attribute.setSource(extractSource(metaElement)); attributeAccessor.addMetadataAttribute(attribute); &#125; &#125;&#125; lookup-method标签解析 同样的lookup-method标签的解析也是类似的 123456789101112131415161718public void parseLookupOverrideSubElements(Element beanEle, MethodOverrides overrides) &#123; NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, LOOKUP_METHOD_ELEMENT)) &#123; Element ele = (Element) node; //获取name属性 String methodName = ele.getAttribute(NAME_ATTRIBUTE); //获取bean属性 String beanRef = ele.getAttribute(BEAN_ELEMENT); //封装成 LookupOverride LookupOverride override = new LookupOverride(methodName, beanRef); override.setSource(extractSource(ele)); //可能有多个lookup-method标签,所以用list装起来 overrides.addOverride(override); &#125; &#125;&#125; 不过值得一提的是,这个lookup-method的设计精髓主要是代理思想,很方便的实现了多态 1234567891011121314&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean class=\"com.lrj.test.bean.Student\" id=\"student\"/&gt; &lt;bean class=\"com.lrj.test.bean.Women\" id=\"women\"/&gt; &lt;!--实现多态,传入什么就是什么--&gt; &lt;bean class=\"com.lrj.test.bean.AbstractClass\"&gt; &lt;lookup-method name=\"getPeople\" bean=\"women\"/&gt; &lt;/bean&gt;&lt;/beans&gt; 1234567891011121314151617181920212223public abstract class People &#123; public void show()&#123;&#125;&#125;public class Women extends People &#123; @Override public void show() &#123; System.out.println(\"I am women\"); &#125;&#125;public abstract class AbstractClass &#123; public void show() &#123; getPeople().show(); &#125; public abstract People getPeople();&#125;//这个方法最终打印的是I am women@Testpublic void testXml() &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\"); AbstractClass abstractClass=context.getBean(AbstractClass.class); abstractClass.show();&#125; replace-method标签解析 再看看parseReplacedMethodSubElements(…)解析replace-method属性 123456789101112131415161718192021222324252627public void parseReplacedMethodSubElements(Element beanEle, MethodOverrides overrides) &#123; NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, REPLACED_METHOD_ELEMENT)) &#123; Element replacedMethodEle = (Element) node; String name = replacedMethodEle.getAttribute(NAME_ATTRIBUTE); String callback = replacedMethodEle.getAttribute(REPLACER_ATTRIBUTE); //一个replaced-method标签封装成一个ReplaceOverride对象，最后加入到BeanDefinition对象中 ReplaceOverride replaceOverride = new ReplaceOverride(name, callback); // Look for arg-type match elements. List&lt;Element&gt; argTypeEles = DomUtils.getChildElementsByTagName(replacedMethodEle, ARG_TYPE_ELEMENT); for (Element argTypeEle : argTypeEles) &#123; //根据方法参数类型来区分同名的不同的方法 String match = argTypeEle.getAttribute(ARG_TYPE_MATCH_ATTRIBUTE); match = (StringUtils.hasText(match) ? match : DomUtils.getTextValue(argTypeEle)); if (StringUtils.hasText(match)) &#123; replaceOverride.addTypeIdentifier(match); &#125; &#125; replaceOverride.setSource(extractSource(replacedMethodEle)); overrides.addOverride(replaceOverride); &#125; &#125;&#125; 可以发现lookup-method和replace-method都放入了BeanDefinition的MethodOverrides类型的overrides属性中,也就是说,MethodOverrides包含了LookupOverride和ReplaceOverride两种类型的对象 1234567891011&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean class=\"com.lrj.test.bean.Replacement\" id=\"replacement\"/&gt; &lt;bean class=\"com.lrj.test.bean.Origin\"&gt; &lt;replaced-method name=\"show\" replacer=\"replacement\"&gt; &lt;arg-type match=\"int\"/&gt; &lt;/replaced-method&gt; &lt;/bean&gt;&lt;/beans&gt; 123456789101112131415161718192021222324252627public class Origin &#123; public void show(String str) &#123; System.out.println(\"show str:\" + str); &#125; public void show(int str) &#123; System.out.println(\"show int:\" + str); &#125;&#125;public class Replacement implements MethodReplacer &#123; @Override public Object reimplement(Object obj, Method method, Object[] args) throws Throwable &#123; System.out.println(\"I am a placement method......\"); return null; &#125;&#125;//这个打印的将会是show str:hello和I am a placement method......//说明int的被替换了,但是String的没有被替换,@Testpublic void testReplace() &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\"); Origin origin = context.getBean(Origin.class); origin.show(\"hello\"); origin.show(555);&#125; 这个听说是在项目封版之后,不想改代码了,直接改配置,符合开闭原则,但是这个Replacement必须要实现MethodReplacer感觉有点鸡肋 constructor-arg标签解析 这个没啥讲的,无非是根据index或者name来设置 不过需要注意ConstructorArgumentValues对象保存了ValueHolder集合 解析construct-arg标签,读取下标或者name封装成ValueHolder,构成BeanDefinition的ConstructorArgumentValues 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public void parseConstructorArgElements(Element beanEle, BeanDefinition bd) &#123; NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, CONSTRUCTOR_ARG_ELEMENT)) &#123; parseConstructorArgElement((Element) node, bd); &#125; &#125;&#125;public void parseConstructorArgElement(Element ele, BeanDefinition bd) &#123; String indexAttr = ele.getAttribute(INDEX_ATTRIBUTE); String typeAttr = ele.getAttribute(TYPE_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); if (StringUtils.hasLength(indexAttr)) &#123; try &#123; int index = Integer.parseInt(indexAttr); if (index &lt; 0) &#123; error(\"'index' cannot be lower than 0\", ele); &#125; else &#123; try &#123; this.parseState.push(new ConstructorArgumentEntry(index)); Object value = parsePropertyValue(ele, bd, null); ConstructorArgumentValues.ValueHolder valueHolder = new ConstructorArgumentValues.ValueHolder(value); if (StringUtils.hasLength(typeAttr)) &#123; valueHolder.setType(typeAttr); &#125; if (StringUtils.hasLength(nameAttr)) &#123; valueHolder.setName(nameAttr); &#125; valueHolder.setSource(extractSource(ele)); if (bd.getConstructorArgumentValues().hasIndexedArgumentValue(index)) &#123; error(\"Ambiguous constructor-arg entries for index \" + index, ele); &#125; else &#123; //将ValueHolder添加到BeanDefinition bd.getConstructorArgumentValues().addIndexedArgumentValue(index, valueHolder); &#125; &#125; finally &#123; this.parseState.pop(); &#125; &#125; &#125; catch (NumberFormatException ex) &#123; error(\"Attribute 'index' of tag 'constructor-arg' must be an integer\", ele); &#125; &#125; else &#123; try &#123; this.parseState.push(new ConstructorArgumentEntry()); Object value = parsePropertyValue(ele, bd, null); ConstructorArgumentValues.ValueHolder valueHolder = new ConstructorArgumentValues.ValueHolder(value); if (StringUtils.hasLength(typeAttr)) &#123; valueHolder.setType(typeAttr); &#125; if (StringUtils.hasLength(nameAttr)) &#123; valueHolder.setName(nameAttr); &#125; valueHolder.setSource(extractSource(ele)); //将ValueHolder添加到BeanDefinition bd.getConstructorArgumentValues().addGenericArgumentValue(valueHolder); &#125; finally &#123; this.parseState.pop(); &#125; &#125;&#125; property标签解析 1234567891011121314151617181920212223242526272829303132333435public void parsePropertyElements(Element beanEle, BeanDefinition bd) &#123; NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, PROPERTY_ELEMENT)) &#123; parsePropertyElement((Element) node, bd); &#125; &#125;&#125;public void parsePropertyElement(Element ele, BeanDefinition bd) &#123; //获取name属性 String propertyName = ele.getAttribute(NAME_ATTRIBUTE); if (!StringUtils.hasLength(propertyName)) &#123; error(\"Tag 'property' must have a 'name' attribute\", ele); return; &#125; this.parseState.push(new PropertyEntry(propertyName)); try &#123; if (bd.getPropertyValues().contains(propertyName)) &#123; error(\"Multiple 'property' definitions for property '\" + propertyName + \"'\", ele); return; &#125; Object val = parsePropertyValue(ele, bd, propertyName); //将属性设置包装成 PropertyValue PropertyValue pv = new PropertyValue(propertyName, val); parseMetaElements(ele, pv); pv.setSource(extractSource(ele)); //将 PropertyValue添加到BeanDefinition bd.getPropertyValues().addPropertyValue(pv); &#125; finally &#123; this.parseState.pop(); &#125;&#125; 和解析构造函数的参数一样,对于property标签的解析,同样是将key,value封装成PropertyValue,添加到BeanDefinition中,形成MutablePropertyValues类型 qualifier标签解析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public void parseQualifierElements(Element beanEle, AbstractBeanDefinition bd) &#123; NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, QUALIFIER_ELEMENT)) &#123; parseQualifierElement((Element) node, bd); &#125; &#125;&#125;public void parseQualifierElement(Element ele, AbstractBeanDefinition bd) &#123; String typeName = ele.getAttribute(TYPE_ATTRIBUTE); if (!StringUtils.hasLength(typeName)) &#123; error(\"Tag 'qualifier' must have a 'type' attribute\", ele); return; &#125; this.parseState.push(new QualifierEntry(typeName)); try &#123; AutowireCandidateQualifier qualifier = new AutowireCandidateQualifier(typeName); qualifier.setSource(extractSource(ele)); String value = ele.getAttribute(VALUE_ATTRIBUTE); if (StringUtils.hasLength(value)) &#123; qualifier.setAttribute(AutowireCandidateQualifier.VALUE_KEY, value); &#125; NodeList nl = ele.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, QUALIFIER_ATTRIBUTE_ELEMENT)) &#123; Element attributeEle = (Element) node; String attributeName = attributeEle.getAttribute(KEY_ATTRIBUTE); String attributeValue = attributeEle.getAttribute(VALUE_ATTRIBUTE); if (StringUtils.hasLength(attributeName) &amp;&amp; StringUtils.hasLength(attributeValue)) &#123; BeanMetadataAttribute attribute = new BeanMetadataAttribute(attributeName, attributeValue); attribute.setSource(extractSource(attributeEle)); qualifier.addMetadataAttribute(attribute); &#125; else &#123; error(\"Qualifier 'attribute' tag must have a 'name' and 'value'\", attributeEle); return; &#125; &#125; &#125; bd.addQualifier(qualifier); &#125; finally &#123; this.parseState.pop(); &#125;&#125; 至此,BeanDefinition的解析完成,此时再回到BeanDefinitionParserDelegate.parseBeanDefinitionElement(org.w3c.dom.Element, org.springframework.beans.factory.config.BeanDefinition) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) &#123; String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;&gt;(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; beanName = aliases.remove(0); if (logger.isTraceEnabled()) &#123; logger.trace(\"No XML 'id' specified - using '\" + beanName + \"' as bean name and \" + aliases + \" as aliases\"); &#125; &#125; //检查beanName是否重复 if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; //点进去 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); // Register an alias for the plain bean class name, if still possible, // if the generator returned the class name plus a suffix. // This is expected for Spring 1.2/2.0 backwards compatibility. String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(\"Neither XML 'id' nor 'name' specified - \" + \"using generated bean name [\" + beanName + \"]\"); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; return null;&#125; 这里返回的是,又对BeanDefinition做了一层包装,成BeanDefinitionHolder,形成name-&gt;BeanDefinition的映射","categories":[{"name":"Spring","slug":"Spring","permalink":"https://lurongjiang.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://lurongjiang.github.io/tags/Spring/"}]},{"title":"Scala基础-类","slug":"Scala基础-类","date":"2019-04-18T03:45:04.000Z","updated":"2019-04-18T10:23:45.000Z","comments":true,"path":"2019/04/18/Scala基础-类/","link":"","permalink":"https://lurongjiang.github.io/2019/04/18/Scala%E5%9F%BA%E7%A1%80-%E7%B1%BB/","excerpt":"Scala中类的介绍和使用包括抽象类,类的伴生,接口等","text":"Scala基础-类 抽象类 abstract 抽象类可包含未实现的方法和未初始化的属性 不可直接new,否则报错 1234567891011abstract class People &#123; def eat() def play() //抽象类可以具有普通方法 def work(): Unit = &#123; println(name + \"is working.....\") &#125; //不初始化,必须指定类型,否则报错 val name: String&#125; 具体实现交给子类实现 123456789101112class Worker extends People&#123; override def eat(): Unit = &#123; println(name + \"is eating.....\") &#125; override def play(): Unit = &#123; println(name + \"is playing.....\") &#125; override val name: String = \"Zhangsan\"&#125; 使用时直接new具体的实现类即可 1234567object PeopleTest &#123; def main(args: Array[String]): Unit = &#123; val worker = new Worker() worker.play() worker.work() &#125;&#125; 伴生类和伴生对象-Companion 其实在Scala中是不存在静态的属性和方法的概念 但是scala中,object却是和java中类似的用法,不需要new对象,可以直接使用 12345678910111213object ObjectTest &#123; val name = \"zhangsan\" def play(name: String=\"War3\"): Unit = &#123; println(\"play game :\" + name) &#125; def main(args: Array[String]): Unit = &#123; //不需要new直接用 ObjectTest.play() ObjectTest.play(\"FIFA 2019\") &#125;&#125; 名称相同的Class和Object互为伴生 Class称为Object的伴生类 Object称为Class的伴生对象 但是有一个很特殊的方法apply() 12345678910111213141516171819202122232425262728class CompanionTest&#123; println(\"class start----\") def apply(): Unit = &#123; println(\"apply in class...\") &#125; println(\"class end----\")&#125;object CompanionTest &#123; println(\"object start----\") def apply(): Unit = &#123; println(\"apply in object...\") new CompanionTest() &#125; println(\"object end----\")&#125;object Companion &#123; def main(args: Array[String]): Unit = &#123; val test = new CompanionTest() //对象()调用的是class中的apply方法 test() //不需要new的是调用object中的apply方法 CompanionTest() &#125;&#125; 因为object就是类的伴生对象,所以直接类()也相当于对一个具体的对象进行调用apply() apply()方法很神奇,不同的类都提供了不同的实现,只需要看懂这种用法就行 枚举类-Enumeration 枚举类对于有限个数的类型十分有用 需要通过继承Enumeration 123456789101112131415161718object WeekDay extends Enumeration &#123; val Mon = Value(1, \"星期一\") val Tue = Value(2, \"星期二\") val Wed = Value(3, \"星期三\") val Thu = Value(4, \"星期四\") val Fri = Value(5, \"星期五\") val San = Value(6, \"星期六\") val Sun = Value(7, \"星期日\") def main(args: Array[String]): Unit = &#123; println(WeekDay.Mon) println(WeekDay(2)) println(WeekDay.withName(\"星期三\")) for (elem &lt;- WeekDay.values) &#123; println(elem) &#125; &#125;&#125; 但是枚举类使用不存在的值时会报错 样例类-case class scala中case class称为样例类 使用时不需要new,当然,想加也是ok的 在sparkSQL中大量使用了case class case class已经实现了序列化,不需要实现序列化了 1234567case class CaseClass(name: String, age: Int)object CaseClassTest &#123; def main(args: Array[String]): Unit = &#123; println(CaseClass(\"Jack\", 22)) println(new CaseClass(\"拉布拉多\", 15)) &#125;&#125; 当然了有case class也有case object 需要注意的是 case object不能加参数,否则报错 12345678910111213case class CaseClass(name: String, age: Int)case object CaseClass&#123; def apply(): Unit =&#123; println(\"case object\") &#125;&#125;object CaseClassTest &#123; def main(args: Array[String]): Unit = &#123; println(CaseClass(\"Jack\", 22)) println(new CaseClass(\"拉布拉多\", 15)) CaseClass() &#125;&#125; 接口-trait Scala中和Java接口概念对应的是trait trait的用法和抽象类的用法类似 实现也用extends关键字 多实现,第二个trait开始,使用with进行连接 12345678910111213141516171819202122trait TraitTest &#123;def play()&#125;trait TraitTest1 &#123; def play1()&#125;class TraitTestImpl extends TraitTest with TraitTest1&#123; override def play(): Unit = &#123; println(\"Hello,trait play.....\") &#125; override def play1(): Unit = &#123; println(\"Hello,trait1 play.....\") &#125;&#125;object TraitTestA&#123; def main(args: Array[String]): Unit = &#123; val impl = new TraitTestImpl() impl.play() &#125;&#125; 注意,实现的两个trait不能有相同的方法签名,否则,编译时报错,运行时过不去 例如,下面这个就运行不过去 123456789101112131415161718//错误的案例trait TraitTest &#123;def play()&#125;trait TraitTest1 &#123; def play()&#125;class TraitTestImpl extends TraitTest with TraitTest1&#123; override def play(): Unit = &#123; println(\"Hello,trait play.....\") &#125;&#125;object TraitTestA&#123; def main(args: Array[String]): Unit = &#123; val impl = new TraitTestImpl() impl.play() &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/tags/Scala/"}]},{"title":"Flume稍复杂的一些玩意","slug":"Flume稍复杂的一些玩意","date":"2019-04-06T07:36:48.000Z","updated":"2019-04-06T02:36:45.000Z","comments":true,"path":"2019/04/06/Flume稍复杂的一些玩意/","link":"","permalink":"https://lurongjiang.github.io/2019/04/06/Flume%E7%A8%8D%E5%A4%8D%E6%9D%82%E7%9A%84%E4%B8%80%E4%BA%9B%E7%8E%A9%E6%84%8F/","excerpt":"Flume稍复杂的一些玩意,例如多个agent,sink,source,高可用,负载均衡等的基本配置和使用","text":"Flume稍复杂的一些玩意 Flume的稍复杂配置 两个agent的传递 agent A -&gt;发送数据到agent B a.conf 12345678910111213141516171819202122232425# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F ~/usr/software/test.txt# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44441# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 b.conf 12345678910111213141516171819202122# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44441# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动 注意启动顺序 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/b.conf 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/a.conf 两个source 采集多个渠道的日志 two_source.conf 123456789101112131415161718192021222324252627# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1 r2a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txta1.sources.r2.type = netcata1.sources.r2.bind = 0.0.0.0a1.sources.r2.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sources.r2.channels = c1a1.sinks.k1.channel = c1 启动 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/two_source.conf 之后可以telnet 44444端口和写文件进行测试 两个Sink 上游下来一份数据,写到两个位置 two_sink.conf 1234567891011121314151617181920212223242526# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txt# Describe the sinka1.sinks.k1.type = loggera1.sinks.k2.type = hdfsa1.sinks.k2.hdfs.path = hdfs://hadoop001:9000/flume/test# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1a1.sinks.k2.channel = c1 启动 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/two_sink.conf 但是启动之后会发现,有一些数据在hdfs上,有的在logger中,由此可见,flume默认同一份你数据只会发一次 两个channel和两个Sink 前面的两个Sink似乎并不是我们想要的结果,我们希望写到两个位置,上面虽然写道了两个位置,但是都不是完整的,所以还需要调整一下 c2s2.conf 12345678910111213141516171819202122232425# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txt# Describe the sinka1.sinks.k1.type = loggera1.sinks.k2.type = hdfsa1.sinks.k2.hdfs.path = hdfs://hadoop001:9000/flume/test# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c2.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2 启动 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/c2s2.conf 多个个Channel其实涉及到一个ChannelSelector的概念,但是ChannelSelector的策略默认是replicating复制策略,正是我们需要的,所以就不用指定了 a1.sources.r1.selector.type=…. a1.sources.r1.selector.optional=…. Multiplexing Channel Selector 有时候日志是从不同的地方传过来的,携带了不同的标识信息header,需要存放不同的地方或者归类 因为需要添加header,所以需要增加Interceptor sink1.conf 1234567891011121314151617181920212223242526# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44441# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44444# Use a channel which buffers events in memorya1.channels.c1.type = memory# 添加CN Headera1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = statica1.sources.r1.interceptors.i1.key = headera1.sources.r1.interceptors.i1.value = CN# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 sink2.conf 1234567891011121314151617181920212223242526# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txt# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44444# Use a channel which buffers events in memorya1.channels.c1.type = memory# 添加EN Headera1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = statica1.sources.r1.interceptors.i1.key = headera1.sources.r1.interceptors.i1.value = EN# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 sink3.conf 12345678910111213141516171819202122# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = TAILDIRa1.sources.r1.filegroups = f1a1.sources.r1.filegroups.f1 = /usr/software/test/.*.log# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44444# Use a channel which buffers events in memorya1.channels.c1.type = memory#不加Header# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 接收端的sink-all.conf 123456789101112131415161718192021222324252627282930313233343536373839# Name the components on this agenta1.sources = r1a1.sinks = k1 k2 k3a1.channels = c1 c2 c3# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://hadoop001:9000/flume/cna1.sinks.k1.hdfs.writeFormat = Texta1.sinks.k1.hdfs.fileType = DataStreama1.sinks.k2.type = hdfsa1.sinks.k2.hdfs.path = hdfs://hadoop001:9000/flume/ena1.sinks.k2.hdfs.writeFormat = Texta1.sinks.k2.hdfs.fileType = DataStreama1.sinks.k3.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c2.type = memorya1.channels.c3.type = memorya1.sources.r1.selector.type = multiplexinga1.sources.r1.selector.header = headera1.sources.r1.selector.mapping.CN = c1a1.sources.r1.selector.mapping.EN = c2a1.sources.r1.selector.default = c3# Bind the source and sink to the channela1.sources.r1.channels = c1 c2 c3a1.sinks.k1.channel = c1a1.sinks.k1.channel = c2a1.sinks.k1.channel = c3 启动 注意启动顺序 先启动sink-all 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/sink-all.conf 启动sink 1,2,3 1234567891011121314151617# 使用telnet 44441端口测试flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/sink1.conf# 监听的是 /usr/software/test.txtflume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/sink2.conf# 监听的是/usr/software/test/.*.logflume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/sink-3.conf 使用对应的方式进行测试,即可 Sink Processor FailOver配置 容错配置和两个Sink类似,但是正常情况下,只会用优先级高的Sink,当另一个Sink不可用的时候才会使用备用的Sink first.conf 123456789101112131415161718192021# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44441# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 second.conf 123456789101112131415161718192021# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44442# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 failover.conf 1234567891011121314151617181920212223242526272829303132333435# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txt# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44441a1.sinks.k2.type = avroa1.sinks.k2.hostname = 0.0.0.0a1.sinks.k2.port = 44442# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1a1.sinks.k2.channel = c1# 设置sink的优先级,sink process为容错机制a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.g1.processor.type = failovera1.sinkgroups.g1.processor.priority.k1 = 5a1.sinkgroups.g1.processor.priority.k2 = 10a1.sinkgroups.g1.processor.maxpenalty = 10000 启动 启动k1,k2 12345678910flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/first.confflume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/second.conf 启动failover.conf 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/failover.conf 然后追加内容到/usr/software/test.txt就可以看到变化了,当两个都可用的时候,44442端口优先使用,当关闭44442对应的agent,则会自动切换到44441的agent 可以发现,优先的值越高,优先级越高 LoadBalance配置 load1.conf 123456789101112131415161718192021# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44441# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 load2.conf 123456789101112131415161718192021# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44442# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 load.conf 12345678910111213141516171819202122232425262728293031323334# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txt# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44441a1.sinks.k2.type = avroa1.sinks.k2.hostname = 0.0.0.0a1.sinks.k2.port = 44442# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1a1.sinks.k2.channel = c1# 配置负载均衡为随机数机制a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.g1.processor.type = load_balancea1.sinkgroups.g1.processor.backoff = truea1.sinkgroups.g1.processor.selector = random 启动 启动load1,load2 123456789flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/load1.confflume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/load2.conf 启动load.conf 12345flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/load.conf 之后往/usr/software/test.txt添加内容可以看到两个都可以看到内容的变化,实现负载均衡. 负载均衡还有一个机制:轮询","categories":[{"name":"Flume","slug":"Flume","permalink":"https://lurongjiang.github.io/categories/Flume/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://lurongjiang.github.io/tags/Flume/"}]},{"title":"Azkaban配置代理用户执行任务","slug":"Azkaban配置代理用户执行任务","date":"2019-04-06T05:15:54.000Z","updated":"2019-04-06T08:45:23.000Z","comments":true,"path":"2019/04/06/Azkaban配置代理用户执行任务/","link":"","permalink":"https://lurongjiang.github.io/2019/04/06/Azkaban%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86%E7%94%A8%E6%88%B7%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1/","excerpt":"简单的配置,实现登录的Azkaban用户和执行任务用户不同的需求","text":"Azkaban配置代理用户执行任务 有时候我们希望启动azkaban的用户和启动任务脚本使用不同的用户 这时候我们可以使用azkaban的execute-as-user插件完成,参考文档Plugin Configurations 1234567891011# 这里修正一下,execute-as-user.c的位置官网写成azkaban-common的目录# 但是3.81.0我在github在azkaban-util下面找到的scp ./az-exec-util/src/main/c/execute-as-user.c# 编译C文件,需要提前安装gccgcc execute-as-user.c -o execute-as-user# 设置权限为rootchown root execute-as-user# 这个权限比较有意思,---Sr-s---,不懂意思,很特殊的权限chmod 6050 execute-as-user 配置一下plugin $AZKABAN_HOME/plugins/jobtypes/commonprivate.properties 1234execute.as.user=trueazkaban.native.lib=/home/lurongjiang/azkaban-3.81.0proxy.user=azkaban.should.proxy=true 这样在project的.flow中就可以设置user.to.proxy参数来实现用户代理执行了 注意 要代理的用户记得要在linux中存在 默认使用的是azkaban用户组,所以需要groupadd,或者参考文档中的另外一个参数 记得赋予proxy用户执行脚本的权限,否则百搭 test.flow 1234567891011121314151617config: time: \"\" user.to.proxy: lurongjiangnodes: - name: jobA type: command config: command: /home/lurongjiang/test/a.sh $&#123;time&#125; - name: jobB type: command config: command: /home/lurongjiang/test/b.sh $&#123;time&#125; - name: jobC type: command config: command: /home/lurongjiang/test/c.sh $&#123;time&#125;","categories":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://lurongjiang.github.io/categories/Azkaban/"}],"tags":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://lurongjiang.github.io/tags/Azkaban/"}]},{"title":"SpringCloud概述","slug":"SpringCloud","date":"2019-04-05T01:25:14.000Z","updated":"2019-05-03T12:15:28.000Z","comments":true,"path":"2019/04/05/SpringCloud/","link":"","permalink":"https://lurongjiang.github.io/2019/04/05/SpringCloud/","excerpt":"SpringCloud相关介绍","text":"SpringCloud SpringCloud概述 官网 官网 主要功能 常用子项目 版本与兼容 SpringCloud的版本命名 版本命名 SpringCloud的版本,前半部分(如Hoxton,Greenwich),意思是发布列车(ReleaseTrain),以伦敦地铁的站名命名,因为SpringCloud有很多的子项目,每个项目都有自己的版本管理,按照发布顺序以A,B,C等为首字母依次命名,已经发布的版本顺序为: Angel -&gt; Brixton -&gt; Camden -&gt; Dalston -&gt; Edgware -&gt; Finchley -&gt; Greenwich -&gt; Hoxton 后半部分(如SR,SR1,SR2),意思是服务发布(ServiceRelease),即重大Bug修复 版本发布流程 SNAPSHOT -&gt; Mx -&gt; RELEASE -&gt; SRx,其中x就是一些数字序号,例如M1,M2,SR1,SR2.SNAPSHOT为快照版本(开发版本),Mx为里程碑版本,此时并不是正式版本,但是已经接近正式版,经过多个版本迭代之后,发布第一个RELEASE版本,正式版本;在RELEASE版本之后如果有重大bug修复就会发布SR版本 Hoxton SR1 CURRENT GA Reference Doc. Hoxton SNAPSHOT Reference Doc. Greenwich SR5 GA Reference Doc. Greenwich SNAPSHOT Reference Doc. SpringCloud的版本生命周期 版本发布规划 https://github.com/spring-cloud/spring-cloud-release/milestones 版本发布记录 https://github.com/spring-cloud/spring-cloud-release/releases 版本终止声明 https://spring.io/projects/spring-cloud#overview SpringBoot与SpringCloud的兼容性 版本兼容性非常重要https://spring.io/projects/spring-cloud#overview Release Train Boot Version Hoxton 2.2.x Greenwich 2.1.x Finchley 2.0.x Edgware 1.5.x Dalston 1.5.x 生产环境如何选择选择 坚决不适用非稳定版本 坚决不适用end-of-life版本 尽量使用最新版本 RELEASE版本可以观望/调研,因为是第一个正式版,并没有在生产上得以广泛应用 SR2之后可以大规模使用 版本选择 SpringCloud Hoxton SR1 SpringBoot 2.2.4.RELEASE 12345678910111213141516171819&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-dependencies --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Hoxton.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 检查项目是否能运行 1mvn clean install -U SpringCloud服务注册与发现 使得服务消费者总能找到服务提供者 Consul单机版安装 Consul下载 下载Consoule https://releases.hashicorp.com/consul/1.6.3/consul_1.6.3_linux_amd64.zip 需要的端口 Use Default Ports DNS: The DNS server (TCP and UDP) 8600 HTTP: The HTTP API (TCP Only) 8500 HTTPS: The HTTPs API disabled (8501)* gRPC: The gRPC API disabled (8502)* LAN Serf: The Serf LAN port (TCP and UDP) 8301 Wan Serf: The Serf WAN port TCP and UDP) 8302 server: Server RPC address (TCP Only) 8300 Sidecar Proxy Min: Inclusive min port number to use for automatically assigned sidecar service registrations. 21000 Sidecar Proxy Max: Inclusive max port number to use for automatically assigned sidecar service registrations. 21255 检查端口是否被占用的方法 12345678910111213Windows:# 如果没有结果说明没有被占用netstat -ano| findstr \"8500\"Linux:# 如果没有结果说明没有被占用netstat -antp |grep 8500macOS:# 如果没有结果说明没有被占用netstat -ant | grep 8500或lsof -i:8500 安装和启动 解压 1./consul agent -dev -client 0.0.0.0 严重是否成功 1./consul -v 访问Consul首页localhost:8500 启动参数 -ui 开启ui -client 让consul拥有client功能,接受服务注册;0.0.0.0允许任意ip注册,不写只能使用localhost连接 -dev 以开发模式运行consul 整合Consul 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 配置 1234567891011spring: application: # 指定注册到consul的服务名称,分隔符不能是下划线 # 如果服务发现组件是Consul,会强制转换成中划线,导致找不到服务 # 如果服务发现组件是Ribbon,则因为Ribbon的问题(把默认名称当初虚拟主机名,而虚拟主机名不能用下划线),会造成微服务之间无法调用 name: micro-service-user cloud: consul: host: 192.168.238.128 port: 8500 启动,检查consul ui的服务上线情况 Consul健康检查 添加健康检查依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置 12345management: endpoints: web: exposure: include: '*' 端点 http://localhost:8080/actuator 查看端点 http://localhost:8080/actuator/health 健康检查 添加详情配置,可以检查详细的健康情况 1234management: endpoint: health: show-details: always 简单研究一下健康检查的源码 以磁盘检查的为例 健康检查的类都继承了AbstractHealthIndicator抽象类,而AbstractHealthIndicator实现了HealthIndicator接口,所有健康检查实现类都必须实现doHealthCheck(Health.Builder builder)方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class DiskSpaceHealthIndicator extends AbstractHealthIndicator &#123; private static final Log logger = LogFactory.getLog(DiskSpaceHealthIndicator.class); private final File path; private final DataSize threshold; /** * Create a new &#123;@code DiskSpaceHealthIndicator&#125; instance. * @param path the Path used to compute the available disk space * @param threshold the minimum disk space that should be available */ public DiskSpaceHealthIndicator(File path, DataSize threshold) &#123; super(\"DiskSpace health check failed\"); this.path = path; this.threshold = threshold; &#125; @Override protected void doHealthCheck(Health.Builder builder) throws Exception &#123; //获取可用的空间字节数 long diskFreeInBytes = this.path.getUsableSpace(); //如果可用的字节数大于预留字节数阈值则认为是健康的,设置status为UP if (diskFreeInBytes &gt;= this.threshold.toBytes()) &#123; builder.up(); &#125; else &#123; //否则任务是不健康的,设置status为DOWN logger.warn(LogMessage.format(\"Free disk space below threshold. Available: %d bytes (threshold: %s)\", diskFreeInBytes, this.threshold)); builder.down(); &#125; //输出总空间,可用空间和预留阈值 builder.withDetail(\"total\", this.path.getTotalSpace()).withDetail(\"free\", diskFreeInBytes) .withDetail(\"threshold\", this.threshold.toBytes()); &#125;&#125;//这个获取可用字节数还是挺好的,直接利用了File提供的方法public long getUsableSpace() &#123; SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; sm.checkPermission(new RuntimePermission(\"getFileSystemAttributes\")); sm.checkRead(path); &#125; if (isInvalid()) &#123; return 0L; &#125; //fs是默认的文件系统FileSystem fs = DefaultFileSystem.getFileSystem(); return fs.getSpace(this, FileSystem.SPACE_USABLE);&#125; 健康检查使用了建造者模式,对于不同的健康指标非常方便,值得学习 整合Consul和SpringCloud的actuator 修改配置 1234567spring: cloud: consul: host: 192.168.238.128 port: 8500 discovery: health-check-path: /actuator/health 这样启动之后,再检查consul ui就可以发现没有红色的叉了 其他的健康检查配置 注册课程微服务到Consul 添加依赖 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置 1234567891011121314151617181920212223242526272829303132spring: datasource: url: jdbc:mysql://192.168.238.128:3306/ms?serverTimezone=GMT%2B8&amp;characterEncoding=utf8&amp;useSSL=false hikari: username: lrj password: lu11221015 driver-class-name: com.mysql.cj.jdbc.Driver# JPA配置 jpa: hibernate: ddl-auto: update show-sql: true application: name: micro-service-class cloud: consul: host: 192.168.238.128 port: 8500 discovery: health-check-path: /actuator/health# 暴露所有的actuator端点management: endpoints: web: exposure: include: '*' # 开启健康检查详细信息 endpoint: health: show-details: alwaysserver: port: 8081 重构用户微服务 12345678910111213141516171819@RestController@RequestMapping(\"user\")public class UserController &#123; @Resource private UserService userService; @Resource private DiscoveryClient discoveryClient; @GetMapping(\"&#123;id&#125;\") public Object findUserById(@PathVariable(\"id\") Integer id) &#123; return userService.findUserById(id); &#125; @GetMapping(\"discoveryTest\") public Object discoveryTest() &#123; return discoveryClient.getInstances(\"micro-service-class\"); &#125;&#125; 访问端点,可以发现不需要指定课程微服务的主机和端口就可以拿到相关信息,实现了服务发现 重构课程微服务 原来 123456789101112131415161718192021222324252627282930313233@Servicepublic class LessonServiceImpl implements LessonService &#123; @Resource private LessonRepository lessonRepository; @Resource private LessonUserRepository lessonUserRepository; @Resource private RestTemplate restTemplate; @Override public Lesson buyById(Integer id) &#123; // 1. 根据课程id查询课程 Lesson lesson = lessonRepository.findById(id).orElseThrow(() -&gt; new IllegalArgumentException(\"该课程不存在\")); //根据课程查询是否已经购买过 LessonUser lessonUser = lessonUserRepository.findByLessonId(id); if (lessonUser != null) &#123; return lesson; &#125; //todo 2.登录之后获取userId String userId = \"1\"; // 3. 如果没有购买过,查询用户余额 UserDTO userDTO = restTemplate.getForObject(\"http://localhost:8080/user/&#123;userId&#125;\", UserDTO.class, userId); if (userDTO != null &amp;&amp; userDTO.getMoney() != null &amp;&amp; userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() &lt; 0) &#123; throw new IllegalArgumentException(\"余额不足\"); &#125; //4. 购买逻辑 //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录 return lesson; &#125;&#125; 这个写死了主机地址,无法动态获取微服务路径 重构 123456789101112131415161718192021222324252627282930313233343536373839public class LessonServiceImpl implements LessonService &#123; @Resource private LessonRepository lessonRepository; @Resource private LessonUserRepository lessonUserRepository; @Resource private DiscoveryClient discoveryClient; @Resource private RestTemplate restTemplate; @Override public Lesson buyById(Integer id) &#123; // 1. 根据课程id查询课程 Lesson lesson = lessonRepository.findById(id).orElseThrow(() -&gt; new IllegalArgumentException(\"该课程不存在\")); //根据课程查询是否已经购买过 LessonUser lessonUser = lessonUserRepository.findByLessonId(id); if (lessonUser != null) &#123; return lesson; &#125; //todo 2.登录之后获取userId String userId = \"1\"; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(\"micro-service-user\"); if (instances != null &amp;&amp; !instances.isEmpty()) &#123; //todo 需要改进,如果存在多个实例,需要考虑负载均衡 ServiceInstance instance = instances.get(0); URI uri = instance.getUri(); UserDTO userDTO = restTemplate.getForObject(uri + \"/user/&#123;userId&#125;\", UserDTO.class, userId); if (userDTO != null &amp;&amp; userDTO.getMoney() != null &amp;&amp; userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() &lt; 0) &#123; throw new IllegalArgumentException(\"余额不足\"); &#125; //4. 购买逻辑 //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录 return lesson; &#125; throw new IllegalArgumentException(\"用户微服务异常,无法购买课程\"); &#125;&#125; 可以动态的获取到用户微服务的地址,请求正常 元数据 Consul是没有元数据的概念的,所以SpringCloud做了个适配,在consul下设置tags作为元数据. 元数据可以对微服务添加描述,标识,例如机房在哪里,这样可以进行就近判断,或者当就近机房不可用时才检查远程机房,当两者都不可用时才认为服务不可用等实现容灾或者跨机房 配置 12345678spring: cloud: consul: host: 192.168.238.128 port: 8500 discovery: health-check-path: /actuator/health tags: JiFang=Beijing,JiFang=Shanghai 实现机房选择 123456789101112@GetMapping(\"discoveryTest\")public Object discoveryTest() &#123; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(\"micro-service-class\"); if (instances != null) &#123; List&lt;ServiceInstance&gt; shanghaiInstances = instances.stream() .filter(s -&gt; s.getMetadata().containsKey(\"Shanghai\")).collect(Collectors.toList()); if (!shanghaiInstances.isEmpty()) &#123; return shanghaiInstances; &#125; &#125; return instances;&#125;","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://lurongjiang.github.io/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://lurongjiang.github.io/tags/SpringCloud/"}]},{"title":"Flume使用教程","slug":"Flume","date":"2019-04-04T07:14:54.000Z","updated":"2019-04-04T10:45:45.000Z","comments":true,"path":"2019/04/04/Flume/","link":"","permalink":"https://lurongjiang.github.io/2019/04/04/Flume/","excerpt":"Flume的简单使用教程,分别列了几个常见的Flume配置","text":"Flume Flume是高可靠,高可用的,分布式的海量日志收集,聚合和传输的系统, 数据流模型 A Flume event is defined as a unit of data flow having a byte payload and an optional set of string attributes. A Flume agent is a (JVM) process that hosts the components through which events flow from an external source to the next destination (hop). A Flume source consumes events delivered to it by an external source like a web server. The external source sends events to Flume in a format that is recognized by the target Flume source. For example, an Avro Flume source can be used to receive Avro events from Avro clients or other Flume agents in the flow that send events from an Avro sink. A similar flow can be defined using a Thrift Flume Source to receive events from a Thrift Sink or a Flume Thrift Rpc Client or Thrift clients written in any language generated from the Flume thrift protocol.When a Flume source receives an event, it stores it into one or more channels. The channel is a passive store that keeps the event until it’s consumed by a Flume sink. The file channel is one example – it is backed by the local filesystem. The sink removes the event from the channel and puts it into an external repository like HDFS (via Flume HDFS sink) or forwards it to the Flume source of the next Flume agent (next hop) in the flow. The source and sink within the given agent run asynchronously with the events staged in the channel. Flume中,数据流的单位称为事件(event) 它具有一个字节的有效载荷,还可以携带一系列的字符串属性. Flume中Agent是一个JVM进程 Agent由一系列的组件组成,通过这些组件,将事件从外部数据源传到另一个位置 Flume源端部分(source)消费着来至外部数据源的事件,例如web服务端的 外部数据源需要以目标flume源端能够识别的形式发出.例如，Avro类型的flume源端可用于接收从Avro客户端发出的事件,或从其他的Avro类型的Flume发送端(sink)发出的事件 flume源端接收事件后,会将事件存入一个或者多个管道(Channel) Flume的管道存储所源端接收到的事件,直到它Flume发送端消费了这些事件.File Channel就是一个例子,它将事件备份在本地文件系统中 flume的发送端消费着Channel中的事件,并把它推送HDFS等外部仓库中(通过HDFS类型的Sink发送) Agent的三个组件 Source 数据源 Channel 通道,缓冲 数据存在哪里 Sink 数据输出到哪里 下载 1wget http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2.tar.gz 配置 所有的配置都参考官网的配置,只要配置agent,source,channel,sink即可 http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#setting-up-an-agent 其中所有的粗体都是必须要配置的,其他普通的属性是可选的 样例 为了便于配置,可以先把这个模板copy到notepad,根据需要改一下就可以 1234567891011121314151617181920212223242526272829303132333435# example.conf: A single-node Flume configuration# Name the components on this agent# agent a1的 source 的名字设置为r1a1.sources = r1 # agent a1的 sinks 的名字设置为k1a1.sinks = k1# agent a1的 channels 的名字设置为c1a1.channels = c1# Describe/configure the source# agent a1的 source r1 的类型设置为netcata1.sources.r1.type = netcat# agent a1的 source r1 的绑定的host和porta1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sink# agent a1的 sinks k1 的类型为logger类型,也就是控制台输出a1.sinks.k1.type = loggerc# Use a channel which buffers events in memory# agent a1的 channels c1 的类型为memory类型,也就是基于内存进行缓冲,所以flume对内存还是由一定要求的a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel# agent a1的 source,channel,sink如何连接# 这里配置了agent a1的source r1的channel为c1# 也就是说,事件从在被sink消费之前被保存在c1的channel中a1.sources.r1.channels = c1# 这里配置了agent a1的sink k1的channel为c1,# 也就是说agent a1的sink k1要从c1 channel消费事件a1.sinks.k1.channel = c1 参数说明 a1 这个是agent的名字,启动flume必须指定agent的名字,独一无二的即可,因为agent是一个独立jvm进程 netcat样例 netcat.conf 12345678910111213141516171819myConsoleAgent.sources = source1myConsoleAgent.sinks = sink1myConsoleAgent.channels = channel1# 数据从哪里来myConsoleAgent.sources.source1.type = netcatmyConsoleAgent.sources.source1.bind = localhostmyConsoleAgent.sources.source1.port = 44444# 输出到哪里去myConsoleAgent.sinks.sink1.type = logger# 读取数据之后先放到哪里myConsoleAgent.channels.channel1.type = memory# 将source,channel,sink连接起来myConsoleAgent.sources.source1.channels = channel1myConsoleAgent.sinks.sink1.channel = channel1 启动agent 1234$FLUME_HOME/bin/flume-ng \\--conf $FLUME_HOME \\--conf-file ./netcat.conf \\--name myConsoleAgent exec 命令行输入 exec.conf 1234567891011121314151617181920212223myConsoleAgent.sources = source1myConsoleAgent.sinks = sink1myConsoleAgent.channels = channel1# 数据从哪里来myConsoleAgent.sources.source1.type = execmyConsoleAgent.sources.source1.command = tail -F /usr/software/flume-1.6.0-cdh5.16.2/input/exec.log # 输出到哪里去myConsoleAgent.sinks.sink1.type = hdfsmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/execmyConsoleAgent.sinks.sink1.hdfs.batchSize = 10myConsoleAgent.sinks.sink1.hdfs.fileType = DataStream myConsoleAgent.sinks.sink1.writeFormat = Text # 读取数据之后先放到哪里myConsoleAgent.channels.channel1.type = memory# 将source,channel,sink连接起来myConsoleAgent.sources.source1.channels = channel1myConsoleAgent.sinks.sink1.channel = channel1 启动agent 1234$FLUME_HOME/bin/flume-ng \\--conf $FLUME_HOME \\--conf-file ./exec.conf \\--name myConsoleAgent spooldir 监听文件夹 spool.conf 12345678910111213141516171819202122232425262728myConsoleAgent.sources = source1myConsoleAgent.sinks = sink1myConsoleAgent.channels = channel1# 数据从哪里来myConsoleAgent.sources.source1.type = spooldirmyConsoleAgent.sources.source1.spoolDir = /usr/software/flume-1.6.0-cdh5.16.2/inputmemory.sources.source1.includePattern = *.log# 输出到哪里去myConsoleAgent.sinks.sink1.type = hdfsmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/spool/%Y%m%d%H%MmyConsoleAgent.sinks.sink1.hdfs.batchSize = 10myConsoleAgent.sinks.sink1.hdfs.fileType = DataStream myConsoleAgent.sinks.sink1.writeFormat = Text myConsoleAgent.sinks.sink1.hdfs.rollInterval=30myConsoleAgent.sinks.sink1.hdfs.rollSize=1024myConsoleAgent.sinks.sink1.hdfs.hdfs.rollCount=100myConsoleAgent.sinks.sink1.hdfs.useLocalTimeStamp =true# 读取数据之后先放到哪里myConsoleAgent.channels.channel1.type = memory# 将source,channel,sink连接起来myConsoleAgent.sources.source1.channels = channel1myConsoleAgent.sinks.sink1.channel = channel1 启动agent 1234$FLUME_HOME/bin/flume-ng \\--conf $FLUME_HOME \\--conf-file ./spool.conf \\--name myConsoleAgent taildir 监听文件夹(最常用的) 这个监听最常用,也是必须掌握的,因为spoolDir并没有偏移量的概念,使用有很大的局限性 spooldir的缺点 每次采集完成之后,会在文件结尾设置Complete的后缀,而且之后不能再使用相同的文件名,否则报错 采集完之后的文件不能再次写入,否则报错 而taildir很好的使用了偏移量的概念,记录在一个json文件中,可以实现断点还原 taildir.conf 123456789101112131415161718192021222324252627282930313233343536myConsoleAgent.sources = source1myConsoleAgent.sinks = sink1myConsoleAgent.channels = channel1# 数据从哪里来myConsoleAgent.sources.source1.type = TAILDIRmyConsoleAgent.sources.source1.filegroups = f1 f2myConsoleAgent.sources.source1.filegroups.f1 = input/taildir/test1/hello.txtmyConsoleAgent.sources.source1.headers.f1.headerKey1 = value1myConsoleAgent.sources.source1.filegroups.f2 = input/taildir/test2/.*.logmyConsoleAgent.sources.source1.headers.f2.headerKey1 = value2-1myConsoleAgent.sources.source1.headers.f2.headerKey2 = value2-2myConsoleAgent..sources.source1.maxBatchCount = 1000myConsoleAgent.sources.source1.fileHeader = true # 输出到哪里去myConsoleAgent.sinks.sink1.type = hdfsmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/tailDir/%Y%m%d%H%MmyConsoleAgent.sinks.sink1.hdfs.batchSize = 100myConsoleAgent.sinks.sink1.hdfs.fileType = DataStream myConsoleAgent.sinks.sink1.writeFormat = Text myConsoleAgent.sinks.sink1.hdfs.rollInterval=30myConsoleAgent.sinks.sink1.hdfs.rollSize=10240myConsoleAgent.sinks.sink1.hdfs.hdfs.rollCount=100myConsoleAgent.sinks.sink1.hdfs.useLocalTimeStamp =true# 读取数据之后先放到哪里myConsoleAgent.channels.channel1.type = memory# 将source,channel,sink连接起来myConsoleAgent.sources.source1.channels = channel1myConsoleAgent.sinks.sink1.channel = channel1 启动agent 1234$FLUME_HOME/bin/flume-ng \\--conf $FLUME_HOME \\--conf-file ./taildir.conf \\--name myConsoleAgent","categories":[{"name":"Flume","slug":"Flume","permalink":"https://lurongjiang.github.io/categories/Flume/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://lurongjiang.github.io/tags/Flume/"}]},{"title":"Azkaband的安装和使用","slug":"Azkaband的安装和使用","date":"2019-04-03T06:24:04.000Z","updated":"2019-04-05T01:27:49.000Z","comments":true,"path":"2019/04/03/Azkaband的安装和使用/","link":"","permalink":"https://lurongjiang.github.io/2019/04/03/Azkaband%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","excerpt":"工作流调度框架Azkaban的安装和简单使用教程","text":"Azkaband的安装和使用 Azkaban可以设计很复杂的工作流,解决任务之间的依赖关系,还是很方便的 Azkaban是一个由LinkedIn公司开发的分布式工作流管理框架,主要是为了解决Hadoop工作之间的依赖关系.例如,我们需要有顺序地执行任务,把ETL任务的数据导入到RMBS中,以提供数据分析支持. Azkaban是一个开源的工作流管理框架 Azkaban是由LinkedIn(领英,还有一个比较出名的产品Kafka)公司创建,用来管理批处理工作流的任务调度 Azkaban提供了WebUI接口来维护工作流 Feature Compatible with any version of Hadoop 兼容Hadoop各个版本 Easy to use web UI 提供易用的WebUI Simple web and http workflow uploads web和http方式上传工作流(这个被诟病了,应该提供拖拉拽的,写了配置还要上传,确实不方便) Project workspaces Scheduling of workflows 工作流调度 Modular and pluginable 模块化,可插拔 Authentication and Authorization 提供认证和授权 Tracking of user actions Email alerts on failure and successes 任务失败和成功邮件告警 SLA alerting and auto killing SLA告警和自动结束任务 Retrying of failed jobs 任务失败重试 版本选择 2.x 基本可以不用看了 3.x 当前推荐的 模式 stand alone mode(or solo-server) 单个 web server和executor server运行在同一个进程里 数据存储的数据库是内嵌的,不需要自己安装 主要使用在小规模场景中 Multiple executor mode 多用于生产 存储的DB应该是一个主从结构的MySQL web server和executor server运行在不同的host上,这样升级和维护时,互不影响 下载和编译 Azkban不直接提供安装包,需要自己从源码编译 Azkaban是Gradle构建的,Java版本需要1.8以上 1234wget https://github.com/azkaban/azkaban/archive/3.81.0.tar.gztar -zxvf ./3.81.0.tar.gz -C /usr/softwarecd /usr/software/azkaban-3.81.0./gradlew build installDist -x test 这个需要git,如果不安装会报错Failed to apply plugin [id ‘com.cinnober.gradle.semver-git’] 所以先得安装git 1sudo yum install -y git 执行有报错….缺少g++:Could not find Linker ‘g++’ in system path 好吧在安装g++ 12sudo yum install g++sudo yum install -y gcc-c++* 部署 编译完成后会出现3个目录,一个是azkaban-solo-server,azkaban-web-server,azkaban-exec-server,分别对应了单机版的azkaban-executor-server,azkaban-web-server和多实例的azkaban-executor-server 我们只需要把他们目录下的build/distributions的压缩包拷贝出来就行 单机版 1tar -zxvf ./azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz 修改azkban的配置文件azkaban.properties,用默认也可以 1vi conf/azkaban.properties 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# Azkaban Personalization Settings# 设置azkban的名称,这个是UI首页左上角,图标右侧上方的文字azkaban.name=LRJ# UI首页左上角,图标右侧下方的文字azkaban.label=MyAzkabanazkaban.color=#FF3601azkaban.default.servlet.path=/indexweb.resource.dir=web/# 修改一下时区,免得调度时间不对default.timezone.id=Asia/Shanghai# Azkaban UserManager classuser.manager.class=azkaban.user.XmlUserManageruser.manager.xml.file=conf/azkaban-users.xml# Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projectsdatabase.type=h2h2.path=./h2h2.create.tables=true# Velocity dev modevelocity.dev.mode=false# Azkaban Jetty server properties.jetty.use.ssl=falsejetty.maxThreads=25# Web端口,端口占用的时候改一改jetty.port=8081# Azkaban Executor settingsexecutor.port=12321# mail settings# 邮件服务器设置mail.sender=mail.host=# 开启SSL设置# User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.# enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081# when this parameters set then these parameters are used to generate email links.# if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.# azkaban.webserver.external_hostname=myazkabanhost.com# azkaban.webserver.external_ssl_port=443# azkaban.webserver.external_port=8081# 全局邮箱设置job.failure.email=job.success.email=lockdown.create.projects=falsecache.directory=cache# JMX statsjetty.connector.stats=trueexecutor.connector.stats=true# Azkaban plugin settings# 插件设置azkaban.jobtype.plugin.dir=plugins/jobtypes# Number of executions to be displayedazkaban.display.execution_page_size=16azkaban.use.multiple.executors=true# Azkaban Ramp Feature Configuration#Ramp Feature Relatedazkaban.ramp.enabled=trueazkaban.ramp.status.polling.enabled=trueazkaban.ramp.status.polling.interval.min=30azkaban.ramp.status.push.interval.threshold=15azkaban.ramp.status.pull.interval.threshold=100 添加个用户名和密码,也可以使用默认的 12345678&lt;azkaban-users&gt; &lt;user groups=\"azkaban\" password=\"azkaban\" roles=\"admin\" username=\"azkaban\"/&gt; &lt;!--添加自己的用户--&gt; &lt;user groups=\"lurongjiang\" password=\"lurongjiang\" roles=\"admin\" username=\"lurongjiang\"/&gt; &lt;user password=\"metrics\" roles=\"metrics\" username=\"metrics\"/&gt; &lt;role name=\"admin\" permissions=\"ADMIN\"/&gt; &lt;role name=\"metrics\" permissions=\"METRICS\"/&gt;&lt;/azkaban-users&gt; 启动 123./bin/start-solo.sh# 查看一下jps -m 访问webUI端口就可以显示了,可以看到我们改的两个位置 输入用户名和密码就就可以进去了 使用 参考Create Flows 创建一个名字为flow20.project的文件,写入内容: 1azkaban-flow-version: 2.0 创建一个basic.flow的文件,写入内容: 12345nodes: - name: jobA type: command config: command: echo \"This is an echoed text.\" 把两个文件打个zip包 在WebUI中创建工程 上传打包好的附件 这样project就有任务了 点击运行后就有任务执行历史了 依赖型任务 依赖型可以使用dependsOn来进行关联 12345678910111213141516171819202122232425262728nodes: - name: jobC type: noop # jobC depends on jobA and jobB dependsOn: - jobA - jobB - name: jobA type: command config: command: echo \"This is an echoed text.\" - name: jobB type: command config: command: pwd - name: jobD type: command config: command: echo 'I am D job' - name: jobE type: command dependsOn: - jobC - jobD config: command: echo 'I am E' 这样就配置三个任务,其中A,B,D任务互不干扰,同时运行,jobC必须要等任务A,B执行完了才执行,E任务需要任务C,D都执行完才执行,依赖关系 注意: 文件和yml文件格式类似,但是千万不要用tab,这个不识别\\t 不同的project,需要多次创建project,同一个project,后一次上传的会覆盖掉前一次上传的 .flow文件的文件名就是project中显示的flow的name 点击job进去,可以对参数进行设置 定时调度 点击Schedule可以设置定时任务的周期, 这样就可在Scheduling下看到定时任务列表,等待下一个时钟来临就会执行 如果需要移除,可以RemoveSchedule","categories":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://lurongjiang.github.io/categories/Azkaban/"}],"tags":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://lurongjiang.github.io/tags/Azkaban/"}]},{"title":"MapReduce教程","slug":"MapReduce Tutorial","date":"2019-04-01T10:23:05.000Z","updated":"2019-04-03T07:05:09.000Z","comments":true,"path":"2019/04/01/MapReduce Tutorial/","link":"","permalink":"https://lurongjiang.github.io/2019/04/01/MapReduce%20Tutorial/","excerpt":"MapReduce官网教程文档翻译","text":"MapReduce Tutorial Overview Hadoop MapReduce是一个运行在集群上,并行处理大量数据(TB级别)的框架 MapReduce任务通常讲输入切分成多个独立的块,这些数据块被独立的map任务并行的处理 该框架会对map输出进行排序,作为reduce任务的输入 该框架负责调度任务,监视任务并重新执行失败的任务 通常,计算的节点和数据存储节点是同一个节点,也就是说,MapReduce框架和HDFS都运行在同一些列节点中.这个约束使得框架在数据已经存在的节点上有效地调度任务,从而产生跨集群的非常高的聚合带宽 MapReduce框架由一个ResourceManager,集群每个节点的NodeManager和每个应用程序的MRAppMaster组成 必须指定输入输出路径,实现指定的接口或者抽象类,覆写map和reduce方法 hadoop任务客户端提交任务和相关配置到ResouceManager,ResouceManager负责把任务/配置分发到其他的从节点,并调度和监控任务,给客户端提供任务的状态和诊断信息 hadoop stream允许用户使用任何可执行的程序来作为mapper/reducer任务 hadoop pipes工具可以使用C++ API来实现mapper/reducer Inputs and Outputs MapReduce框架只针对&lt;Key,Value&gt;键值对类型操作.也就是说,每个MapReduce任务的输入是&lt;Key,Value&gt;形式,输入也是&lt;Key,Value&gt;形式,输入输出类型可不相同 Key,Value的类型必须是可以被框架序列化的类型,因此他们必须实现Writable接口. Key的类型除了实现Writable接口之外,还需要实现WritableComparable接口,这样才能被排序 (input) &lt;k1,v1&gt; -&gt; map -&gt; &lt;k2,v2&gt; -&gt; combine -&gt; &lt;k2,v2&gt; -&gt; reduce -&gt; &lt;k3,v3&gt; (output) hadoop jar的一些参数 -files 可以使用逗号分隔,指定多个文件 -libjars 可以添加jar包到map和reduce类路径下 -archives 可以使用逗号分隔传入多个压缩包路径 MapReduce - User Interfaces 实现Mapper和Reducer接口吗,并提供map/reduce的实现是任务的核心 Mapper Mapper将输入的,Key/Value键值对类型映射成中间结果的Key/Value键值对类型 Maps是独立的任务,负责将输入转成中间结果 中间结果的类型无需和输入的类型一样 一个输入可能对应0,1,或者多个输出 每个InputSplit(由InputFormat产生)都有一个map任务 可以通过Job.setMapperClass(Class) 来传入Mapper的实现.框架将对每个键值对形式的InputSplit调用map(WritableComparable, Writable, Context) 方法.如果需要清理一些必要资源,可以覆写cleanup(Context)方法 map的输出可以通过调用context.write(WritableComparable, Writable)来收集 所有的中间结果会被框架分组,然后传给Reducer.用户使用 Job.setGroupingComparatorClass(Class)指定比较器Comparator来控制分组 Mapper的输出会被排序(sort)和打散(partitioner)分发给每一个Reducer.partitioner数目和reduce任务的数量相同.用户可以实现Partitioner接口来自定义打散规则,控制不同的Key分到对应的reduce任务中 用户可以使用Job.setCombinerClass(Class)对中间输出结果进行本地聚合,这可以减少从Mapper传到Reduce的传输量 中间结果都是以简单的 (key-len, key, value-len, value) 形式存储,也可通过Configuration设置对中间结果进行压缩 How Many Maps? map任务的通常是由输入数据的大小来决定的,也就是输入文件的块数 对于cpu轻量级任务来说,每个节点map的并行度可达300,但是一般情况下并行度在10-100之间.任务的启动需要一定的时间,所以map任务至少需要1min的执行时间 Reducer Reducer将相同key的中间结果集进行处理 reduce任务的个数是通过Job.setNumReduceTasks(int)来设置的 通过 Job.setReducerClass(Class)来设置Reducer的实现类.框架对每组&lt;key, (list of values)&gt;的输入进行调用reduce(WritableComparable, Iterable, Context) 方法进行处理,需要清理资源可以覆写cleanup(Context) Shuffle 传到Reducer的输入是经过排序后的mapper的输出.shuffle阶段,框架将通过http获取相关partition的mapper输出 Sort 排序阶段,框架将Reducer的输入进行按Key进行分组 shuffle和sort同时进行.在map输出被拉取时,他们进行合并 Secondary Sort 如果中间结果key的分组规则需要和进入reducer前的keys的分组规则不一样,那么可以通过Job.setSortComparatorClass(Class)来设置比较器.因为Job.setSortComparatorClass(Class)时用来控制中间结果的keys是怎么分组的,所以可以用这个来对值进行二次排序 Reduce reduce阶段,将对每一组&lt;key, (list of values)&gt;输入调用reduce(WritableComparable, Iterable&lt;Writable&gt;, Context)方法 reduce任务通过 Context.write(WritableComparable, Writable)将输出结果写入文件系统 输出结果并不会进行排序 How Many Reduces? 比较合理的reduce任务的个数计算公式是:0.95(或1.75)×节点数(注意,不是每个节点的最大container数) 0.95系数可以使得reduce任务在map任务的输出传输结束后同时开始运行 1.75系数可以使得计算快的节点在一批reduce任务计算结束之后开始计算第二批 reduce任务,实现负载均衡 增加reduce的数量虽然会增加负载，但是可以改善负载匀衡，降低任务失败带来的负面影响 放缩系数要比整数略小是因为要给推测性任务和失败任务预留reduce位置 Reducer NONE 如果不需要reduce任务,将reduce任务个数设置为0是合法的 这种情况下,map任务的输出会直接写入文件系统的指定输出路径FileOutputFormat.setOutputPath(Job, Path).在写入文件系统前,map的输出是进行排序的 Partitioner partitioner控制中间map输出的key的分区 可以按照key(或者key的一部分)来产生分区,默认是使用hash进行分区 分区数和reduce任务的个数相等 控制发送给reduce的任务个数 Counter Counter是一个公共基础工具,用来报告MapReduce应用的统计信息 Mapper和Reducer实现类都可以使用Counter来报告统计 Job Configuration Job就是MapReduce任务的job配置代表 一般MapReduce框架会严格按照Job的配置执行,但是有几种情况例外 某些配置参数被标记为final类型,所以是修改配置是没法达到目的的,例如1.1比例 某些配置虽然可以直接配置,但是还需要配合其他的参数一起配置才能生效 Job通常会指定Mapper,combiner(有必要的话),Partitioner,Reducer,InputFormat,OutputFormat的实现类 输入可以使用下列方式指定输入数据文件集 (FileInputFormat.setInputPaths(Job, Path…)/ FileInputFormat.addInputPath(Job, Path)) (FileInputFormat.setInputPaths(Job, String…)/ FileInputFormat.addInputPaths(Job, String) 输出可以使用(FileOutputFormat.setOutputPath(Path))来指定输出文件集 其他配置都是可选的,如Caparator的使用,将文件放置到DistributeCache,是否中间结果或者最终输出结果需要压缩,是否允许推测模式,最大任务重试次数等 可以通过Configuration.set(String, String)/ Configuration.get(String)来设置和获取任意需要的参数.但是对于大的只读数据集,还是要用DistributedCache Task Execution &amp; Environment MRAppMaster在独立的JVM中执行每个Mapper/Reducer任务(任务进程级别) 子任务继承了MRAppMaster的环境. 用户可以通过 mapreduce.{map|reduce}.java.opts 给子任务添加额外的参数 运行时非标准类库路径可以通过-Djava.library.path=&lt;&gt;指定 如果mapreduce.{map|reduce}.java.opts参数配置包含了@taskid@则在运行时被替换成taskId 显示JVM GC,JVM JMX无密代理(这样可以结合jconsole,查看内存,线程,线程垃圾回收),最大堆内存,添加其他路径到任务java.library.path的例子 123456789101112131415&lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt; -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt; &lt;value&gt; -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt; Memory Management 用户可以通过mapreduce.{map|reduce}.memory.mb指定子任务的最大虚拟内存.注意这个设置是进程级别的 注意这个参数不要大于-Xmx的参数,否则VM可能会无法启动 mapreduce.{map|reduce}.java.opt只能配置MRAppMaster的子任务.配置守护线程的需要参考 Configuring the Environment of the Hadoop Daemons map/reduce任务的性能,可能会被并发数,写入磁盘的频率影响.检查文件系统的统计报告,尤其是从map进入reduce的字节数,这参数是非常宝贵的. Map Parameters map输出的记录会被序列化到缓冲区,元数据存储在统计缓冲区 当缓冲区或者元数据超过一定的阈值,缓冲区的内容会被排序然后存储和写入磁盘 如果缓冲区一直是满状态的,map线程将被阻塞 map结束后,没有写入磁盘的map输出记录继续写入. 磁盘上所有的map输出文件段会合并成单个文件 减少写入磁盘的次数,可以减少map的次数,但是加大缓存区会压缩mapper的可用内存 Name Type Description mapreduce.task.io.sort.mb int 序列化和map输出到缓冲区的记录预排序的累计大小,单位为MB mapreduce.map.sort.spill.percent float 序列化缓冲区spill阈值比例,超过会将缓冲区内容写入磁盘 spill之后,如果在写入磁盘过程中,map的输出没有超过spill阈值,则会继续收集到spill结束 如果是spill设置为0.33,在spill到磁盘的过程,缓冲区继续会被map的输出填充,下一次spill的时候再将这期间填充的内容写到磁盘 如果spill设置为0.66,则不会触发下一次spill.也就是说,spill可以触发,但是不会阻塞 一条记录大于缓冲区的会先触发spill,而且会被spill到一个单独的文件.无论这条记录有没有定义combiner,它都会被combiner传输 Shuffle/Reduce Parameters reduce将partitioner通过http指派给自己的map输出加载到内存,并定期合并输出到磁盘. 如果中间结果是压缩输出,那么输出也是被reduce压缩的读进内存中,减少了内存的压力 Name Type Description mapreduce.task.io.soft.factor int 每次合并磁盘上段的数目.如果超过这个设置会分多次进行合并 mapreduce.reduce.merge.inmem.thresholds int 在合并写入磁盘之前,将排序后的map输出加载到内存的map输出数目.这个值通常设置很大(1000)或者直接禁用(0),因为内存合并要比磁盘合并的代价小得多.这个阈值只影响shuffle期间内存中合并的频率 mapreduce.reduce.shuffle.merge.percent float 在内存合并之前,读取map输出的内存阈值,代表着用于存储map输出在内存中的百分比.因为map的输出并不适合存储在内存,所以设置很高会知道使得获取和合并的并行度下降.相反,设置为1可以使得内存运行的reduce更快.这个参数只影响shuffle期间的内存内合并频率 mapreduce.reduce.shuffle.input.buffer.percent float 在shuffle期间,可以分配来存储map输出的内存百分比,相对于mapreduce.reduce.java.opts指定的最大堆内存.把这个值设的大一点可以存储更多的map输出,但是也应该为框架预留一些内存 mapreduce.reduce.input.buffer.percent float 相当于reduce阶段,用于存储map输出的最大堆内存的内存百分比.reduce开始的时候,map的输出被合并到磁盘,知道map输出在一定的阈值之内.默认情况下,在reduce开始之前,map的输出都会被合并到磁盘,这样才能使得reduce充分的利用到内存.对于只要内存密集型的reduce任务,应该增加这个值,减少磁盘的的往返时间 Configured Parameters 这些参数都是局部的,每个任务的 Name Type Description mapreduce.job.id String The job id mapreduce.job.jar String job.jar location in job directory mapreduce.job.local.dir String The job specific shared scratch space mapreduce.task.id String The task id mapreduce.task.attempt.id String The task attempt id mapreduce.task.is.map boolean Is this a map task mapreduce.task.partition int The id of the task within the job mapreduce.map.input.file String The filename that the map is reading from mapreduce.map.input.start long The offset of the start of the map input split mapreduce.map.input.length long The number of bytes in the map input split mapreduce.task.output.dir String The task’s temporary output directory 在流任务执行过程中,这些参数会被转化.点(.)会被转成下划线(_),所以要想在流任务的mapper/reducer中获得这些值,需要使用下划线形式. Distributing Libraries DistributedCache分布式缓存可以分发jars和本地类库给map/reduce任务使用. child-jvm总将自己的工作目录添加到java.library.path和LD_LIBRARY_PATH 缓存中的类库可以通过System.loadLibrary或者System.load Job Submission and Monitoring Job是用户任务和ResourceManager交互的主要接口 Job的提交流程包括 检查输入输出路径 计算任务的InputSplit 有必要的话,设置必要的分布式缓存 拷贝任务的jar和配置到MapReduce系统目录 提交任务到ResourceManager.监控任务状态是可选的 任务的执行记录历史存放在 mapreduce.jobhistory.intermediate-done-dir 和mapreduce.jobhistory.done-dir Job Control 对于单个MapReduce任务无法完成的任务,用户可能需要执行MapReduce任务链,才能完成.这还是非常容易的,因为任务的输出一般是存储在分布式文件系统中,所以一个任务的输出可以作为另一个任务的输入.这也就使得判断任务是否完成,不管成功或者失败,都需要用户来控制.主要有两种控制手段 Job.submit() 提交任务到集群中,立即返回 Job.waitForCompletion(boolean) 提交任务到集群中,等待其完成 Job Input InputFormat描述了MapReduce任务的输入规范 InputFormat的职责是: 校验输入是否合法 将输入逻辑切分成InputSplit实例,之后将它们发送到独立的Mapper RecordReader 实现了从符合框架逻辑的InputSplit实例收集输入的记录,提供给Mapper进行处理 默认的InputFormat是基于输入文件的总字节大小,将输入文件切分成逻辑的InputSplit实例,例如FileInputFormat的子类.然而,文件系统的blocksize只是split的上限,下限需要通过mapreduce.input.fileinputformat.split.minsize来设置 压缩文件并不一定可以被切分,如.gz文件会把完整的文件交给一个mapper来处理 InputSplit InputSplit代表了一个独立Mapper处理的输入数据 通常InputSplit是面向字节的,把面向字节转为面向记录是RecordReader的职责 FileSplit是默认的InputSplit实现,它把输入设置成mapreduce.map.input.file 属性,用于进行逻辑分割 RecordReader RecordReader负责将InputSplit的面向字节的输入转换成面向记录,提供给Mapper实现去处理每一条记录.因此RecordReader承担了从记录中提取出键值对的任务 Job Output OutputFormat描述了MapReduce输出的规范 OutputFormat的职责: 校验任务的输出,例如输出目录是否存在 RecordWriter实现可以将任务的输出写入到文件,存储在文件系统中 TextOutputFormat是默认的OutputFormat实现","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/categories/Hadoop/"},{"name":"MapReduce","slug":"Hadoop/MapReduce","permalink":"https://lurongjiang.github.io/categories/Hadoop/MapReduce/"}],"tags":[{"name":"Hadoop,MapReduce","slug":"Hadoop-MapReduce","permalink":"https://lurongjiang.github.io/tags/Hadoop-MapReduce/"}]},{"title":"Hive的一些坑","slug":"hive的一些坑","date":"2019-04-01T10:23:05.000Z","updated":"2019-04-03T07:05:09.000Z","comments":true,"path":"2019/04/01/hive的一些坑/","link":"","permalink":"https://lurongjiang.github.io/2019/04/01/hive%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/","excerpt":"总结一下在使用hive的时候遇到的一些坑","text":"Hive的一些坑 specified datastore driver(“com.mysql.jdbc.Driver”) was not found 这个是因为驱动不对,下载了个新的就行了 Unable to open a test connection to the given database. JDBC url = jdbc:mysql://hadoop001:3306/test?useSSL=true&amp;serverTimezone=GMT%2B8, username = lrj. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: 这个需要把ssl禁用了,在jdbcUrl上指定useSSL=false MetaException(message:Version information not found in metastore. ) 这个需要将hive-site.xml中的hive.metastore.schema.verification设置为false Required table missing : “VERSION“ in Catalog “” Schema “”. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable “datanucleus.autoCreateTables” 这个需要初始化一下schema,执行 schematool -dbType mysql -initSchema 之后就可以启动metastore + hiveserver2服务 12nohup hive --service metastore &gt; ~/metastore.log 2&gt;&amp;1 &amp;nohup hiveserver2 &gt; ~/hiveserver2.log 2&gt;&amp;1 &amp; 测试hiveserver2服务是否ok 1beeline 打印日志 1234567891011which: no hbase in (&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;sbin:&#x2F;mysql&#x2F;bin:&#x2F;home&#x2F;lurongjiang&#x2F;.local&#x2F;bin:&#x2F;home&#x2F;lurongjiang&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;hadoop-2.6.0-cdh5.16.2&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;hadoop-2.6.0-cdh5.16.2&#x2F;sbin:&#x2F;usr&#x2F;software&#x2F;jdk1.8.0_231&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;apache-maven-3.6.3&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;scala-2.11.12&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;hive-1.1.0-cdh5.16.2&#x2F;bin)Beeline version 1.1.0-cdh5.16.2 by Apache Hive# 查看下数据库,此时发现没连接beeline&gt; show databases;No current connection# 尝试连接数据库,只需要输入用户名就行,不需要密码beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;defaultConnecting to jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;defaultEnter username for jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;default: lrjEnter password for jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;default: Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user&#x3D;lrj, access&#x3D;EXECUTE, inode&#x3D;&quot;&#x2F;tmp&quot;:lurongjiang:supergroup:drwx java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=lrj, access=EXECUTE, inode=”/tmp”:lurongjiang:supergroup:drwx—— 这个是没权限 hadoop fs -chmod -R 777 /tmp 再次启动就ok了.","categories":[{"name":"Hive","slug":"Hive","permalink":"https://lurongjiang.github.io/categories/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://lurongjiang.github.io/tags/Hive/"}]},{"title":"Hive UDF","slug":"Hive UDF","date":"2019-03-15T03:19:04.000Z","updated":"2019-03-15T07:23:24.000Z","comments":true,"path":"2019/03/15/Hive UDF/","link":"","permalink":"https://lurongjiang.github.io/2019/03/15/Hive%20UDF/","excerpt":"Hive UDF的介绍和基本使用","text":"Hive UDF hive内置函数并不一定满足我们的业务要求,所以需要拓展,即用户自定义函数 UDF User Defined Function UDF (one-to-one) UDAF(many-to-one) UDTF(one-to-many) 创建UDF步骤 添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.cdh.version&#125;&lt;/version&gt;&lt;/dependency&gt; 创建自定义类,继承UDF","categories":[{"name":"Hive","slug":"Hive","permalink":"https://lurongjiang.github.io/categories/Hive/"}],"tags":[{"name":"hive","slug":"hive","permalink":"https://lurongjiang.github.io/tags/hive/"},{"name":"udf","slug":"udf","permalink":"https://lurongjiang.github.io/tags/udf/"},{"name":"user-defined-function","slug":"user-defined-function","permalink":"https://lurongjiang.github.io/tags/user-defined-function/"}]},{"title":"Hadoop MapReduce编程核心","slug":"Hadoop MapReduce编程核心","date":"2019-03-12T07:14:04.000Z","updated":"2019-03-15T13:01:05.000Z","comments":true,"path":"2019/03/12/Hadoop MapReduce编程核心/","link":"","permalink":"https://lurongjiang.github.io/2019/03/12/Hadoop%20MapReduce%E7%BC%96%E7%A8%8B%E6%A0%B8%E5%BF%83/","excerpt":"Hadoop MapReduce编程核心相关介绍","text":"Hadoop MapReduce编程核心 Partitioner 分区 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Partitions the key space. * * &lt;p&gt;&lt;code&gt;Partitioner&lt;/code&gt; controls the partitioning of the keys of the * intermediate map-outputs. The key (or a subset of the key) is used to derive * the partition, typically by a hash function. The total number of partitions * is the same as the number of reduce tasks for the job. Hence this controls * which of the &lt;code&gt;m&lt;/code&gt; reduce tasks the intermediate key (and hence the * record) is sent for reduction.&lt;/p&gt; * partitioner是控制中间map阶段输出结果的key的分区.key通常被hash,分发到各个分区 * 分区数一般和reduce job的个数相等, * @see Reducer */@InterfaceAudience.Public@InterfaceStability.Stablepublic interface Partitioner&lt;K2, V2&gt; extends JobConfigurable &#123; /** * Get the paritition number for a given key (hence record) given the total * number of partitions i.e. number of reduce-tasks for the job. * * &lt;p&gt;Typically a hash function on a all or a subset of the key.&lt;/p&gt; * 根据分区总数,例如reduce job个数,获取分区的编号.一般是对所有key或者key的一部分进行进行hash处理 * @param key the key to be paritioned. * @param value the entry value. * @param numPartitions the total number of partitions. * @return the partition number for the &lt;code&gt;key&lt;/code&gt;. */ int getPartition(K2 key, V2 value, int numPartitions);&#125;/*** hash分区的实现就是key取hashCode和reduce个数进行取模*/public class HashPartitioner&lt;K2, V2&gt; implements Partitioner&lt;K2, V2&gt; &#123; public void configure(JobConf job) &#123;&#125; /** Use &#123;@link Object#hashCode()&#125; to partition. */ public int getPartition(K2 key, V2 value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 需要注意的是 分区数一般和reduce job个数相等 如果分区数&lt;reduce job个数,将导致输出有很多无用的空文件 如果分区数&gt;reduce job个数,将导致有些map输出找不到hash路径,出现java.io.IOException: Illegal partition for xxx的异常 Combiner 局部汇总 Combiner是hadoop对map阶段输出结果进行本地局部聚合,提高后面reduce的效率,避免大量数据进行网络传输. 需要注意的是 并非所有的任务都适用于Combiner 求和等操作,局部聚合可以有效的提高后面reduce的效率 平均值等操作,这种并不适用,因为局部平均值和全局平均值还是有差异的","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/categories/Hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://lurongjiang.github.io/tags/hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://lurongjiang.github.io/tags/MapReduce/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-07-18T01:35:45.000Z","updated":"2018-07-19T15:45:14.000Z","comments":false,"path":"2018/07/18/hello-world/","link":"","permalink":"https://lurongjiang.github.io/2018/07/18/hello-world/","excerpt":"这是一段文章摘要，是通过 Front-Matter 的 excerpt 属性设置的。","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[{"name":"TestNest","slug":"TestNest","permalink":"https://lurongjiang.github.io/categories/TestNest/"},{"name":"test1","slug":"test1","permalink":"https://lurongjiang.github.io/categories/test1/"},{"name":"nest1","slug":"TestNest/nest1","permalink":"https://lurongjiang.github.io/categories/TestNest/nest1/"},{"name":"nest2","slug":"TestNest/nest2","permalink":"https://lurongjiang.github.io/categories/TestNest/nest2/"}],"tags":[{"name":"PlayStation","slug":"PlayStation","permalink":"https://lurongjiang.github.io/tags/PlayStation/"},{"name":"Games","slug":"Games","permalink":"https://lurongjiang.github.io/tags/Games/"}]}]}