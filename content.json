{"meta":{"title":"Blog","subtitle":"个人博客,记录成长历程","description":"个人博客,记录成长历程","author":"LRJ","url":"https://lurongjiang.github.io","root":"/"},"pages":[{"title":"categories","date":"2020-02-19T08:11:20.000Z","updated":"2020-02-19T08:12:50.443Z","comments":true,"path":"categories/index.html","permalink":"https://lurongjiang.github.io/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-02-19T08:14:30.000Z","updated":"2020-02-19T10:04:40.479Z","comments":true,"path":"about/index.html","permalink":"https://lurongjiang.github.io/about/index.html","excerpt":"","text":"关于我 姓名：LRJ家乡：贵州现居：北京GitHub: qq11221015@sina.comsina: qq11221015@sina.comQQ: 1817975066"},{"title":"tags","date":"2020-02-19T08:11:40.000Z","updated":"2020-02-19T08:13:13.730Z","comments":true,"path":"tags/index.html","permalink":"https://lurongjiang.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"SpringCloud概述","slug":"SpringCloud","date":"2019-04-05T01:25:14.000Z","updated":"2019-05-03T12:15:28.000Z","comments":true,"path":"2019/04/05/SpringCloud/","link":"","permalink":"https://lurongjiang.github.io/2019/04/05/SpringCloud/","excerpt":"SpringCloud相关介绍","text":"SpringCloud SpringCloud概述 官网 官网 主要功能 常用子项目 版本与兼容 SpringCloud的版本命名 版本命名 SpringCloud的版本,前半部分(如Hoxton,Greenwich),意思是发布列车(ReleaseTrain),以伦敦地铁的站名命名,因为SpringCloud有很多的子项目,每个项目都有自己的版本管理,按照发布顺序以A,B,C等为首字母依次命名,已经发布的版本顺序为: Angel -&gt; Brixton -&gt; Camden -&gt; Dalston -&gt; Edgware -&gt; Finchley -&gt; Greenwich -&gt; Hoxton 后半部分(如SR,SR1,SR2),意思是服务发布(ServiceRelease),即重大Bug修复 版本发布流程 SNAPSHOT -&gt; Mx -&gt; RELEASE -&gt; SRx,其中x就是一些数字序号,例如M1,M2,SR1,SR2.SNAPSHOT为快照版本(开发版本),Mx为里程碑版本,此时并不是正式版本,但是已经接近正式版,经过多个版本迭代之后,发布第一个RELEASE版本,正式版本;在RELEASE版本之后如果有重大bug修复就会发布SR版本 Hoxton SR1 CURRENT GA Reference Doc. Hoxton SNAPSHOT Reference Doc. Greenwich SR5 GA Reference Doc. Greenwich SNAPSHOT Reference Doc. SpringCloud的版本生命周期 版本发布规划 https://github.com/spring-cloud/spring-cloud-release/milestones 版本发布记录 https://github.com/spring-cloud/spring-cloud-release/releases 版本终止声明 https://spring.io/projects/spring-cloud#overview SpringBoot与SpringCloud的兼容性 版本兼容性非常重要https://spring.io/projects/spring-cloud#overview Release Train Boot Version Hoxton 2.2.x Greenwich 2.1.x Finchley 2.0.x Edgware 1.5.x Dalston 1.5.x 生产环境如何选择选择 坚决不适用非稳定版本 坚决不适用end-of-life版本 尽量使用最新版本 RELEASE版本可以观望/调研,因为是第一个正式版,并没有在生产上得以广泛应用 SR2之后可以大规模使用 版本选择 SpringCloud Hoxton SR1 SpringBoot 2.2.4.RELEASE 12345678910111213141516171819&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-dependencies --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Hoxton.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 检查项目是否能运行 1mvn clean install -U SpringCloud服务注册与发现 使得服务消费者总能找到服务提供者 Consul单机版安装 Consul下载 下载Consoule https://releases.hashicorp.com/consul/1.6.3/consul_1.6.3_linux_amd64.zip 需要的端口 Use Default Ports DNS: The DNS server (TCP and UDP) 8600 HTTP: The HTTP API (TCP Only) 8500 HTTPS: The HTTPs API disabled (8501)* gRPC: The gRPC API disabled (8502)* LAN Serf: The Serf LAN port (TCP and UDP) 8301 Wan Serf: The Serf WAN port TCP and UDP) 8302 server: Server RPC address (TCP Only) 8300 Sidecar Proxy Min: Inclusive min port number to use for automatically assigned sidecar service registrations. 21000 Sidecar Proxy Max: Inclusive max port number to use for automatically assigned sidecar service registrations. 21255 检查端口是否被占用的方法 12345678910111213Windows:# 如果没有结果说明没有被占用netstat -ano| findstr \"8500\"Linux:# 如果没有结果说明没有被占用netstat -antp |grep 8500macOS:# 如果没有结果说明没有被占用netstat -ant | grep 8500或lsof -i:8500 安装和启动 解压 1./consul agent -dev -client 0.0.0.0 严重是否成功 1./consul -v 访问Consul首页localhost:8500 启动参数 -ui 开启ui -client 让consul拥有client功能,接受服务注册;0.0.0.0允许任意ip注册,不写只能使用localhost连接 -dev 以开发模式运行consul 整合Consul 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 配置 1234567891011spring: application: # 指定注册到consul的服务名称,分隔符不能是下划线 # 如果服务发现组件是Consul,会强制转换成中划线,导致找不到服务 # 如果服务发现组件是Ribbon,则因为Ribbon的问题(把默认名称当初虚拟主机名,而虚拟主机名不能用下划线),会造成微服务之间无法调用 name: micro-service-user cloud: consul: host: 192.168.238.128 port: 8500 启动,检查consul ui的服务上线情况 Consul健康检查 添加健康检查依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置 12345management: endpoints: web: exposure: include: '*' 端点 http://localhost:8080/actuator 查看端点 http://localhost:8080/actuator/health 健康检查 添加详情配置,可以检查详细的健康情况 1234management: endpoint: health: show-details: always 简单研究一下健康检查的源码 以磁盘检查的为例 健康检查的类都继承了AbstractHealthIndicator抽象类,而AbstractHealthIndicator实现了HealthIndicator接口,所有健康检查实现类都必须实现doHealthCheck(Health.Builder builder)方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class DiskSpaceHealthIndicator extends AbstractHealthIndicator &#123; private static final Log logger = LogFactory.getLog(DiskSpaceHealthIndicator.class); private final File path; private final DataSize threshold; /** * Create a new &#123;@code DiskSpaceHealthIndicator&#125; instance. * @param path the Path used to compute the available disk space * @param threshold the minimum disk space that should be available */ public DiskSpaceHealthIndicator(File path, DataSize threshold) &#123; super(\"DiskSpace health check failed\"); this.path = path; this.threshold = threshold; &#125; @Override protected void doHealthCheck(Health.Builder builder) throws Exception &#123; //获取可用的空间字节数 long diskFreeInBytes = this.path.getUsableSpace(); //如果可用的字节数大于预留字节数阈值则认为是健康的,设置status为UP if (diskFreeInBytes &gt;= this.threshold.toBytes()) &#123; builder.up(); &#125; else &#123; //否则任务是不健康的,设置status为DOWN logger.warn(LogMessage.format(\"Free disk space below threshold. Available: %d bytes (threshold: %s)\", diskFreeInBytes, this.threshold)); builder.down(); &#125; //输出总空间,可用空间和预留阈值 builder.withDetail(\"total\", this.path.getTotalSpace()).withDetail(\"free\", diskFreeInBytes) .withDetail(\"threshold\", this.threshold.toBytes()); &#125;&#125;//这个获取可用字节数还是挺好的,直接利用了File提供的方法public long getUsableSpace() &#123; SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; sm.checkPermission(new RuntimePermission(\"getFileSystemAttributes\")); sm.checkRead(path); &#125; if (isInvalid()) &#123; return 0L; &#125; //fs是默认的文件系统FileSystem fs = DefaultFileSystem.getFileSystem(); return fs.getSpace(this, FileSystem.SPACE_USABLE);&#125; 健康检查使用了建造者模式,对于不同的健康指标非常方便,值得学习 整合Consul和SpringCloud的actuator 修改配置 1234567spring: cloud: consul: host: 192.168.238.128 port: 8500 discovery: health-check-path: /actuator/health 这样启动之后,再检查consul ui就可以发现没有红色的叉了 其他的健康检查配置 注册课程微服务到Consul 添加依赖 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置 1234567891011121314151617181920212223242526272829303132spring: datasource: url: jdbc:mysql://192.168.238.128:3306/ms?serverTimezone=GMT%2B8&amp;characterEncoding=utf8&amp;useSSL=false hikari: username: lrj password: lu11221015 driver-class-name: com.mysql.cj.jdbc.Driver# JPA配置 jpa: hibernate: ddl-auto: update show-sql: true application: name: micro-service-class cloud: consul: host: 192.168.238.128 port: 8500 discovery: health-check-path: /actuator/health# 暴露所有的actuator端点management: endpoints: web: exposure: include: '*' # 开启健康检查详细信息 endpoint: health: show-details: alwaysserver: port: 8081 重构用户微服务 12345678910111213141516171819@RestController@RequestMapping(\"user\")public class UserController &#123; @Resource private UserService userService; @Resource private DiscoveryClient discoveryClient; @GetMapping(\"&#123;id&#125;\") public Object findUserById(@PathVariable(\"id\") Integer id) &#123; return userService.findUserById(id); &#125; @GetMapping(\"discoveryTest\") public Object discoveryTest() &#123; return discoveryClient.getInstances(\"micro-service-class\"); &#125;&#125; 访问端点,可以发现不需要指定课程微服务的主机和端口就可以拿到相关信息,实现了服务发现 重构课程微服务 原来 123456789101112131415161718192021222324252627282930313233@Servicepublic class LessonServiceImpl implements LessonService &#123; @Resource private LessonRepository lessonRepository; @Resource private LessonUserRepository lessonUserRepository; @Resource private RestTemplate restTemplate; @Override public Lesson buyById(Integer id) &#123; // 1. 根据课程id查询课程 Lesson lesson = lessonRepository.findById(id).orElseThrow(() -&gt; new IllegalArgumentException(\"该课程不存在\")); //根据课程查询是否已经购买过 LessonUser lessonUser = lessonUserRepository.findByLessonId(id); if (lessonUser != null) &#123; return lesson; &#125; //todo 2.登录之后获取userId String userId = \"1\"; // 3. 如果没有购买过,查询用户余额 UserDTO userDTO = restTemplate.getForObject(\"http://localhost:8080/user/&#123;userId&#125;\", UserDTO.class, userId); if (userDTO != null &amp;&amp; userDTO.getMoney() != null &amp;&amp; userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() &lt; 0) &#123; throw new IllegalArgumentException(\"余额不足\"); &#125; //4. 购买逻辑 //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录 return lesson; &#125;&#125; 这个写死了主机地址,无法动态获取微服务路径 重构 123456789101112131415161718192021222324252627282930313233343536373839public class LessonServiceImpl implements LessonService &#123; @Resource private LessonRepository lessonRepository; @Resource private LessonUserRepository lessonUserRepository; @Resource private DiscoveryClient discoveryClient; @Resource private RestTemplate restTemplate; @Override public Lesson buyById(Integer id) &#123; // 1. 根据课程id查询课程 Lesson lesson = lessonRepository.findById(id).orElseThrow(() -&gt; new IllegalArgumentException(\"该课程不存在\")); //根据课程查询是否已经购买过 LessonUser lessonUser = lessonUserRepository.findByLessonId(id); if (lessonUser != null) &#123; return lesson; &#125; //todo 2.登录之后获取userId String userId = \"1\"; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(\"micro-service-user\"); if (instances != null &amp;&amp; !instances.isEmpty()) &#123; //todo 需要改进,如果存在多个实例,需要考虑负载均衡 ServiceInstance instance = instances.get(0); URI uri = instance.getUri(); UserDTO userDTO = restTemplate.getForObject(uri + \"/user/&#123;userId&#125;\", UserDTO.class, userId); if (userDTO != null &amp;&amp; userDTO.getMoney() != null &amp;&amp; userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() &lt; 0) &#123; throw new IllegalArgumentException(\"余额不足\"); &#125; //4. 购买逻辑 //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录 return lesson; &#125; throw new IllegalArgumentException(\"用户微服务异常,无法购买课程\"); &#125;&#125; 可以动态的获取到用户微服务的地址,请求正常 元数据 Consul是没有元数据的概念的,所以SpringCloud做了个适配,在consul下设置tags作为元数据. 元数据可以对微服务添加描述,标识,例如机房在哪里,这样可以进行就近判断,或者当就近机房不可用时才检查远程机房,当两者都不可用时才认为服务不可用等实现容灾或者跨机房 配置 12345678spring: cloud: consul: host: 192.168.238.128 port: 8500 discovery: health-check-path: /actuator/health tags: JiFang=Beijing,JiFang=Shanghai 实现机房选择 123456789101112@GetMapping(\"discoveryTest\")public Object discoveryTest() &#123; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(\"micro-service-class\"); if (instances != null) &#123; List&lt;ServiceInstance&gt; shanghaiInstances = instances.stream() .filter(s -&gt; s.getMetadata().containsKey(\"Shanghai\")).collect(Collectors.toList()); if (!shanghaiInstances.isEmpty()) &#123; return shanghaiInstances; &#125; &#125; return instances;&#125;","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://lurongjiang.github.io/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://lurongjiang.github.io/tags/SpringCloud/"}]},{"title":"MapReduce教程","slug":"MapReduce Tutorial","date":"2019-04-01T10:23:05.000Z","updated":"2019-04-03T07:05:09.000Z","comments":true,"path":"2019/04/01/MapReduce Tutorial/","link":"","permalink":"https://lurongjiang.github.io/2019/04/01/MapReduce%20Tutorial/","excerpt":"MapReduce官网教程文档翻译","text":"MapReduce Tutorial Overview Hadoop MapReduce是一个运行在集群上,并行处理大量数据(TB级别)的框架 MapReduce任务通常讲输入切分成多个独立的块,这些数据块被独立的map任务并行的处理 该框架会对map输出进行排序,作为reduce任务的输入 该框架负责调度任务,监视任务并重新执行失败的任务 通常,计算的节点和数据存储节点是同一个节点,也就是说,MapReduce框架和HDFS都运行在同一些列节点中.这个约束使得框架在数据已经存在的节点上有效地调度任务,从而产生跨集群的非常高的聚合带宽 MapReduce框架由一个ResourceManager,集群每个节点的NodeManager和每个应用程序的MRAppMaster组成 必须指定输入输出路径,实现指定的接口或者抽象类,覆写map和reduce方法 hadoop任务客户端提交任务和相关配置到ResouceManager,ResouceManager负责把任务/配置分发到其他的从节点,并调度和监控任务,给客户端提供任务的状态和诊断信息 hadoop stream允许用户使用任何可执行的程序来作为mapper/reducer任务 hadoop pipes工具可以使用C++ API来实现mapper/reducer Inputs and Outputs MapReduce框架只针对&lt;Key,Value&gt;键值对类型操作.也就是说,每个MapReduce任务的输入是&lt;Key,Value&gt;形式,输入也是&lt;Key,Value&gt;形式,输入输出类型可不相同 Key,Value的类型必须是可以被框架序列化的类型,因此他们必须实现Writable接口. Key的类型除了实现Writable接口之外,还需要实现WritableComparable接口,这样才能被排序 (input) &lt;k1,v1&gt; -&gt; map -&gt; &lt;k2,v2&gt; -&gt; combine -&gt; &lt;k2,v2&gt; -&gt; reduce -&gt; &lt;k3,v3&gt; (output) hadoop jar的一些参数 -files 可以使用逗号分隔,指定多个文件 -libjars 可以添加jar包到map和reduce类路径下 -archives 可以使用逗号分隔传入多个压缩包路径 MapReduce - User Interfaces 实现Mapper和Reducer接口吗,并提供map/reduce的实现是任务的核心 Mapper Mapper将输入的,Key/Value键值对类型映射成中间结果的Key/Value键值对类型 Maps是独立的任务,负责将输入转成中间结果 中间结果的类型无需和输入的类型一样 一个输入可能对应0,1,或者多个输出 每个InputSplit(由InputFormat产生)都有一个map任务 可以通过Job.setMapperClass(Class) 来传入Mapper的实现.框架将对每个键值对形式的InputSplit调用map(WritableComparable, Writable, Context) 方法.如果需要清理一些必要资源,可以覆写cleanup(Context)方法 map的输出可以通过调用context.write(WritableComparable, Writable)来收集 所有的中间结果会被框架分组,然后传给Reducer.用户使用 Job.setGroupingComparatorClass(Class)指定比较器Comparator来控制分组 Mapper的输出会被排序(sort)和打散(partitioner)分发给每一个Reducer.partitioner数目和reduce任务的数量相同.用户可以实现Partitioner接口来自定义打散规则,控制不同的Key分到对应的reduce任务中 用户可以使用Job.setCombinerClass(Class)对中间输出结果进行本地聚合,这可以减少从Mapper传到Reduce的传输量 中间结果都是以简单的 (key-len, key, value-len, value) 形式存储,也可通过Configuration设置对中间结果进行压缩 How Many Maps? map任务的通常是由输入数据的大小来决定的,也就是输入文件的块数 对于cpu轻量级任务来说,每个节点map的并行度可达300,但是一般情况下并行度在10-100之间.任务的启动需要一定的时间,所以map任务至少需要1min的执行时间 Reducer Reducer将相同key的中间结果集进行处理 reduce任务的个数是通过Job.setNumReduceTasks(int)来设置的 通过 Job.setReducerClass(Class)来设置Reducer的实现类.框架对每组&lt;key, (list of values)&gt;的输入进行调用reduce(WritableComparable, Iterable, Context) 方法进行处理,需要清理资源可以覆写cleanup(Context) Shuffle 传到Reducer的输入是经过排序后的mapper的输出.shuffle阶段,框架将通过http获取相关partition的mapper输出 Sort 排序阶段,框架将Reducer的输入进行按Key进行分组 shuffle和sort同时进行.在map输出被拉取时,他们进行合并 Secondary Sort 如果中间结果key的分组规则需要和进入reducer前的keys的分组规则不一样,那么可以通过Job.setSortComparatorClass(Class)来设置比较器.因为Job.setSortComparatorClass(Class)时用来控制中间结果的keys是怎么分组的,所以可以用这个来对值进行二次排序 Reduce reduce阶段,将对每一组&lt;key, (list of values)&gt;输入调用reduce(WritableComparable, Iterable&lt;Writable&gt;, Context)方法 reduce任务通过 Context.write(WritableComparable, Writable)将输出结果写入文件系统 输出结果并不会进行排序 How Many Reduces? 比较合理的reduce任务的个数计算公式是:0.95(或1.75)×节点数(注意,不是每个节点的最大container数) 0.95系数可以使得reduce任务在map任务的输出传输结束后同时开始运行 1.75系数可以使得计算快的节点在一批reduce任务计算结束之后开始计算第二批 reduce任务,实现负载均衡 增加reduce的数量虽然会增加负载，但是可以改善负载匀衡，降低任务失败带来的负面影响 放缩系数要比整数略小是因为要给推测性任务和失败任务预留reduce位置 Reducer NONE 如果不需要reduce任务,将reduce任务个数设置为0是合法的 这种情况下,map任务的输出会直接写入文件系统的指定输出路径FileOutputFormat.setOutputPath(Job, Path).在写入文件系统前,map的输出是进行排序的 Partitioner partitioner控制中间map输出的key的分区 可以按照key(或者key的一部分)来产生分区,默认是使用hash进行分区 分区数和reduce任务的个数相等 控制发送给reduce的任务个数 Counter Counter是一个公共基础工具,用来报告MapReduce应用的统计信息 Mapper和Reducer实现类都可以使用Counter来报告统计 Job Configuration Job就是MapReduce任务的job配置代表 一般MapReduce框架会严格按照Job的配置执行,但是有几种情况例外 某些配置参数被标记为final类型,所以是修改配置是没法达到目的的,例如1.1比例 某些配置虽然可以直接配置,但是还需要配合其他的参数一起配置才能生效 Job通常会指定Mapper,combiner(有必要的话),Partitioner,Reducer,InputFormat,OutputFormat的实现类 输入可以使用下列方式指定输入数据文件集 (FileInputFormat.setInputPaths(Job, Path…)/ FileInputFormat.addInputPath(Job, Path)) (FileInputFormat.setInputPaths(Job, String…)/ FileInputFormat.addInputPaths(Job, String) 输出可以使用(FileOutputFormat.setOutputPath(Path))来指定输出文件集 其他配置都是可选的,如Caparator的使用,将文件放置到DistributeCache,是否中间结果或者最终输出结果需要压缩,是否允许推测模式,最大任务重试次数等 可以通过Configuration.set(String, String)/ Configuration.get(String)来设置和获取任意需要的参数.但是对于大的只读数据集,还是要用DistributedCache Task Execution &amp; Environment MRAppMaster在独立的JVM中执行每个Mapper/Reducer任务(任务进程级别) 子任务继承了MRAppMaster的环境. 用户可以通过 mapreduce.{map|reduce}.java.opts 给子任务添加额外的参数 运行时非标准类库路径可以通过-Djava.library.path=&lt;&gt;指定 如果mapreduce.{map|reduce}.java.opts参数配置包含了@taskid@则在运行时被替换成taskId 显示JVM GC,JVM JMX无密代理(这样可以结合jconsole,查看内存,线程,线程垃圾回收),最大堆内存,添加其他路径到任务java.library.path的例子 123456789101112131415&lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt; -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt; &lt;value&gt; -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt; Memory Management 用户可以通过mapreduce.{map|reduce}.memory.mb指定子任务的最大虚拟内存.注意这个设置是进程级别的 注意这个参数不要大于-Xmx的参数,否则VM可能会无法启动 mapreduce.{map|reduce}.java.opt只能配置MRAppMaster的子任务.配置守护线程的需要参考 Configuring the Environment of the Hadoop Daemons map/reduce任务的性能,可能会被并发数,写入磁盘的频率影响.检查文件系统的统计报告,尤其是从map进入reduce的字节数,这参数是非常宝贵的. Map Parameters map输出的记录会被序列化到缓冲区,元数据存储在统计缓冲区 当缓冲区或者元数据超过一定的阈值,缓冲区的内容会被排序然后存储和写入磁盘 如果缓冲区一直是满状态的,map线程将被阻塞 map结束后,没有写入磁盘的map输出记录继续写入. 磁盘上所有的map输出文件段会合并成单个文件 减少写入磁盘的次数,可以减少map的次数,但是加大缓存区会压缩mapper的可用内存 Name Type Description mapreduce.task.io.sort.mb int 序列化和map输出到缓冲区的记录预排序的累计大小,单位为MB mapreduce.map.sort.spill.percent float 序列化缓冲区spill阈值比例,超过会将缓冲区内容写入磁盘 spill之后,如果在写入磁盘过程中,map的输出没有超过spill阈值,则会继续收集到spill结束 如果是spill设置为0.33,在spill到磁盘的过程,缓冲区继续会被map的输出填充,下一次spill的时候再将这期间填充的内容写到磁盘 如果spill设置为0.66,则不会触发下一次spill.也就是说,spill可以触发,但是不会阻塞 一条记录大于缓冲区的会先触发spill,而且会被spill到一个单独的文件.无论这条记录有没有定义combiner,它都会被combiner传输 Shuffle/Reduce Parameters reduce将partitioner通过http指派给自己的map输出加载到内存,并定期合并输出到磁盘. 如果中间结果是压缩输出,那么输出也是被reduce压缩的读进内存中,减少了内存的压力 Name Type Description mapreduce.task.io.soft.factor int 每次合并磁盘上段的数目.如果超过这个设置会分多次进行合并 mapreduce.reduce.merge.inmem.thresholds int 在合并写入磁盘之前,将排序后的map输出加载到内存的map输出数目.这个值通常设置很大(1000)或者直接禁用(0),因为内存合并要比磁盘合并的代价小得多.这个阈值只影响shuffle期间内存中合并的频率 mapreduce.reduce.shuffle.merge.percent float 在内存合并之前,读取map输出的内存阈值,代表着用于存储map输出在内存中的百分比.因为map的输出并不适合存储在内存,所以设置很高会知道使得获取和合并的并行度下降.相反,设置为1可以使得内存运行的reduce更快.这个参数只影响shuffle期间的内存内合并频率 mapreduce.reduce.shuffle.input.buffer.percent float 在shuffle期间,可以分配来存储map输出的内存百分比,相对于mapreduce.reduce.java.opts指定的最大堆内存.把这个值设的大一点可以存储更多的map输出,但是也应该为框架预留一些内存 mapreduce.reduce.input.buffer.percent float 相当于reduce阶段,用于存储map输出的最大堆内存的内存百分比.reduce开始的时候,map的输出被合并到磁盘,知道map输出在一定的阈值之内.默认情况下,在reduce开始之前,map的输出都会被合并到磁盘,这样才能使得reduce充分的利用到内存.对于只要内存密集型的reduce任务,应该增加这个值,减少磁盘的的往返时间 Configured Parameters 这些参数都是局部的,每个任务的 Name Type Description mapreduce.job.id String The job id mapreduce.job.jar String job.jar location in job directory mapreduce.job.local.dir String The job specific shared scratch space mapreduce.task.id String The task id mapreduce.task.attempt.id String The task attempt id mapreduce.task.is.map boolean Is this a map task mapreduce.task.partition int The id of the task within the job mapreduce.map.input.file String The filename that the map is reading from mapreduce.map.input.start long The offset of the start of the map input split mapreduce.map.input.length long The number of bytes in the map input split mapreduce.task.output.dir String The task’s temporary output directory 在流任务执行过程中,这些参数会被转化.点(.)会被转成下划线(_),所以要想在流任务的mapper/reducer中获得这些值,需要使用下划线形式. Distributing Libraries DistributedCache分布式缓存可以分发jars和本地类库给map/reduce任务使用. child-jvm总将自己的工作目录添加到java.library.path和LD_LIBRARY_PATH 缓存中的类库可以通过System.loadLibrary或者System.load Job Submission and Monitoring Job是用户任务和ResourceManager交互的主要接口 Job的提交流程包括 检查输入输出路径 计算任务的InputSplit 有必要的话,设置必要的分布式缓存 拷贝任务的jar和配置到MapReduce系统目录 提交任务到ResourceManager.监控任务状态是可选的 任务的执行记录历史存放在 mapreduce.jobhistory.intermediate-done-dir 和mapreduce.jobhistory.done-dir Job Control 对于单个MapReduce任务无法完成的任务,用户可能需要执行MapReduce任务链,才能完成.这还是非常容易的,因为任务的输出一般是存储在分布式文件系统中,所以一个任务的输出可以作为另一个任务的输入.这也就使得判断任务是否完成,不管成功或者失败,都需要用户来控制.主要有两种控制手段 Job.submit() 提交任务到集群中,立即返回 Job.waitForCompletion(boolean) 提交任务到集群中,等待其完成 Job Input InputFormat描述了MapReduce任务的输入规范 InputFormat的职责是: 校验输入是否合法 将输入逻辑切分成InputSplit实例,之后将它们发送到独立的Mapper RecordReader 实现了从符合框架逻辑的InputSplit实例收集输入的记录,提供给Mapper进行处理 默认的InputFormat是基于输入文件的总字节大小,将输入文件切分成逻辑的InputSplit实例,例如FileInputFormat的子类.然而,文件系统的blocksize只是split的上限,下限需要通过mapreduce.input.fileinputformat.split.minsize来设置 压缩文件并不一定可以被切分,如.gz文件会把完整的文件交给一个mapper来处理 InputSplit InputSplit代表了一个独立Mapper处理的输入数据 通常InputSplit是面向字节的,把面向字节转为面向记录是RecordReader的职责 FileSplit是默认的InputSplit实现,它把输入设置成mapreduce.map.input.file 属性,用于进行逻辑分割 RecordReader RecordReader负责将InputSplit的面向字节的输入转换成面向记录,提供给Mapper实现去处理每一条记录.因此RecordReader承担了从记录中提取出键值对的任务 Job Output OutputFormat描述了MapReduce输出的规范 OutputFormat的职责: 校验任务的输出,例如输出目录是否存在 RecordWriter实现可以将任务的输出写入到文件,存储在文件系统中 TextOutputFormat是默认的OutputFormat实现","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/categories/Hadoop/"},{"name":"MapReduce","slug":"Hadoop/MapReduce","permalink":"https://lurongjiang.github.io/categories/Hadoop/MapReduce/"}],"tags":[{"name":"Hadoop,MapReduce","slug":"Hadoop-MapReduce","permalink":"https://lurongjiang.github.io/tags/Hadoop-MapReduce/"}]},{"title":"Hive的一些坑","slug":"hive的一些坑","date":"2019-04-01T10:23:05.000Z","updated":"2019-04-03T07:05:09.000Z","comments":true,"path":"2019/04/01/hive的一些坑/","link":"","permalink":"https://lurongjiang.github.io/2019/04/01/hive%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/","excerpt":"总结一下在使用hive的时候遇到的一些坑","text":"Hive的一些坑 specified datastore driver(“com.mysql.jdbc.Driver”) was not found 这个是因为驱动不对,下载了个新的就行了 Unable to open a test connection to the given database. JDBC url = jdbc:mysql://hadoop001:3306/test?useSSL=true&amp;serverTimezone=GMT%2B8, username = lrj. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: 这个需要把ssl禁用了,在jdbcUrl上指定useSSL=false MetaException(message:Version information not found in metastore. ) 这个需要将hive-site.xml中的hive.metastore.schema.verification设置为false Required table missing : “VERSION“ in Catalog “” Schema “”. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable “datanucleus.autoCreateTables” 这个需要初始化一下schema,执行 schematool -dbType mysql -initSchema 之后就可以启动metastore + hiveserver2服务 12nohup hive --service metastore &gt; ~/metastore.log 2&gt;&amp;1 &amp;nohup hiveserver2 &gt; ~/hiveserver2.log 2&gt;&amp;1 &amp; 测试hiveserver2服务是否ok 1beeline 打印日志 1234567891011which: no hbase in (&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;sbin:&#x2F;mysql&#x2F;bin:&#x2F;home&#x2F;lurongjiang&#x2F;.local&#x2F;bin:&#x2F;home&#x2F;lurongjiang&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;hadoop-2.6.0-cdh5.16.2&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;hadoop-2.6.0-cdh5.16.2&#x2F;sbin:&#x2F;usr&#x2F;software&#x2F;jdk1.8.0_231&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;apache-maven-3.6.3&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;scala-2.11.12&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;hive-1.1.0-cdh5.16.2&#x2F;bin)Beeline version 1.1.0-cdh5.16.2 by Apache Hive# 查看下数据库,此时发现没连接beeline&gt; show databases;No current connection# 尝试连接数据库,只需要输入用户名就行,不需要密码beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;defaultConnecting to jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;defaultEnter username for jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;default: lrjEnter password for jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;default: Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user&#x3D;lrj, access&#x3D;EXECUTE, inode&#x3D;&quot;&#x2F;tmp&quot;:lurongjiang:supergroup:drwx java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=lrj, access=EXECUTE, inode=”/tmp”:lurongjiang:supergroup:drwx—— 这个是没权限 hadoop fs -chmod -R 777 /tmp 再次启动就ok了.","categories":[{"name":"Hive","slug":"Hive","permalink":"https://lurongjiang.github.io/categories/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://lurongjiang.github.io/tags/Hive/"}]},{"title":"Hive UDF","slug":"Hive UDF","date":"2019-03-15T03:19:04.000Z","updated":"2019-03-15T07:23:24.000Z","comments":true,"path":"2019/03/15/Hive UDF/","link":"","permalink":"https://lurongjiang.github.io/2019/03/15/Hive%20UDF/","excerpt":"Hive UDF的介绍和基本使用","text":"Hive UDF hive内置函数并不一定满足我们的业务要求,所以需要拓展,即用户自定义函数 UDF User Defined Function UDF (one-to-one) UDAF(many-to-one) UDTF(one-to-many) 创建UDF步骤 添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.cdh.version&#125;&lt;/version&gt;&lt;/dependency&gt; 创建自定义类,继承UDF","categories":[{"name":"Hive","slug":"Hive","permalink":"https://lurongjiang.github.io/categories/Hive/"}],"tags":[{"name":"hive","slug":"hive","permalink":"https://lurongjiang.github.io/tags/hive/"},{"name":"udf","slug":"udf","permalink":"https://lurongjiang.github.io/tags/udf/"},{"name":"user-defined-function","slug":"user-defined-function","permalink":"https://lurongjiang.github.io/tags/user-defined-function/"}]},{"title":"Hadoop MapReduce编程核心","slug":"Hadoop MapReduce编程核心","date":"2019-03-12T07:14:04.000Z","updated":"2019-03-15T13:01:05.000Z","comments":true,"path":"2019/03/12/Hadoop MapReduce编程核心/","link":"","permalink":"https://lurongjiang.github.io/2019/03/12/Hadoop%20MapReduce%E7%BC%96%E7%A8%8B%E6%A0%B8%E5%BF%83/","excerpt":"Hadoop MapReduce编程核心相关介绍","text":"Hadoop MapReduce编程核心 Partitioner 分区 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Partitions the key space. * * &lt;p&gt;&lt;code&gt;Partitioner&lt;/code&gt; controls the partitioning of the keys of the * intermediate map-outputs. The key (or a subset of the key) is used to derive * the partition, typically by a hash function. The total number of partitions * is the same as the number of reduce tasks for the job. Hence this controls * which of the &lt;code&gt;m&lt;/code&gt; reduce tasks the intermediate key (and hence the * record) is sent for reduction.&lt;/p&gt; * partitioner是控制中间map阶段输出结果的key的分区.key通常被hash,分发到各个分区 * 分区数一般和reduce job的个数相等, * @see Reducer */@InterfaceAudience.Public@InterfaceStability.Stablepublic interface Partitioner&lt;K2, V2&gt; extends JobConfigurable &#123; /** * Get the paritition number for a given key (hence record) given the total * number of partitions i.e. number of reduce-tasks for the job. * * &lt;p&gt;Typically a hash function on a all or a subset of the key.&lt;/p&gt; * 根据分区总数,例如reduce job个数,获取分区的编号.一般是对所有key或者key的一部分进行进行hash处理 * @param key the key to be paritioned. * @param value the entry value. * @param numPartitions the total number of partitions. * @return the partition number for the &lt;code&gt;key&lt;/code&gt;. */ int getPartition(K2 key, V2 value, int numPartitions);&#125;/*** hash分区的实现就是key取hashCode和reduce个数进行取模*/public class HashPartitioner&lt;K2, V2&gt; implements Partitioner&lt;K2, V2&gt; &#123; public void configure(JobConf job) &#123;&#125; /** Use &#123;@link Object#hashCode()&#125; to partition. */ public int getPartition(K2 key, V2 value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 需要注意的是 分区数一般和reduce job个数相等 如果分区数&lt;reduce job个数,将导致输出有很多无用的空文件 如果分区数&gt;reduce job个数,将导致有些map输出找不到hash路径,出现java.io.IOException: Illegal partition for xxx的异常 Combiner 局部汇总 Combiner是hadoop对map阶段输出结果进行本地局部聚合,提高后面reduce的效率,避免大量数据进行网络传输. 需要注意的是 并非所有的任务都适用于Combiner 求和等操作,局部聚合可以有效的提高后面reduce的效率 平均值等操作,这种并不适用,因为局部平均值和全局平均值还是有差异的","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/categories/Hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://lurongjiang.github.io/tags/hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://lurongjiang.github.io/tags/MapReduce/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-07-18T01:35:45.000Z","updated":"2018-07-19T15:45:14.000Z","comments":false,"path":"2018/07/18/hello-world/","link":"","permalink":"https://lurongjiang.github.io/2018/07/18/hello-world/","excerpt":"这是一段文章摘要，是通过 Front-Matter 的 excerpt 属性设置的。","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[{"name":"TestNest","slug":"TestNest","permalink":"https://lurongjiang.github.io/categories/TestNest/"},{"name":"test1","slug":"test1","permalink":"https://lurongjiang.github.io/categories/test1/"},{"name":"nest1","slug":"TestNest/nest1","permalink":"https://lurongjiang.github.io/categories/TestNest/nest1/"},{"name":"nest2","slug":"TestNest/nest2","permalink":"https://lurongjiang.github.io/categories/TestNest/nest2/"}],"tags":[{"name":"PlayStation","slug":"PlayStation","permalink":"https://lurongjiang.github.io/tags/PlayStation/"},{"name":"Games","slug":"Games","permalink":"https://lurongjiang.github.io/tags/Games/"}]}]}