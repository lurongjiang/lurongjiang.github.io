{"meta":{"title":"Blog","subtitle":"个人博客,记录成长历程","description":"个人博客,记录成长历程","author":"LRJ","url":"https://lurongjiang.github.io","root":"/"},"pages":[{"title":"tags","date":"2020-02-19T08:11:40.000Z","updated":"2020-02-19T08:13:13.730Z","comments":true,"path":"tags/index.html","permalink":"https://lurongjiang.github.io/tags/index.html","excerpt":"","text":""},{"title":"about","date":"2020-02-19T08:14:30.000Z","updated":"2020-02-19T10:04:40.479Z","comments":true,"path":"about/index.html","permalink":"https://lurongjiang.github.io/about/index.html","excerpt":"","text":"关于我 姓名：LRJ家乡：贵州现居：北京GitHub: qq11221015@sina.comsina: qq11221015@sina.comQQ: 1817975066"},{"title":"categories","date":"2020-02-19T08:11:20.000Z","updated":"2020-02-19T08:12:50.443Z","comments":true,"path":"categories/index.html","permalink":"https://lurongjiang.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Hive UDF","slug":"Hive UDF","date":"2019-03-15T03:19:04.000Z","updated":"2020-02-19T10:23:51.231Z","comments":true,"path":"2019/03/15/Hive UDF/","link":"","permalink":"https://lurongjiang.github.io/2019/03/15/Hive%20UDF/","excerpt":"Hive UDF的介绍和基本使用","text":"Hive UDF hive内置函数并不一定满足我们的业务要求,所以需要拓展,即用户自定义函数 UDF User Defined Function UDF (one-to-one) UDAF(many-to-one) UDTF(one-to-many) 创建UDF步骤 添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.cdh.version&#125;&lt;/version&gt;&lt;/dependency&gt; 创建自定义类,继承UDF","categories":[{"name":"Hive","slug":"Hive","permalink":"https://lurongjiang.github.io/categories/Hive/"}],"tags":[{"name":"hive","slug":"hive","permalink":"https://lurongjiang.github.io/tags/hive/"},{"name":"udf","slug":"udf","permalink":"https://lurongjiang.github.io/tags/udf/"},{"name":"user-defined-function","slug":"user-defined-function","permalink":"https://lurongjiang.github.io/tags/user-defined-function/"}]},{"title":"Hadoop MapReduce编程核心","slug":"Hadoop MapReduce编程核心","date":"2019-03-12T07:14:04.000Z","updated":"2020-02-19T10:23:21.879Z","comments":true,"path":"2019/03/12/Hadoop MapReduce编程核心/","link":"","permalink":"https://lurongjiang.github.io/2019/03/12/Hadoop%20MapReduce%E7%BC%96%E7%A8%8B%E6%A0%B8%E5%BF%83/","excerpt":"Hadoop MapReduce编程核心相关介绍","text":"Hadoop MapReduce编程核心 Partitioner 分区 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Partitions the key space. * * &lt;p&gt;&lt;code&gt;Partitioner&lt;/code&gt; controls the partitioning of the keys of the * intermediate map-outputs. The key (or a subset of the key) is used to derive * the partition, typically by a hash function. The total number of partitions * is the same as the number of reduce tasks for the job. Hence this controls * which of the &lt;code&gt;m&lt;/code&gt; reduce tasks the intermediate key (and hence the * record) is sent for reduction.&lt;/p&gt; * partitioner是控制中间map阶段输出结果的key的分区.key通常被hash,分发到各个分区 * 分区数一般和reduce job的个数相等, * @see Reducer */@InterfaceAudience.Public@InterfaceStability.Stablepublic interface Partitioner&lt;K2, V2&gt; extends JobConfigurable &#123; /** * Get the paritition number for a given key (hence record) given the total * number of partitions i.e. number of reduce-tasks for the job. * * &lt;p&gt;Typically a hash function on a all or a subset of the key.&lt;/p&gt; * 根据分区总数,例如reduce job个数,获取分区的编号.一般是对所有key或者key的一部分进行进行hash处理 * @param key the key to be paritioned. * @param value the entry value. * @param numPartitions the total number of partitions. * @return the partition number for the &lt;code&gt;key&lt;/code&gt;. */ int getPartition(K2 key, V2 value, int numPartitions);&#125;/*** hash分区的实现就是key取hashCode和reduce个数进行取模*/public class HashPartitioner&lt;K2, V2&gt; implements Partitioner&lt;K2, V2&gt; &#123; public void configure(JobConf job) &#123;&#125; /** Use &#123;@link Object#hashCode()&#125; to partition. */ public int getPartition(K2 key, V2 value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 需要注意的是 分区数一般和reduce job个数相等 如果分区数&lt;reduce job个数,将导致输出有很多无用的空文件 如果分区数&gt;reduce job个数,将导致有些map输出找不到hash路径,出现java.io.IOException: Illegal partition for xxx的异常 Combiner 局部汇总 Combiner是hadoop对map阶段输出结果进行本地局部聚合,提高后面reduce的效率,避免大量数据进行网络传输. 需要注意的是 并非所有的任务都适用于Combiner 求和等操作,局部聚合可以有效的提高后面reduce的效率 平均值等操作,这种并不适用,因为局部平均值和全局平均值还是有差异的","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/categories/Hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://lurongjiang.github.io/tags/hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://lurongjiang.github.io/tags/MapReduce/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-07-18T01:35:45.000Z","updated":"2018-07-19T15:45:14.000Z","comments":false,"path":"2018/07/18/hello-world/","link":"","permalink":"https://lurongjiang.github.io/2018/07/18/hello-world/","excerpt":"这是一段文章摘要，是通过 Front-Matter 的 excerpt 属性设置的。","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[{"name":"TestNest","slug":"TestNest","permalink":"https://lurongjiang.github.io/categories/TestNest/"},{"name":"test1","slug":"test1","permalink":"https://lurongjiang.github.io/categories/test1/"},{"name":"nest1","slug":"TestNest/nest1","permalink":"https://lurongjiang.github.io/categories/TestNest/nest1/"},{"name":"nest2","slug":"TestNest/nest2","permalink":"https://lurongjiang.github.io/categories/TestNest/nest2/"}],"tags":[{"name":"PlayStation","slug":"PlayStation","permalink":"https://lurongjiang.github.io/tags/PlayStation/"},{"name":"Games","slug":"Games","permalink":"https://lurongjiang.github.io/tags/Games/"}]}]}