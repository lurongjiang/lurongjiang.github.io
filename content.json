{"meta":{"title":"Blog","subtitle":"个人博客,记录成长历程","description":"个人博客,记录成长历程","author":"LRJ","url":"https://lurongjiang.github.io","root":"/"},"pages":[{"title":"about","date":"2020-02-19T08:14:30.000Z","updated":"2020-02-19T10:04:40.479Z","comments":true,"path":"about/index.html","permalink":"https://lurongjiang.github.io/about/index.html","excerpt":"","text":"关于我 姓名：LRJ家乡：贵州现居：北京GitHub: qq11221015@sina.comsina: qq11221015@sina.comQQ: 1817975066"},{"title":"categories","date":"2020-02-19T08:11:20.000Z","updated":"2020-02-19T08:12:50.443Z","comments":true,"path":"categories/index.html","permalink":"https://lurongjiang.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-02-19T08:11:40.000Z","updated":"2020-02-19T08:13:13.730Z","comments":true,"path":"tags/index.html","permalink":"https://lurongjiang.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"如何选择大数据平台落地方案","slug":"如何选择大数据平台落地方案","date":"2020-03-14T03:06:29.000Z","updated":"2020-03-15T22:14:18.000Z","comments":true,"path":"2020/03/14/如何选择大数据平台落地方案/","link":"","permalink":"https://lurongjiang.github.io/2020/03/14/%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E8%90%BD%E5%9C%B0%E6%96%B9%E6%A1%88/","excerpt":"如何选择大数据平台落地方案?真正的从0到1,到底有哪些坑.","text":"这个是跟若泽大数据J哥学习的时候做的笔记,虽然J哥也发了,不过这个是自己的笔记 如何选择大数据平台落地方案 机器: 云上 IDC机房 公司内部机器 云上 优点: 节省运维成本 对运维要求很低 快速扩容,缩减 缺点: 云上机器都是虚拟化,性能-20% 云上服务器不算公司资产,不方便上市 总结: 支付缓慢型 后期持续支付 周期长时,费用最高 高速盘: 主要是做系统盘数据盘: 主要存hdfs数据的 IDC机房 优点: 不虚拟化,性能高 算公司资产 缺点: 运维能力要求搞 总结: 开始支付多 后期只需要支付IDC机房托管 内部机器 优点: 没有IDC托管费用 缺点: 不可靠,园区断电 电费 空调 人工巡检 UPS 总结: 创业公司 小团队 如何采购服务器 找三家供应商 说需求 推荐报价 说预算 服务器供应商: dell 浪潮 华为 惠普 IBM 对比供应商 表格 最终报价单 优缺点 实地考察 综合价格+服务+口碑 汇报领导 meet讨论 采购部门询价 从开始到确定,最好控制再1-2周,因为内存条的价格有浮动 确定供应商 合同word-&gt;法务-&gt;boss审核-&gt;修改 时间一个月 供应商打印合同-&gt;盖章-&gt;我们 第一笔钱30%,3-6个月结清 等机器时间约1-2周 IDC机房选择 选择三家 报价 电信机房 xx 选择 2u服务器,散热性好 2个机架 一个防火墙 两个交换机 坑:问IDC供应商,我有xx台服务器,采购的配置,电源功率发给人家,让他算 重复之前的流程进行确认付款交付 云专线供应商 定期将云的数据同步到IDC,公网不可靠 云专线光纤直达,20ms 500M的带宽光纤约7000/月,需要买2根,一次操作费1.6w 重复之前的流程进行确认付款交付 上架 都准备好之后,运维约时间上架服务器,搭建防火墙,打通网络,熟练工2天左右都调试完了之后,重启一次机器,网络重启,格式化 网络拓扑图","categories":[{"name":"others","slug":"others","permalink":"https://lurongjiang.github.io/categories/others/"}],"tags":[{"name":"others","slug":"others","permalink":"https://lurongjiang.github.io/tags/others/"}]},{"title":"SpringSecurity获取Token源码","slug":"Spring-SpringSecurity创建Token的源码跟踪","date":"2020-02-20T09:05:52.000Z","updated":"2020-02-24T21:52:17.000Z","comments":true,"path":"2020/02/20/Spring-SpringSecurity创建Token的源码跟踪/","link":"","permalink":"https://lurongjiang.github.io/2020/02/20/Spring-SpringSecurity%E5%88%9B%E5%BB%BAToken%E7%9A%84%E6%BA%90%E7%A0%81%E8%B7%9F%E8%B8%AA/","excerpt":"SpringSecurity获取Token源码跟踪","text":"SpringSecurity-创建AccessToken TokenEndPoint 就是判断grant_type到底是什么模式 ClientDetailService 判断client_id ClientDetails封装请求的client的信息 TokenRequest将ClientDetails封装到请求里,因为Client信息也是token的一部分 TokenRequest请求TokenGranter(CompositeTokenGranter,默认),TokenGranter封装了四种授权模式,挑一个模式实现来生成token逻辑 Oauth2Request包含了client信息的ToekenRequests Authentication包含了当前认证的用户信息,通过UserDetailsService出来的UserDetails Oauth2Authentication包含了当前是哪个第三方client_id请求哪个用户授权,授权模式是什么,授权的参数是什么 AuthorizationServerTokenServices(默认实现DefaultTokenService) TokenStore 令牌的存取关联 TokenEnhancer 令牌的增强体,令牌生成后,可以加一些东西 创建Token的源码跟踪 TokenEndPoint 申请token入口 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@RequestMapping(value = \"/oauth/token\", method=RequestMethod.POST)public ResponseEntity&lt;OAuth2AccessToken&gt; postAccessToken(Principal principal, @RequestParamMap&lt;String, String&gt; parameters) throws HttpRequestMethodNotSupportedException &#123; if (!(principal instanceof Authentication)) &#123; throw new InsufficientAuthenticationException( \"There is no client authentication. Try adding an appropriate authentication filter.\"); &#125; //从请求令牌中获取client_id String clientId = getClientId(principal); //调用ClientDetailsService查询第三方应用的详细信息 ClientDetails authenticatedClient = getClientDetailsService().loadClientByClientId(clientId); //使用第三方应用的信息创建TokenRequest TokenRequest tokenRequest = getOAuth2RequestFactory().createTokenRequest(parameters, authenticatedClient); if (clientId != null &amp;&amp; !clientId.equals(\"\")) &#123; // Only validate the client details if a client authenticated during this // request. if (!clientId.equals(tokenRequest.getClientId())) &#123; // double check to make sure that the client ID in the token request is the same as that in the // authenticated client throw new InvalidClientException(\"Given client ID does not match authenticated client\"); &#125; &#125; if (authenticatedClient != null) &#123; oAuth2RequestValidator.validateScope(tokenRequest, authenticatedClient); &#125; //必须带grant_type if (!StringUtils.hasText(tokenRequest.getGrantType())) &#123; throw new InvalidRequestException(\"Missing grant type\"); &#125; //如果是简化模式,不会有grant_type,第一次就发放令牌 if (tokenRequest.getGrantType().equals(\"implicit\")) &#123; throw new InvalidGrantException(\"Implicit grant type not supported from token endpoint\"); &#125; //如果是授权码模式请求,先将token请求的scope置空 //因为此时只是请求授权码,即使你设置了all,但你也不一定有all的权限 //用户的实际scope是在用户授权之后才能决定 if (isAuthCodeRequest(parameters)) &#123; // The scope was requested or determined during the authorization step if (!tokenRequest.getScope().isEmpty()) &#123; logger.debug(\"Clearing scope of incoming token request\"); tokenRequest.setScope(Collections.&lt;String&gt; emptySet()); &#125; &#125; //如果是刷新令牌,令牌有自己的scope,所以重新设置scope if (isRefreshTokenRequest(parameters)) &#123; // A refresh token has its own default scopes, so we should ignore any added by the factory here. tokenRequest.setScope(OAuth2Utils.parseParameterList(parameters.get(OAuth2Utils.SCOPE))); &#125; //生成token OAuth2AccessToken token = getTokenGranter().grant(tokenRequest.getGrantType(), tokenRequest); if (token == null) &#123; throw new UnsupportedGrantTypeException(\"Unsupported grant type: \" + tokenRequest.getGrantType()); &#125; //返回token return getResponse(token);&#125; TokenRequest 封装Client信息成TokenRequest的逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public TokenRequest createTokenRequest(Map&lt;String, String&gt; requestParameters, ClientDetails authenticatedClient) &#123; //从请求中获取client_id参数 String clientId = requestParameters.get(OAuth2Utils.CLIENT_ID); if (clientId == null) &#123; // if the clientId wasn't passed in in the map, we add pull it from the authenticated client object clientId = authenticatedClient.getClientId(); &#125; else &#123; //如果两个client_id不匹配,抛异常 // otherwise, make sure that they match if (!clientId.equals(authenticatedClient.getClientId())) &#123; throw new InvalidClientException(\"Given client ID does not match authenticated client\"); &#125; &#125; //获取grant_type String grantType = requestParameters.get(OAuth2Utils.GRANT_TYPE); //获取scope Set&lt;String&gt; scopes = extractScopes(requestParameters, clientId); TokenRequest tokenRequest = new TokenRequest(requestParameters, clientId, scopes, grantType); return tokenRequest;&#125;//查寻client的scopeprivate Set&lt;String&gt; extractScopes(Map&lt;String, String&gt; requestParameters, String clientId) &#123; //从参数中获取scope Set&lt;String&gt; scopes = OAuth2Utils.parseParameterList(requestParameters.get(OAuth2Utils.SCOPE)); //调用clientDetailsService获取client信息 ClientDetails clientDetails = clientDetailsService.loadClientByClientId(clientId); //如果参数没带,直接把数据库的scope设置一下 if ((scopes == null || scopes.isEmpty())) &#123; // If no scopes are specified in the incoming data, use the default values registered with the client // (the spec allows us to choose between this option and rejecting the request completely, so we'll take the // least obnoxious choice as a default). scopes = clientDetails.getScope(); &#125; //检查scope if (checkUserScopes) &#123; scopes = checkUserScopes(scopes, clientDetails); &#125; return scopes;&#125;private Set&lt;String&gt; checkUserScopes(Set&lt;String&gt; scopes, ClientDetails clientDetails) &#123; if (!securityContextAccessor.isUser()) &#123; return scopes; &#125; Set&lt;String&gt; result = new LinkedHashSet&lt;String&gt;(); Set&lt;String&gt; authorities = AuthorityUtils.authorityListToSet(securityContextAccessor.getAuthorities()); //如果用户具有这些scope的权限或者角色,添加到scope for (String scope : scopes) &#123; if (authorities.contains(scope) || authorities.contains(scope.toUpperCase()) || authorities.contains(\"ROLE_\" + scope.toUpperCase())) &#123; result.add(scope); &#125; &#125; return result;&#125; TokenGranter 根据4种模式选择token的生成方式 123456789101112131415161718192021222324252627282930313233343536private final List&lt;TokenGranter&gt; tokenGranters;....public OAuth2AccessToken grant(String grantType, TokenRequest tokenRequest) &#123; //遍历TokenGranter集合(4种模式+1个Refresh),找到符合的就可以了 for (TokenGranter granter : tokenGranters) &#123; OAuth2AccessToken grant = granter.grant(grantType, tokenRequest); if (grant!=null) &#123; return grant; &#125; &#125; return null;&#125;public OAuth2AccessToken grant(String grantType, TokenRequest tokenRequest) &#123; if (!this.grantType.equals(grantType)) &#123; return null; &#125; String clientId = tokenRequest.getClientId(); ClientDetails client = clientDetailsService.loadClientByClientId(clientId); //校验请求授权码的模式 validateGrantType(grantType, client); logger.debug(\"Getting access token for: \" + clientId); return getAccessToken(client, tokenRequest);&#125;protected void validateGrantType(String grantType, ClientDetails clientDetails) &#123; Collection&lt;String&gt; authorizedGrantTypes = clientDetails.getAuthorizedGrantTypes(); //如果请求授权码的模式和client_id配置的请求模式不一样,抛异常 if (authorizedGrantTypes != null &amp;&amp; !authorizedGrantTypes.isEmpty() &amp;&amp; !authorizedGrantTypes.contains(grantType)) &#123; throw new InvalidClientException(\"Unauthorized grant type: \" + grantType); &#125;&#125; 封装TokenRequest 12345678910111213141516171819202122232425262728protected OAuth2AccessToken getAccessToken(ClientDetails client, TokenRequest tokenRequest) &#123; //getOAuth2Authentication根据不同的授权码请求方式,读取相对应的信息 //例如,授权码模式是先把scope返回去,当带scope来请求token的时候,把之前的用户信息读出来 //而密码模式就直接读取用户名密码,生成token return tokenServices.createAccessToken(getOAuth2Authentication(client, tokenRequest));&#125;protected OAuth2Authentication getOAuth2Authentication(ClientDetails client, TokenRequest tokenRequest) &#123; //OAuth2Authentication中封装了TokenRequest //先创建TokenRequest:主要是去掉敏感的password和client_secret //添加grant_type参数,以便在后续的OAuth2Request中可以获取到 OAuth2Request storedOAuth2Request = requestFactory.createOAuth2Request(client, tokenRequest); //创建OAuth2Authentication return new OAuth2Authentication(storedOAuth2Request, null);&#125;public OAuth2Request createOAuth2Request(ClientDetails client) &#123; Map&lt;String, String&gt; requestParameters = getRequestParameters(); HashMap&lt;String, String&gt; modifiable = new HashMap&lt;String, String&gt;(requestParameters); // Remove password if present to prevent leaks modifiable.remove(\"password\"); modifiable.remove(\"client_secret\"); // Add grant type so it can be retrieved from OAuth2Request modifiable.put(\"grant_type\", grantType); return new OAuth2Request(modifiable, client.getClientId(), client.getAuthorities(), true, this.getScope(), client.getResourceIds(), null, null, null);&#125; 创建AccessToken 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException &#123; //根据token的存储方式,取出之前的token OAuth2AccessToken existingAccessToken = tokenStore.getAccessToken(authentication); OAuth2RefreshToken refreshToken = null; if (existingAccessToken != null) &#123; //如果token过期了,直接移除refreshToken和accessToken if (existingAccessToken.isExpired()) &#123; if (existingAccessToken.getRefreshToken() != null) &#123; refreshToken = existingAccessToken.getRefreshToken(); // The token store could remove the refresh token when the // access token is removed, but we want to // be sure... tokenStore.removeRefreshToken(refreshToken); &#125; tokenStore.removeAccessToken(existingAccessToken); &#125; else &#123; //没过期,可能你又换了中方式来请求token,直接把之前的token重新存储一下 // Re-store the access token in case the authentication has changed tokenStore.storeAccessToken(existingAccessToken, authentication); return existingAccessToken; &#125; &#125; // Only create a new refresh token if there wasn't an existing one // associated with an expired access token. // Clients might be holding existing refresh tokens, so we re-use it in // the case that the old access token // expired. if (refreshToken == null) &#123; //刷新token没有,创建一个 refreshToken = createRefreshToken(authentication); &#125; // But the refresh token itself might need to be re-issued if it has // expired. else if (refreshToken instanceof ExpiringOAuth2RefreshToken) &#123; //如果刷新token过期了,重新颁发一个 ExpiringOAuth2RefreshToken expiring = (ExpiringOAuth2RefreshToken) refreshToken; if (System.currentTimeMillis() &gt; expiring.getExpiration().getTime()) &#123; refreshToken = createRefreshToken(authentication); &#125; &#125; //创建accessToken OAuth2AccessToken accessToken = createAccessToken(authentication, refreshToken); //存储token tokenStore.storeAccessToken(accessToken, authentication); // In case it was modified //存储刷新token防止修改 refreshToken = accessToken.getRefreshToken(); if (refreshToken != null) &#123; tokenStore.storeRefreshToken(refreshToken, authentication); &#125; return accessToken;&#125;//创建accessTokenprivate OAuth2AccessToken createAccessToken(OAuth2Authentication authentication, OAuth2RefreshToken refreshToken) &#123; //accessToken的创建逻辑就是使用UUID,设置过期时间和刷新token DefaultOAuth2AccessToken token = new DefaultOAuth2AccessToken(UUID.randomUUID().toString()); int validitySeconds = getAccessTokenValiditySeconds(authentication.getOAuth2Request()); if (validitySeconds &gt; 0) &#123; token.setExpiration(new Date(System.currentTimeMillis() + (validitySeconds * 1000L))); &#125; token.setRefreshToken(refreshToken); token.setScope(authentication.getOAuth2Request().getScope()); //如果配置了token增强,则对token调用增强 return accessTokenEnhancer != null ? accessTokenEnhancer.enhance(token, authentication) : token;&#125;//创建refreshTokenprivate OAuth2RefreshToken createRefreshToken(OAuth2Authentication authentication) &#123; if (!isSupportRefreshToken(authentication.getOAuth2Request())) &#123; return null; &#125; //刷新令牌的创建,直接是UUID,设置过期时间偏移一下 int validitySeconds = getRefreshTokenValiditySeconds(authentication.getOAuth2Request()); String value = UUID.randomUUID().toString(); if (validitySeconds &gt; 0) &#123; return new DefaultExpiringOAuth2RefreshToken(value, new Date(System.currentTimeMillis() + (validitySeconds * 1000L))); &#125; return new DefaultOAuth2RefreshToken(value);&#125;","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://lurongjiang.github.io/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://lurongjiang.github.io/tags/SpringCloud/"}]},{"title":"SpringCloud概述","slug":"Spring-SpringCloud基本使用","date":"2020-02-11T13:37:42.000Z","updated":"2020-02-13T18:50:19.000Z","comments":true,"path":"2020/02/11/Spring-SpringCloud基本使用/","link":"","permalink":"https://lurongjiang.github.io/2020/02/11/Spring-SpringCloud%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"SpringCloud相关介绍","text":"SpringCloud SpringCloud概述 官网 官网 主要功能 常用子项目 版本与兼容 SpringCloud的版本命名 版本命名 SpringCloud的版本,前半部分(如Hoxton,Greenwich),意思是发布列车(ReleaseTrain),以伦敦地铁的站名命名,因为SpringCloud有很多的子项目,每个项目都有自己的版本管理,按照发布顺序以A,B,C等为首字母依次命名,已经发布的版本顺序为: Angel -&gt; Brixton -&gt; Camden -&gt; Dalston -&gt; Edgware -&gt; Finchley -&gt; Greenwich -&gt; Hoxton 后半部分(如SR,SR1,SR2),意思是服务发布(ServiceRelease),即重大Bug修复 版本发布流程 SNAPSHOT -&gt; Mx -&gt; RELEASE -&gt; SRx,其中x就是一些数字序号,例如M1,M2,SR1,SR2.SNAPSHOT为快照版本(开发版本),Mx为里程碑版本,此时并不是正式版本,但是已经接近正式版,经过多个版本迭代之后,发布第一个RELEASE版本,正式版本;在RELEASE版本之后如果有重大bug修复就会发布SR版本 Hoxton SR1 CURRENT GA Reference Doc. Hoxton SNAPSHOT Reference Doc. Greenwich SR5 GA Reference Doc. Greenwich SNAPSHOT Reference Doc. SpringCloud的版本生命周期 版本发布规划 https://github.com/spring-cloud/spring-cloud-release/milestones 版本发布记录 https://github.com/spring-cloud/spring-cloud-release/releases 版本终止声明 https://spring.io/projects/spring-cloud#overview SpringBoot与SpringCloud的兼容性 版本兼容性非常重要https://spring.io/projects/spring-cloud#overview Release Train Boot Version Hoxton 2.2.x Greenwich 2.1.x Finchley 2.0.x Edgware 1.5.x Dalston 1.5.x 生产环境如何选择选择 坚决不适用非稳定版本 坚决不适用end-of-life版本 尽量使用最新版本 RELEASE版本可以观望/调研,因为是第一个正式版,并没有在生产上得以广泛应用 SR2之后可以大规模使用 版本选择 SpringCloud Hoxton SR1 SpringBoot 2.2.4.RELEASE 12345678910111213141516171819&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-dependencies --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Hoxton.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 检查项目是否能运行 1mvn clean install -U SpringCloud服务注册与发现 使得服务消费者总能找到服务提供者 Consul单机版安装 Consul下载 下载Consoule https://releases.hashicorp.com/consul/1.6.3/consul_1.6.3_linux_amd64.zip 需要的端口 Use Default Ports DNS: The DNS server (TCP and UDP) 8600 HTTP: The HTTP API (TCP Only) 8500 HTTPS: The HTTPs API disabled (8501)* gRPC: The gRPC API disabled (8502)* LAN Serf: The Serf LAN port (TCP and UDP) 8301 Wan Serf: The Serf WAN port TCP and UDP) 8302 server: Server RPC address (TCP Only) 8300 Sidecar Proxy Min: Inclusive min port number to use for automatically assigned sidecar service registrations. 21000 Sidecar Proxy Max: Inclusive max port number to use for automatically assigned sidecar service registrations. 21255 检查端口是否被占用的方法 12345678910111213Windows:# 如果没有结果说明没有被占用netstat -ano| findstr \"8500\"Linux:# 如果没有结果说明没有被占用netstat -antp |grep 8500macOS:# 如果没有结果说明没有被占用netstat -ant | grep 8500或lsof -i:8500 安装和启动 解压 1./consul agent -dev -client 0.0.0.0 严重是否成功 1./consul -v 访问Consul首页localhost:8500 启动参数 -ui 开启ui -client 让consul拥有client功能,接受服务注册;0.0.0.0允许任意ip注册,不写只能使用localhost连接 -dev 以开发模式运行consul 整合Consul 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 配置 1234567891011spring: application: # 指定注册到consul的服务名称,分隔符不能是下划线 # 如果服务发现组件是Consul,会强制转换成中划线,导致找不到服务 # 如果服务发现组件是Ribbon,则因为Ribbon的问题(把默认名称当初虚拟主机名,而虚拟主机名不能用下划线),会造成微服务之间无法调用 name: micro-service-user cloud: consul: host: 192.168.238.128 port: 8500 启动,检查consul ui的服务上线情况 Consul健康检查 添加健康检查依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置 12345management: endpoints: web: exposure: include: '*' 端点 http://localhost:8080/actuator 查看端点 http://localhost:8080/actuator/health 健康检查 添加详情配置,可以检查详细的健康情况 1234management: endpoint: health: show-details: always 简单研究一下健康检查的源码 以磁盘检查的为例 健康检查的类都继承了AbstractHealthIndicator抽象类,而AbstractHealthIndicator实现了HealthIndicator接口,所有健康检查实现类都必须实现doHealthCheck(Health.Builder builder)方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class DiskSpaceHealthIndicator extends AbstractHealthIndicator &#123; private static final Log logger = LogFactory.getLog(DiskSpaceHealthIndicator.class); private final File path; private final DataSize threshold; /** * Create a new &#123;@code DiskSpaceHealthIndicator&#125; instance. * @param path the Path used to compute the available disk space * @param threshold the minimum disk space that should be available */ public DiskSpaceHealthIndicator(File path, DataSize threshold) &#123; super(\"DiskSpace health check failed\"); this.path = path; this.threshold = threshold; &#125; @Override protected void doHealthCheck(Health.Builder builder) throws Exception &#123; //获取可用的空间字节数 long diskFreeInBytes = this.path.getUsableSpace(); //如果可用的字节数大于预留字节数阈值则认为是健康的,设置status为UP if (diskFreeInBytes &gt;= this.threshold.toBytes()) &#123; builder.up(); &#125; else &#123; //否则任务是不健康的,设置status为DOWN logger.warn(LogMessage.format(\"Free disk space below threshold. Available: %d bytes (threshold: %s)\", diskFreeInBytes, this.threshold)); builder.down(); &#125; //输出总空间,可用空间和预留阈值 builder.withDetail(\"total\", this.path.getTotalSpace()).withDetail(\"free\", diskFreeInBytes) .withDetail(\"threshold\", this.threshold.toBytes()); &#125;&#125;//这个获取可用字节数还是挺好的,直接利用了File提供的方法public long getUsableSpace() &#123; SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; sm.checkPermission(new RuntimePermission(\"getFileSystemAttributes\")); sm.checkRead(path); &#125; if (isInvalid()) &#123; return 0L; &#125; //fs是默认的文件系统FileSystem fs = DefaultFileSystem.getFileSystem(); return fs.getSpace(this, FileSystem.SPACE_USABLE);&#125; 健康检查使用了建造者模式,对于不同的健康指标非常方便,值得学习 整合Consul和SpringCloud的actuator 修改配置 1234567spring: cloud: consul: host: 192.168.238.128 port: 8500 discovery: health-check-path: /actuator/health 这样启动之后,再检查consul ui就可以发现没有红色的叉了 其他的健康检查配置 注册课程微服务到Consul 添加依赖 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置 1234567891011121314151617181920212223242526272829303132spring: datasource: url: jdbc:mysql://192.168.238.128:3306/ms?serverTimezone=GMT%2B8&amp;characterEncoding=utf8&amp;useSSL=false hikari: username: lrj password: lu11221015 driver-class-name: com.mysql.cj.jdbc.Driver# JPA配置 jpa: hibernate: ddl-auto: update show-sql: true application: name: micro-service-class cloud: consul: host: 192.168.238.128 port: 8500 discovery: health-check-path: /actuator/health# 暴露所有的actuator端点management: endpoints: web: exposure: include: '*' # 开启健康检查详细信息 endpoint: health: show-details: alwaysserver: port: 8081 重构用户微服务 12345678910111213141516171819@RestController@RequestMapping(\"user\")public class UserController &#123; @Resource private UserService userService; @Resource private DiscoveryClient discoveryClient; @GetMapping(\"&#123;id&#125;\") public Object findUserById(@PathVariable(\"id\") Integer id) &#123; return userService.findUserById(id); &#125; @GetMapping(\"discoveryTest\") public Object discoveryTest() &#123; return discoveryClient.getInstances(\"micro-service-class\"); &#125;&#125; 访问端点,可以发现不需要指定课程微服务的主机和端口就可以拿到相关信息,实现了服务发现 重构课程微服务 原来 123456789101112131415161718192021222324252627282930313233@Servicepublic class LessonServiceImpl implements LessonService &#123; @Resource private LessonRepository lessonRepository; @Resource private LessonUserRepository lessonUserRepository; @Resource private RestTemplate restTemplate; @Override public Lesson buyById(Integer id) &#123; // 1. 根据课程id查询课程 Lesson lesson = lessonRepository.findById(id).orElseThrow(() -&gt; new IllegalArgumentException(\"该课程不存在\")); //根据课程查询是否已经购买过 LessonUser lessonUser = lessonUserRepository.findByLessonId(id); if (lessonUser != null) &#123; return lesson; &#125; //todo 2.登录之后获取userId String userId = \"1\"; // 3. 如果没有购买过,查询用户余额 UserDTO userDTO = restTemplate.getForObject(\"http://localhost:8080/user/&#123;userId&#125;\", UserDTO.class, userId); if (userDTO != null &amp;&amp; userDTO.getMoney() != null &amp;&amp; userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() &lt; 0) &#123; throw new IllegalArgumentException(\"余额不足\"); &#125; //4. 购买逻辑 //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录 return lesson; &#125;&#125; 这个写死了主机地址,无法动态获取微服务路径 重构 123456789101112131415161718192021222324252627282930313233343536373839public class LessonServiceImpl implements LessonService &#123; @Resource private LessonRepository lessonRepository; @Resource private LessonUserRepository lessonUserRepository; @Resource private DiscoveryClient discoveryClient; @Resource private RestTemplate restTemplate; @Override public Lesson buyById(Integer id) &#123; // 1. 根据课程id查询课程 Lesson lesson = lessonRepository.findById(id).orElseThrow(() -&gt; new IllegalArgumentException(\"该课程不存在\")); //根据课程查询是否已经购买过 LessonUser lessonUser = lessonUserRepository.findByLessonId(id); if (lessonUser != null) &#123; return lesson; &#125; //todo 2.登录之后获取userId String userId = \"1\"; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(\"micro-service-user\"); if (instances != null &amp;&amp; !instances.isEmpty()) &#123; //todo 需要改进,如果存在多个实例,需要考虑负载均衡 ServiceInstance instance = instances.get(0); URI uri = instance.getUri(); UserDTO userDTO = restTemplate.getForObject(uri + \"/user/&#123;userId&#125;\", UserDTO.class, userId); if (userDTO != null &amp;&amp; userDTO.getMoney() != null &amp;&amp; userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() &lt; 0) &#123; throw new IllegalArgumentException(\"余额不足\"); &#125; //4. 购买逻辑 //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录 return lesson; &#125; throw new IllegalArgumentException(\"用户微服务异常,无法购买课程\"); &#125;&#125; 可以动态的获取到用户微服务的地址,请求正常 元数据 Consul是没有元数据的概念的,所以SpringCloud做了个适配,在consul下设置tags作为元数据. 元数据可以对微服务添加描述,标识,例如机房在哪里,这样可以进行就近判断,或者当就近机房不可用时才检查远程机房,当两者都不可用时才认为服务不可用等实现容灾或者跨机房 配置 12345678spring: cloud: consul: host: 192.168.238.128 port: 8500 discovery: health-check-path: /actuator/health tags: JiFang=Beijing,JiFang=Shanghai 实现机房选择 123456789101112@GetMapping(\"discoveryTest\")public Object discoveryTest() &#123; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(\"micro-service-class\"); if (instances != null) &#123; List&lt;ServiceInstance&gt; shanghaiInstances = instances.stream() .filter(s -&gt; s.getMetadata().containsKey(\"Shanghai\")).collect(Collectors.toList()); if (!shanghaiInstances.isEmpty()) &#123; return shanghaiInstances; &#125; &#125; return instances;&#125;","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://lurongjiang.github.io/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://lurongjiang.github.io/tags/SpringCloud/"}]},{"title":"Spring解析xml成BeanDefinition的过程","slug":"Spring-Bean的解析过程","date":"2020-01-26T15:07:34.000Z","updated":"2020-02-13T10:17:23.000Z","comments":true,"path":"2020/01/26/Spring-Bean的解析过程/","link":"","permalink":"https://lurongjiang.github.io/2020/01/26/Spring-Bean%E7%9A%84%E8%A7%A3%E6%9E%90%E8%BF%87%E7%A8%8B/","excerpt":"Spring解析xml成BeanDefinition的过程。","text":"Spring Bean的解析过程 xml文件的读取 从我们的入口开始 123456@Testpublic void testXml() &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\"); Student student = context.getBean(Student.class); System.out.println(student.getUsername());&#125; 先进入ClassPathXmlApplicationContext的构造器 123public ClassPathXmlApplicationContext(String configLocation) throws BeansException &#123; this(new String[] &#123;configLocation&#125;, true, null);&#125; 继续调用另一个构造器 12345678910public ClassPathXmlApplicationContext( String[] configLocations, boolean refresh, @Nullable ApplicationContext parent) throws BeansException &#123; super(parent); //创建解析器，解析configLocations setConfigLocations(configLocations); if (refresh) &#123; refresh(); &#125;&#125; 这个refresh()方法是核心方法,点进去 123456789101112131415161718192021222324252627public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; //为容器初始化做准备，重要程度：0 // Prepare this context for refreshing. prepareRefresh(); /* 重要程度：5 1、创建BeanFactory对象 * 2、xml解析 * 传统标签解析：bean、import等 * 自定义标签解析 如：&lt;context:component-scan base-package=\"com.xiangxue.jack\"/&gt; * 自定义标签解析流程： * a、根据当前解析标签的头信息找到对应的namespaceUri * b、加载spring所以jar中的spring.handlers文件。并建立映射关系 * c、根据namespaceUri从映射关系中找到对应的实现了NamespaceHandler接口的类 * d、调用类的init方法，init方法是注册了各种自定义标签的解析类 * e、根据namespaceUri找到对应的解析类，然后调用paser方法完成标签解析 * * 3、把解析出来的xml标签封装成BeanDefinition对象 * */ // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); .... &#125;&#125; 继续看obtainFreshBeanFactory()方法 1234567891011121314151617181920212223protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; /** * 核心方法，必须读，重要程度：5 * 这里使用了模板设计模式,Spring使用最多的设计模式,父类定义了模板,子类具体实现 * 其实反过来看,AbstractApplicationContext的refresh()同样也是定义了模板,onFresh()方法交给子类去实现 * 例如SpringBoot中嵌入式Tomcat启动就是覆写了onFresh()方法 * @see AbstractApplicationContext#onRefresh() * */ refreshBeanFactory(); return getBeanFactory();&#125; /** * 因为ClassPathXmlApplicationContext是AbstractRefreshableApplicationContext的子类 * 所以跳转到AbstractRefreshableApplicationContext * @see AbstractRefreshableApplicationContext#refreshBeanFactory() * @throws BeansException if initialization of the bean factory failed * @throws IllegalStateException if already initialized and multiple refresh * attempts are not supported * &lt;p&gt; * */protected abstract void refreshBeanFactory() throws BeansException, IllegalStateException; 跳转到AbstractRefreshableApplicationContext的refreshBeanFactory() 12345678910111213141516171819202122232425262728293031323334protected final void refreshBeanFactory() throws BeansException &#123; //如果BeanFactory不为空，则清除BeanFactory和里面的实例 if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; //创建DefaultListableBeanFactory //BeanFactory实例工厂,不管什么实例都可以从BeanFactory获取到 DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); //设置是否可以循环依赖 allowCircularReferences //是否允许使用相同名称重新注册不同的bean实现. customizeBeanFactory(beanFactory); /** * 解析xml，并把xml中的标签封装成BeanDefinition对象 * 因为ClassPathXmlApplication是继承自 AbstractXmlApplicationContext * 所以进入AbstractXmlApplicationContext,又是一个模板 * @see AbstractXmlApplicationContext#loadBeanDefinitions(org.springframework.beans.factory.support.DefaultListableBeanFactory) */ loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(\"I/O error parsing bean definition source for \" + getDisplayName(), ex); &#125;&#125;protected abstract void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException; 继续看AbstractXmlApplicationContext的loadBeanDefinitions(…) xml的解析交给了XmlBeanDefinitionReader来解析 1234567891011protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // Create a new XmlBeanDefinitionReader for the given BeanFactory. //创建xml的解析器，这里是一个委托模式 //xml的解析工作,委托给XmlBeanDefinitionReader来解析 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory);.... //主要看这个方法 重要程度 5 loadBeanDefinitions(beanDefinitionReader);&#125; 继续看loadBeanDefinitions(…) 123456789101112protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; Resource[] configResources = getConfigResources(); if (configResources != null) &#123; reader.loadBeanDefinitions(configResources); &#125; //获取需要加载的xml配置文件 String[] configLocations = getConfigLocations(); if (configLocations != null) &#123; //委托给reader来解析xml reader.loadBeanDefinitions(configLocations); &#125;&#125; 点进去 12345678910111213141516171819202122232425262728293031323334353637@Overridepublic int loadBeanDefinitions(String location) throws BeanDefinitionStoreException &#123; return loadBeanDefinitions(location, null);&#125;public int loadBeanDefinitions(String location, @Nullable Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; ResourceLoader resourceLoader = getResourceLoader(); if (resourceLoader == null) &#123; throw new BeanDefinitionStoreException( \"Cannot load bean definitions from location [\" + location + \"]: no ResourceLoader available\"); &#125; if (resourceLoader instanceof ResourcePatternResolver) &#123; // Resource pattern matching available. try &#123; //把字符串类型的xml文件路径，形如：classpath*:user/**/*-context.xml,转换成Resource对象类型，其实就是用流 //的方式加载配置文件，然后封装成Resource对象，不重要，可以不看 Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); //主要看这个方法 ** 重要程度 5 int count = loadBeanDefinitions(resources); .... return count; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( \"Could not resolve bean definition resource pattern [\" + location + \"]\", ex); &#125; &#125; else &#123; // Can only load single resources by absolute URL. Resource resource = resourceLoader.getResource(location); int count = loadBeanDefinitions(resource);.... return count; &#125;&#125; 把xml读出来之后封装成了Resource对象,开始解析Resource 123456789101112@Overridepublic int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; Assert.notNull(resources, \"Resource array must not be null\"); int count = 0; for (Resource resource : resources) &#123; //模板设计模式，调用到子类中的方法 //又是一个模板,因为委托给了XmlBeanDefinitionReader /**@see org.springframework.beans.factory.xml.XmlBeanDefinitionReader#loadBeanDefinitions(org.springframework.core.io.Resource)*/ count += loadBeanDefinitions(resource); &#125; return count;&#125; xml读出来之后,又把Resource封装成带编码的对象,委托给XmlBeanDefinitionReader进行解析 123456@Overridepublic int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; //EncodedResource带编码的对Resource对象的封装 //把资源流对象又做了编码的封装 return loadBeanDefinitions(new EncodedResource(resource));&#125; 再看如何解析Resource的 1234567891011121314151617181920212223242526272829303132333435363738public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; ... Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) &#123; currentResources = new HashSet&lt;&gt;(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); &#125; if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( \"Detected cyclic loading of \" + encodedResource + \" - check your import definitions!\"); &#125; try &#123; //获取Resource对象中的xml文件流对象 InputStream inputStream = encodedResource.getResource().getInputStream(); try &#123; //InputSource是jdk中的sax xml文件解析对象 InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; //主要看这个方法 ** 重要程度 5 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; finally &#123; inputStream.close(); &#125; &#125; catch (IOException ex) &#123; ... &#125; finally &#123; currentResources.remove(encodedResource); if (currentResources.isEmpty()) &#123; this.resourcesCurrentlyBeingLoaded.remove(); &#125; &#125;&#125; 把InputStream流对象从Resouce中读出来,封装成InputSource对象 1234567891011121314151617protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; //把inputSource 封装成Document文件对象，这是jdk的API Document doc = doLoadDocument(inputSource, resource); //主要看这个方法，根据解析出来的document对象，拿到里面的标签元素封装成BeanDefinition int count = registerBeanDefinitions(doc, resource); if (logger.isDebugEnabled()) &#123; logger.debug(\"Loaded \" + count + \" bean definitions from \" + resource); &#125; return count; &#125; catch (BeanDefinitionStoreException ex) &#123; ....&#125; 把流对象InputSource使用SAX进行解析成Document对象,对Document对象进行解析 123456789public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; //xml解析成Document之后,又将Document委托给BeanDefinitionDocumentReader来解析Document成BeanDefinition //又来一记委托模式，BeanDefinitionDocumentReader委托这个类进行document的解析 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); int countBefore = getRegistry().getBeanDefinitionCount(); //主要看这个方法，createReaderContext(resource) XmlReaderContext上下文，封装了XmlBeanDefinitionReader对象 documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore;&#125; Document委托给BeanDefinitionDocumentReader来解析Document成BeanDefinition 123456789101112131415161718192021222324void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) throws BeanDefinitionStoreException;@Overridepublic void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; this.readerContext = readerContext; //主要看这个方法，把root节点传进去 doRegisterBeanDefinitions(doc.getDocumentElement());&#125;protected void doRegisterBeanDefinitions(Element root) &#123; BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent);... //又是模板,冗余设计,空实现 preProcessXml(root); //主要看这个方法，标签具体解析过程 parseBeanDefinitions(root, this.delegate); postProcessXml(root); this.delegate = parent;&#125; 把Document的根传进去,开始解析Document 12345678910111213141516171819202122232425protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; //获取根节点下的所有子节点 //遍历所有子节点,依次解析 NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; //默认标签解析,import,alias,bean,beans parseDefaultElement(ele, delegate); &#125; else &#123; //自定义标签解析,委托给BeanDefinitionParserDelegate来解析 //context:component-scan等,使用了namespaceUri delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125;&#125; 标签的解析分为默认标签(包括import,alias,bean,beans)和自定义标签(如context:componet-scan,mvc:annotation-drive等,这类带前缀的标签需要namespaceUri来指定实现,使用了SPI思想) 默认标签的解析 先看默认标签的解析 123456789101112131415161718private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; //import标签解析 重要程度 1 ，可看可不看 if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; importBeanDefinitionResource(ele); &#125; //alias标签解析 别名标签 重要程度 1 ，可看可不看 else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; processAliasRegistration(ele); &#125; //bean标签，重要程度 5，必须看 else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; processBeanDefinition(ele, delegate); &#125; else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; // recurse doRegisterBeanDefinitions(ele); &#125;&#125; 核心方法processBeanDefinition(…) 123456789101112131415161718192021protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; //重点看这个方法，重要程度 5 ，解析document，封装成BeanDefinition BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; //该方法功能不重要，设计模式重点看一下，装饰者设计模式，加上SPI设计思想 bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; //完成document到BeanDefinition对象转换后，对BeanDefinition对象进行缓存注册 // Register the final decorated instance. BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(\"Failed to register bean definition with name '\" + bdHolder.getBeanName() + \"'\", ele, ex); &#125; // Send registration event. getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 继续看BeanDefinitionParserDelegate是如何解析的parseBeanDefinitionElement(ele) 123public BeanDefinitionHolder parseBeanDefinitionElement(Element ele) &#123; return parseBeanDefinitionElement(ele, null);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) &#123; String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;&gt;(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; beanName = aliases.remove(0); ... &#125; //检查beanName是否重复 if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; //继续点进去看 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); // Register an alias for the plain bean class name, if still possible, // if the generator returned the class name plus a suffix. // This is expected for Spring 1.2/2.0 backwards compatibility. String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(\"Neither XML 'id' nor 'name' specified - \" + \"using generated bean name [\" + beanName + \"]\"); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; return null;&#125; 继续点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, @Nullable BeanDefinition containingBean) &#123; this.parseState.push(new BeanEntry(beanName)); String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE); &#125; try &#123; //创建GenericBeanDefinition对象,设置parent和className AbstractBeanDefinition bd = createBeanDefinition(className, parent); //解析bean标签的属性，并把解析出来的属性设置到BeanDefinition对象中 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); //解析bean中的meta标签 parseMetaElements(ele, bd); //解析bean中的lookup-method标签 重要程度：2，可看可不看 parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); //解析bean中的replaced-method标签 重要程度：2，可看可不看 parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); //解析bean中的constructor-arg标签 重要程度：2，可看可不看 parseConstructorArgElements(ele, bd); //解析bean中的property标签 重要程度：2，可看可不看 parsePropertyElements(ele, bd); //可以不看，用不到 parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; &#125; catch (ClassNotFoundException ex) &#123; ... &#125; finally &#123; this.parseState.pop(); &#125; return null;&#125; 先创建BeanDefinition的封装GenericBeanDefinition 属性解析 解析每个节点的属性 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public AbstractBeanDefinition parseBeanDefinitionAttributes(Element ele, String beanName, @Nullable BeanDefinition containingBean, AbstractBeanDefinition bd) &#123; //如果有singleton属性,先提示一下,建议使用scope属性 if (ele.hasAttribute(SINGLETON_ATTRIBUTE)) &#123; error(\"Old 1.x 'singleton' attribute in use - upgrade to 'scope' declaration\", ele); &#125; else if (ele.hasAttribute(SCOPE_ATTRIBUTE)) &#123; //如果有scope属性,设置scope bd.setScope(ele.getAttribute(SCOPE_ATTRIBUTE)); &#125; else if (containingBean != null) &#123; // Take default from containing bean in case of an inner bean definition. bd.setScope(containingBean.getScope()); &#125; //设置abstract属性,不实例化,子类需要parent标签引用,父类提供了公共的属性,子类不需要写那么多了 if (ele.hasAttribute(ABSTRACT_ATTRIBUTE)) &#123; bd.setAbstract(TRUE_VALUE.equals(ele.getAttribute(ABSTRACT_ATTRIBUTE))); &#125; //设置lazy-init属性 String lazyInit = ele.getAttribute(LAZY_INIT_ATTRIBUTE); if (DEFAULT_VALUE.equals(lazyInit)) &#123; lazyInit = this.defaults.getLazyInit(); &#125; bd.setLazyInit(TRUE_VALUE.equals(lazyInit)); //设置autowired属性 String autowire = ele.getAttribute(AUTOWIRE_ATTRIBUTE); bd.setAutowireMode(getAutowireMode(autowire)); //设置depends-on属性 if (ele.hasAttribute(DEPENDS_ON_ATTRIBUTE)) &#123; String dependsOn = ele.getAttribute(DEPENDS_ON_ATTRIBUTE); bd.setDependsOn(StringUtils.tokenizeToStringArray(dependsOn, MULTI_VALUE_ATTRIBUTE_DELIMITERS)); &#125; //设置autowired-candidate String autowireCandidate = ele.getAttribute(AUTOWIRE_CANDIDATE_ATTRIBUTE); if (\"\".equals(autowireCandidate) || DEFAULT_VALUE.equals(autowireCandidate)) &#123; String candidatePattern = this.defaults.getAutowireCandidates(); if (candidatePattern != null) &#123; String[] patterns = StringUtils.commaDelimitedListToStringArray(candidatePattern); bd.setAutowireCandidate(PatternMatchUtils.simpleMatch(patterns, beanName)); &#125; &#125; else &#123; bd.setAutowireCandidate(TRUE_VALUE.equals(autowireCandidate)); &#125; //设置primary if (ele.hasAttribute(PRIMARY_ATTRIBUTE)) &#123; bd.setPrimary(TRUE_VALUE.equals(ele.getAttribute(PRIMARY_ATTRIBUTE))); &#125; //设置init-method if (ele.hasAttribute(INIT_METHOD_ATTRIBUTE)) &#123; String initMethodName = ele.getAttribute(INIT_METHOD_ATTRIBUTE); bd.setInitMethodName(initMethodName); &#125; else if (this.defaults.getInitMethod() != null) &#123; bd.setInitMethodName(this.defaults.getInitMethod()); bd.setEnforceInitMethod(false); &#125; //设置destroy-method if (ele.hasAttribute(DESTROY_METHOD_ATTRIBUTE)) &#123; String destroyMethodName = ele.getAttribute(DESTROY_METHOD_ATTRIBUTE); bd.setDestroyMethodName(destroyMethodName); &#125; else if (this.defaults.getDestroyMethod() != null) &#123; bd.setDestroyMethodName(this.defaults.getDestroyMethod()); bd.setEnforceDestroyMethod(false); &#125; //设置factory-method,指定生成实例的工厂方法 if (ele.hasAttribute(FACTORY_METHOD_ATTRIBUTE)) &#123; bd.setFactoryMethodName(ele.getAttribute(FACTORY_METHOD_ATTRIBUTE)); &#125; //设置factory-bean属性,指定生成实例的工厂,这个需要配合factory-method使用 if (ele.hasAttribute(FACTORY_BEAN_ATTRIBUTE)) &#123; bd.setFactoryBeanName(ele.getAttribute(FACTORY_BEAN_ATTRIBUTE)); &#125; return bd;&#125; parseBeanDefinitionAttributes(..)方法主要是解析Node的属性并设置了BeanDefinition的一些属性 meta标签解析 再看meta标签的解析,其实就是把bean标签的meta属性的key,value读取出来,设置到BeanDefinition,没啥用,一个标识而已 1234567891011121314public void parseMetaElements(Element ele, BeanMetadataAttributeAccessor attributeAccessor) &#123; NodeList nl = ele.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, META_ELEMENT)) &#123; Element metaElement = (Element) node; String key = metaElement.getAttribute(KEY_ATTRIBUTE); String value = metaElement.getAttribute(VALUE_ATTRIBUTE); BeanMetadataAttribute attribute = new BeanMetadataAttribute(key, value); attribute.setSource(extractSource(metaElement)); attributeAccessor.addMetadataAttribute(attribute); &#125; &#125;&#125; lookup-method标签解析 同样的lookup-method标签的解析也是类似的 123456789101112131415161718public void parseLookupOverrideSubElements(Element beanEle, MethodOverrides overrides) &#123; NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, LOOKUP_METHOD_ELEMENT)) &#123; Element ele = (Element) node; //获取name属性 String methodName = ele.getAttribute(NAME_ATTRIBUTE); //获取bean属性 String beanRef = ele.getAttribute(BEAN_ELEMENT); //封装成 LookupOverride LookupOverride override = new LookupOverride(methodName, beanRef); override.setSource(extractSource(ele)); //可能有多个lookup-method标签,所以用list装起来 overrides.addOverride(override); &#125; &#125;&#125; 不过值得一提的是,这个lookup-method的设计精髓主要是代理思想,很方便的实现了多态 1234567891011121314&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean class=\"com.lrj.test.bean.Student\" id=\"student\"/&gt; &lt;bean class=\"com.lrj.test.bean.Women\" id=\"women\"/&gt; &lt;!--实现多态,传入什么就是什么--&gt; &lt;bean class=\"com.lrj.test.bean.AbstractClass\"&gt; &lt;lookup-method name=\"getPeople\" bean=\"women\"/&gt; &lt;/bean&gt;&lt;/beans&gt; 1234567891011121314151617181920212223public abstract class People &#123; public void show()&#123;&#125;&#125;public class Women extends People &#123; @Override public void show() &#123; System.out.println(\"I am women\"); &#125;&#125;public abstract class AbstractClass &#123; public void show() &#123; getPeople().show(); &#125; public abstract People getPeople();&#125;//这个方法最终打印的是I am women@Testpublic void testXml() &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\"); AbstractClass abstractClass=context.getBean(AbstractClass.class); abstractClass.show();&#125; replace-method标签解析 再看看parseReplacedMethodSubElements(…)解析replace-method属性 123456789101112131415161718192021222324252627public void parseReplacedMethodSubElements(Element beanEle, MethodOverrides overrides) &#123; NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, REPLACED_METHOD_ELEMENT)) &#123; Element replacedMethodEle = (Element) node; String name = replacedMethodEle.getAttribute(NAME_ATTRIBUTE); String callback = replacedMethodEle.getAttribute(REPLACER_ATTRIBUTE); //一个replaced-method标签封装成一个ReplaceOverride对象，最后加入到BeanDefinition对象中 ReplaceOverride replaceOverride = new ReplaceOverride(name, callback); // Look for arg-type match elements. List&lt;Element&gt; argTypeEles = DomUtils.getChildElementsByTagName(replacedMethodEle, ARG_TYPE_ELEMENT); for (Element argTypeEle : argTypeEles) &#123; //根据方法参数类型来区分同名的不同的方法 String match = argTypeEle.getAttribute(ARG_TYPE_MATCH_ATTRIBUTE); match = (StringUtils.hasText(match) ? match : DomUtils.getTextValue(argTypeEle)); if (StringUtils.hasText(match)) &#123; replaceOverride.addTypeIdentifier(match); &#125; &#125; replaceOverride.setSource(extractSource(replacedMethodEle)); overrides.addOverride(replaceOverride); &#125; &#125;&#125; 可以发现lookup-method和replace-method都放入了BeanDefinition的MethodOverrides类型的overrides属性中,也就是说,MethodOverrides包含了LookupOverride和ReplaceOverride两种类型的对象 1234567891011&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean class=\"com.lrj.test.bean.Replacement\" id=\"replacement\"/&gt; &lt;bean class=\"com.lrj.test.bean.Origin\"&gt; &lt;replaced-method name=\"show\" replacer=\"replacement\"&gt; &lt;arg-type match=\"int\"/&gt; &lt;/replaced-method&gt; &lt;/bean&gt;&lt;/beans&gt; 123456789101112131415161718192021222324252627public class Origin &#123; public void show(String str) &#123; System.out.println(\"show str:\" + str); &#125; public void show(int str) &#123; System.out.println(\"show int:\" + str); &#125;&#125;public class Replacement implements MethodReplacer &#123; @Override public Object reimplement(Object obj, Method method, Object[] args) throws Throwable &#123; System.out.println(\"I am a placement method......\"); return null; &#125;&#125;//这个打印的将会是show str:hello和I am a placement method......//说明int的被替换了,但是String的没有被替换,@Testpublic void testReplace() &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\"); Origin origin = context.getBean(Origin.class); origin.show(\"hello\"); origin.show(555);&#125; 这个听说是在项目封版之后,不想改代码了,直接改配置,符合开闭原则,但是这个Replacement必须要实现MethodReplacer感觉有点鸡肋 constructor-arg标签解析 这个没啥讲的,无非是根据index或者name来设置 不过需要注意ConstructorArgumentValues对象保存了ValueHolder集合 解析construct-arg标签,读取下标或者name封装成ValueHolder,构成BeanDefinition的ConstructorArgumentValues 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public void parseConstructorArgElements(Element beanEle, BeanDefinition bd) &#123; NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, CONSTRUCTOR_ARG_ELEMENT)) &#123; parseConstructorArgElement((Element) node, bd); &#125; &#125;&#125;public void parseConstructorArgElement(Element ele, BeanDefinition bd) &#123; String indexAttr = ele.getAttribute(INDEX_ATTRIBUTE); String typeAttr = ele.getAttribute(TYPE_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); if (StringUtils.hasLength(indexAttr)) &#123; try &#123; int index = Integer.parseInt(indexAttr); if (index &lt; 0) &#123; error(\"'index' cannot be lower than 0\", ele); &#125; else &#123; try &#123; this.parseState.push(new ConstructorArgumentEntry(index)); Object value = parsePropertyValue(ele, bd, null); ConstructorArgumentValues.ValueHolder valueHolder = new ConstructorArgumentValues.ValueHolder(value); if (StringUtils.hasLength(typeAttr)) &#123; valueHolder.setType(typeAttr); &#125; if (StringUtils.hasLength(nameAttr)) &#123; valueHolder.setName(nameAttr); &#125; valueHolder.setSource(extractSource(ele)); if (bd.getConstructorArgumentValues().hasIndexedArgumentValue(index)) &#123; error(\"Ambiguous constructor-arg entries for index \" + index, ele); &#125; else &#123; //将ValueHolder添加到BeanDefinition bd.getConstructorArgumentValues().addIndexedArgumentValue(index, valueHolder); &#125; &#125; finally &#123; this.parseState.pop(); &#125; &#125; &#125; catch (NumberFormatException ex) &#123; error(\"Attribute 'index' of tag 'constructor-arg' must be an integer\", ele); &#125; &#125; else &#123; try &#123; this.parseState.push(new ConstructorArgumentEntry()); Object value = parsePropertyValue(ele, bd, null); ConstructorArgumentValues.ValueHolder valueHolder = new ConstructorArgumentValues.ValueHolder(value); if (StringUtils.hasLength(typeAttr)) &#123; valueHolder.setType(typeAttr); &#125; if (StringUtils.hasLength(nameAttr)) &#123; valueHolder.setName(nameAttr); &#125; valueHolder.setSource(extractSource(ele)); //将ValueHolder添加到BeanDefinition bd.getConstructorArgumentValues().addGenericArgumentValue(valueHolder); &#125; finally &#123; this.parseState.pop(); &#125; &#125;&#125; property标签解析 1234567891011121314151617181920212223242526272829303132333435public void parsePropertyElements(Element beanEle, BeanDefinition bd) &#123; NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, PROPERTY_ELEMENT)) &#123; parsePropertyElement((Element) node, bd); &#125; &#125;&#125;public void parsePropertyElement(Element ele, BeanDefinition bd) &#123; //获取name属性 String propertyName = ele.getAttribute(NAME_ATTRIBUTE); if (!StringUtils.hasLength(propertyName)) &#123; error(\"Tag 'property' must have a 'name' attribute\", ele); return; &#125; this.parseState.push(new PropertyEntry(propertyName)); try &#123; if (bd.getPropertyValues().contains(propertyName)) &#123; error(\"Multiple 'property' definitions for property '\" + propertyName + \"'\", ele); return; &#125; Object val = parsePropertyValue(ele, bd, propertyName); //将属性设置包装成 PropertyValue PropertyValue pv = new PropertyValue(propertyName, val); parseMetaElements(ele, pv); pv.setSource(extractSource(ele)); //将 PropertyValue添加到BeanDefinition bd.getPropertyValues().addPropertyValue(pv); &#125; finally &#123; this.parseState.pop(); &#125;&#125; 和解析构造函数的参数一样,对于property标签的解析,同样是将key,value封装成PropertyValue,添加到BeanDefinition中,形成MutablePropertyValues类型 qualifier标签解析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public void parseQualifierElements(Element beanEle, AbstractBeanDefinition bd) &#123; NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, QUALIFIER_ELEMENT)) &#123; parseQualifierElement((Element) node, bd); &#125; &#125;&#125;public void parseQualifierElement(Element ele, AbstractBeanDefinition bd) &#123; String typeName = ele.getAttribute(TYPE_ATTRIBUTE); if (!StringUtils.hasLength(typeName)) &#123; error(\"Tag 'qualifier' must have a 'type' attribute\", ele); return; &#125; this.parseState.push(new QualifierEntry(typeName)); try &#123; AutowireCandidateQualifier qualifier = new AutowireCandidateQualifier(typeName); qualifier.setSource(extractSource(ele)); String value = ele.getAttribute(VALUE_ATTRIBUTE); if (StringUtils.hasLength(value)) &#123; qualifier.setAttribute(AutowireCandidateQualifier.VALUE_KEY, value); &#125; NodeList nl = ele.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, QUALIFIER_ATTRIBUTE_ELEMENT)) &#123; Element attributeEle = (Element) node; String attributeName = attributeEle.getAttribute(KEY_ATTRIBUTE); String attributeValue = attributeEle.getAttribute(VALUE_ATTRIBUTE); if (StringUtils.hasLength(attributeName) &amp;&amp; StringUtils.hasLength(attributeValue)) &#123; BeanMetadataAttribute attribute = new BeanMetadataAttribute(attributeName, attributeValue); attribute.setSource(extractSource(attributeEle)); qualifier.addMetadataAttribute(attribute); &#125; else &#123; error(\"Qualifier 'attribute' tag must have a 'name' and 'value'\", attributeEle); return; &#125; &#125; &#125; bd.addQualifier(qualifier); &#125; finally &#123; this.parseState.pop(); &#125;&#125; 至此,BeanDefinition的解析完成,此时再回到BeanDefinitionParserDelegate.parseBeanDefinitionElement(org.w3c.dom.Element, org.springframework.beans.factory.config.BeanDefinition) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) &#123; String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;&gt;(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; beanName = aliases.remove(0); if (logger.isTraceEnabled()) &#123; logger.trace(\"No XML 'id' specified - using '\" + beanName + \"' as bean name and \" + aliases + \" as aliases\"); &#125; &#125; //检查beanName是否重复 if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; //点进去 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); // Register an alias for the plain bean class name, if still possible, // if the generator returned the class name plus a suffix. // This is expected for Spring 1.2/2.0 backwards compatibility. String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(\"Neither XML 'id' nor 'name' specified - \" + \"using generated bean name [\" + beanName + \"]\"); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; return null;&#125; 这里返回的是,又对BeanDefinition做了一层包装,成BeanDefinitionHolder,形成name-&gt;BeanDefinition的映射","categories":[{"name":"Spring","slug":"Spring","permalink":"https://lurongjiang.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://lurongjiang.github.io/tags/Spring/"}]},{"title":"Gradle的安装和使用","slug":"Gradle-安装和使用","date":"2020-01-15T04:45:14.000Z","updated":"2020-01-16T08:14:14.000Z","comments":true,"path":"2020/01/15/Gradle-安装和使用/","link":"","permalink":"https://lurongjiang.github.io/2020/01/15/Gradle-%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","excerpt":"和Maven类似的项目依赖管理和构建工具Gradle的简单教程","text":"Gradle的安装和使用 下载 可以去Gradle官网下载最新的稳定版本,目前是6.2,我自己下的4.8.1 https://downloads.gradle-dn.com/distributions/gradle-4.8.1-bin.zip 安装 Gradle的安装和Maven类似,就简单的解压,配置环境变量到Path就ok了 配置完成之后,打开cmd查看一下是否配置好了 看到正确输出了gradle的版本就说明配置好了 IDEA配置Gradle Gradle和Maven这一点不同,Gradle无需再IDEA中进行配置操作,本地仓库地址的配置可以再IDEA中配置 这个我用的是环境变量来配置的,IDEA会自动识别,只需要在环境变量中新建一个GRADLE_USER_HOME变量指向自己的本地仓库地址就可以了 Gradle初体验 新建Gradle工程 选择Gradle和JDK 填写项目的GAV 填写项目的GAV坐标,点Finished Gradle的目录 Gradle的目录结构和Maven类似 src就是source目录 src/main放代码目录 src/main/java 放java代码目录 src/main/resouces放资源文件 src/test是测试目录 src/test/java 放测试的java代码目录 src/test/resouces放测试的资源文件 Groovy编程 打开Groovy Console Tools-&gt;Groovy Console HelloWorld 凡事先HelloWorld一下 Groovy语法 Groovy可以省略最末尾的分号 Groovy可以省略小括号 这两个特性可以看出,Groovy的书写更加自由,随意 定义变量 def groovy会根据数据自动推断类型 定义集合 定义Map 闭包 闭包就是一段代码块,在Gradle中主要是把闭包当参数使用 带参数的闭包 Gradle配置文件 build.gradle 构建项目配置 优先本地加载 在repositories中指定先从本地加载","categories":[{"name":"Gradle","slug":"Gradle","permalink":"https://lurongjiang.github.io/categories/Gradle/"}],"tags":[{"name":"Gradle","slug":"Gradle","permalink":"https://lurongjiang.github.io/tags/Gradle/"}]},{"title":"Redis缓存问题","slug":"Redis-Redis-缓存问题","date":"2019-10-11T02:23:09.000Z","updated":"2019-10-14T03:58:37.000Z","comments":true,"path":"2019/10/11/Redis-Redis-缓存问题/","link":"","permalink":"https://lurongjiang.github.io/2019/10/11/Redis-Redis-%E7%BC%93%E5%AD%98%E9%97%AE%E9%A2%98/","excerpt":"Redis缓存穿透,雪崩,穿透是怎么回事,如何解决这些问题","text":"Redis缓存问题 缓存穿透 恶意用户刻意伪造数据库不存在的数据进行大量访问,此时,由于数据库中没有,Redis也没把该信息缓存,请求过来之后,发现缓存没有,就去数据库中查找,而短时间内这样的大量请求会导致数据库负载压力大,甚至崩溃 解决方案 非法请求赋值一个特殊值,下次请求时,不查数据库 布隆过滤器 将可能出现的结果映射到一个大的集合,每次都从集合中获取 缓存雪崩 如果key的过期周期都差不多,在一段时间内没用访问之后,所有key的缓存都失效了,此时,如果大量请求过来,那么请求也都落到数据库,数据库瞬间崩盘 解决方案 过期时间设置可以包含一定的随机数,保证不会同时失效,也就是时间错开 使用setnx设置互斥锁,当有一个线程去load数据到缓存时,其他只能在队列等待,当load回来之后,如果命中就不需要去数据库查找了 缓存击穿 热点数据的缓存失效,如果短时间内大量请求,也会对数据库造成压力甚至崩溃 解决方案 数据预热,提前对热点数据加载到缓存 使用互斥锁加载数据到缓存 热点数据生命周期设置为不过期,热度过去之后再设置为常规声明周期","categories":[{"name":"Redis","slug":"Redis","permalink":"https://lurongjiang.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://lurongjiang.github.io/tags/Redis/"}]},{"title":"CDH-镜像恢复","slug":"CDH-CDH镜像恢复","date":"2019-09-03T09:15:01.000Z","updated":"2019-09-08T04:04:30.000Z","comments":true,"path":"2019/09/03/CDH-CDH镜像恢复/","link":"","permalink":"https://lurongjiang.github.io/2019/09/03/CDH-CDH%E9%95%9C%E5%83%8F%E6%81%A2%E5%A4%8D/","excerpt":"SCDH-集群部署详细文档","text":"CDH镜像恢复 阿里云使用oss对cdh集群进行了快照存储,需要使用时,需要使用镜像来创建实例,以恢复cdh集群 镜像创建实例 选择按量付费 检查mysql是否正常 修改hosts 镜像恢复之后,内网地址发生了变化,所以需要再次进行hosts的修改 检查cm-server db.properties com.cloudera.cmf.db.hostname=xx 检查cm-agent config.ini server_host=xxx 检查mysql cmf库的hosts update为相应的host 启动cm-server 启动agent web界面启动scm,cdh","categories":[{"name":"CDH","slug":"CDH","permalink":"https://lurongjiang.github.io/categories/CDH/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://lurongjiang.github.io/tags/CDH/"}]},{"title":"CDH-卸载","slug":"CDH-CDH的卸载","date":"2019-08-30T14:27:45.000Z","updated":"2019-09-07T14:46:55.000Z","comments":true,"path":"2019/08/30/CDH-CDH的卸载/","link":"","permalink":"https://lurongjiang.github.io/2019/08/30/CDH-CDH%E7%9A%84%E5%8D%B8%E8%BD%BD/","excerpt":"SCDH-集群部署详细文档","text":"CDH的卸载 卸载前规划 关闭集群和MySQL 删除部署文件 /opt/cloudera* 删除数据文件夹 /etc/xxx 存储目录 web-&gt;cluster-&gt;服务-&gt;选择对应的Configuration-&gt;data directory 如 /dfs/nn /dfs/dn /dfs/snn /yarn/nm /var/lib/zookeeper 卸载 web 关闭scm 关闭cdh 服务器 mysql drop database cmf drop user cmf drop user amon 关闭agent 关闭server 关闭cloudera进程 kill -9 $(pgrep -f cloudera) 校验df -h umount xx 部署文件夹删除,/opt/cloudera* 删除隐藏文件 /tmp/scm* /tmp/.scm* 全局搜索 find / -name ‘*clouder*‘ /etc/alternatives动态管理,软连接删除","categories":[{"name":"CDH","slug":"CDH","permalink":"https://lurongjiang.github.io/categories/CDH/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://lurongjiang.github.io/tags/CDH/"},{"name":"CM","slug":"CM","permalink":"https://lurongjiang.github.io/tags/CM/"}]},{"title":"CDH-集群部署","slug":"CDH-CHD集群部署","date":"2019-08-28T12:52:49.000Z","updated":"2019-09-11T14:01:27.000Z","comments":true,"path":"2019/08/28/CDH-CHD集群部署/","link":"","permalink":"https://lurongjiang.github.io/2019/08/28/CDH-CHD%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","excerpt":"SCDH-集群部署详细文档","text":"CHD集群部署 软件 1234567CM5.16.1http:&#x2F;&#x2F;archive.cloudera.com&#x2F;cm5&#x2F;cm&#x2F;5&#x2F;cloudera-manager-centos7-cm5.16.1_x86_64.tar.gzCDH5.16.1http:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh5&#x2F;parcels&#x2F;5.16.1&#x2F;CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcelhttp:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh5&#x2F;parcels&#x2F;5.16.1&#x2F;CDH-5.16.1-1.cdh5.16.2.p0.3-el7.parcel.sha1http:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh5&#x2F;parcels&#x2F;5.16.1&#x2F;manifest.json 集群规划 节点 部署 hadoop001 mysql,cm-server,cm-agent,nn,sn,dn,rm,nm,zk,kafka-broker hadoop002 cm-agent,dn,nm,zk,kafka-broker hadoop003 cm-agent,dn,nm,zk,kafka-broker 关闭防火墙 123systemctl stop firewalldsystemctl disable firewalldiptables -F 关闭selinux 12vi /etc/selinux/config SELINUX=diabled 时间同步ntp(云主机忽略) 1234datetimedatectltimedatectl list-timezonestimedatectl set-timezone Asia/Shanghai 选取hadoop001作为ntp主节点 hadoop002,hadoop003为从节点 123456789101112131415161718yum install -y ntpvi /etc/ntp.cnf # 设置time server 1.aisa.pool.ntp.org server 2.aisa.pool.ntp.org server 3.aisa.pool.ntp.org # 当外部time不可用,使用本机 server 127.0.0.1 inburst local lock # 允许哪些网段可以来同步时间 restrict hadoop002 mask 255.255.255.0 nomodify notrap restrict hadoop003 mask 255.255.255.0 nomodify notrap# 启动ntpsystemctl start ntpdsystemctl restart ntpd# 查看状态systemctl status ntpd# 验证ntp -q 另外节点不需要ntp,关闭ntp 12systemctl stop ntpdsystemctl disable ntpd 同步hadoop001时间 1ntpdate hadoop001 配置crontab进行定时同步 12345# 每天0点0分同步ntpcrontab -e 0 0 * * * ntpdate hadoop001# 查看定时任务crontab -l 免密登录 为了方便拷贝东西,做一下免密 12345678910ssh-keygen #之后一路entercat ~/.ssh/id_rsa.pub # 将三台的公钥拷贝到文本中# 将公钥写入三台的authorized_keysvi ~/.ssh/authorized_keys # 修改文件权限chmod 0600 ~/.ssh/authorized_keys# 测试ssh hadoop001 datessh hadoop002 datessh hadoop003 date 拷贝文件到另外两台 1scp -r hadoop001:~/cdh5.16.1/* ~/cdh5.16.1/ 部署JDK jdk统一部署到/usr/java下,避免乱七八糟的错误 123456789101112131415tar -zxvf jdk-8u45-linux-x64.gzmkdir /usr/javamv ./jdk1.8.0_45 /usr/java# 修改权限chown -R root:root /usr/java/jdk1.8.0_45ln -s /usr/java/jdk1.8.0_45 /usr/java/java8vi ~/.bash_profile export JAVA_HOME=/usr/java/java8 export PATH=$JAVA_HOME/bin:$PATHsource ~/.bash_profile# 查看一下jdk版本java -version 部署MySQL 解压 1234tar -zxvf mysql-5.7.11-linux-glibc2.5-x86_64.tar.gzchown -R root:root ./*ln -s /root/cdh5.16.1/mysql-5.7.11-linux-glibc2.5-x86_64 /usr/local/mysqlmkdir /usr/local/mysql/arch /usr/local/mysql/data /usr/local/mysql/tmp my.cnf 12cp /etc/my.cnf /etc/my.cnf.bakvi /etc/my.cnf #先清空 配置my.cnf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125[client]port &#x3D; 3306socket &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;mysql.sockdefault-character-set&#x3D;utf8mb4[mysqld]port &#x3D; 3306socket &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;mysql.sockskip-slave-startskip-external-lockingkey_buffer_size &#x3D; 256Msort_buffer_size &#x3D; 2Mread_buffer_size &#x3D; 2Mread_rnd_buffer_size &#x3D; 4Mquery_cache_size&#x3D; 32Mmax_allowed_packet &#x3D; 16Mmyisam_sort_buffer_size&#x3D;128Mtmp_table_size&#x3D;32Mtable_open_cache &#x3D; 512thread_cache_size &#x3D; 8wait_timeout &#x3D; 86400interactive_timeout &#x3D; 86400max_connections &#x3D; 600# Try number of CPU&#39;s*2 for thread_concurrency#thread_concurrency &#x3D; 32 #isolation level and default engine default-storage-engine &#x3D; INNODBtransaction-isolation &#x3D; READ-COMMITTEDserver-id &#x3D; 1739basedir &#x3D; &#x2F;usr&#x2F;local&#x2F;mysqldatadir &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;datapid-file &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;hostname.pid#open performance schemalog-warningssysdate-is-nowbinlog_format &#x3D; ROWlog_bin_trust_function_creators&#x3D;1log-error &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;hostname.errlog-bin &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;arch&#x2F;mysql-binexpire_logs_days &#x3D; 7innodb_write_io_threads&#x3D;16relay-log &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;relay_log&#x2F;relay-logrelay-log-index &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;relay_log&#x2F;relay-log.indexrelay_log_info_file&#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;relay_log&#x2F;relay-log.infolog_slave_updates&#x3D;1gtid_mode&#x3D;OFFenforce_gtid_consistency&#x3D;OFF# slaveslave-parallel-type&#x3D;LOGICAL_CLOCKslave-parallel-workers&#x3D;4master_info_repository&#x3D;TABLErelay_log_info_repository&#x3D;TABLErelay_log_recovery&#x3D;ON#other logs#general_log &#x3D;1#general_log_file &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;general_log.err#slow_query_log&#x3D;1#slow_query_log_file&#x3D;&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;slow_log.err#for replication slavesync_binlog &#x3D; 500#for innodb options innodb_data_home_dir &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data&#x2F;innodb_data_file_path &#x3D; ibdata1:1G;ibdata2:1G:autoextendinnodb_log_group_home_dir &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;archinnodb_log_files_in_group &#x3D; 4innodb_log_file_size &#x3D; 1Ginnodb_log_buffer_size &#x3D; 200M#根据生产需要，调整pool size innodb_buffer_pool_size &#x3D; 2G#innodb_additional_mem_pool_size &#x3D; 50M #deprecated in 5.6tmpdir &#x3D; &#x2F;usr&#x2F;local&#x2F;mysql&#x2F;tmpinnodb_lock_wait_timeout &#x3D; 1000#innodb_thread_concurrency &#x3D; 0innodb_flush_log_at_trx_commit &#x3D; 2innodb_locks_unsafe_for_binlog&#x3D;1#innodb io features: add for mysql5.5.8performance_schemainnodb_read_io_threads&#x3D;4innodb-write-io-threads&#x3D;4innodb-io-capacity&#x3D;200#purge threads change default(0) to 1 for purgeinnodb_purge_threads&#x3D;1innodb_use_native_aio&#x3D;on#case-sensitive file names and separate tablespaceinnodb_file_per_table &#x3D; 1lower_case_table_names&#x3D;1[mysqldump]quickmax_allowed_packet &#x3D; 128M[mysql]no-auto-rehashdefault-character-set&#x3D;utf8mb4[mysqlhotcopy]interactive-timeout[myisamchk]key_buffer_size &#x3D; 256Msort_buffer_size &#x3D; 256Mread_buffer &#x3D; 2Mwrite_buffer &#x3D; 2M 配置用户组 1234567groupadd -g 101 dbauseradd -u 514 -g dba -G root -d /usr/local/mysql mysqladmin# 查看一下 uid=514(mysqladmin) gid=101(dba) groups=101(dba),0(root)id mysqladmin# 拷贝环境变量文件到mysqladmin home目录cp /etc/skel/.* /usr/local/mysql5.7 配置mysqladmin环境变量 12su - mysqladminvi .bash_profile 追加 123456export MYSQL_BASE&#x3D;&#x2F;usr&#x2F;local&#x2F;mysqlexport PATH&#x3D;$&#123;MYSQL_BASE&#125;&#x2F;bin:$PATHunset USERNAMEset umask to 022umask 022PS1&#x3D;&#96;uname -n&#96;&quot;:&quot;&#39;$USER&#39;&quot;:&quot;&#39;$PWD&#39;&quot;:&gt;&quot;; export PS1 修改权限 1234chown mysqladmin:dba /etc/my.cnfchmod 640 /etc/my.cnfchown -R mysqladmin:dba /usr/local/mysqlchmod -R 755 /usr/local/mysql 自启动配置 1234cp /usr/local/mysql/support-files/mysql.server /etc/rc.d/init.d/mysqlchmod +x /etc/rc.d/init.d/mysqlchkconfig --add mysqlchkconfig --level 345 mysql on 按照mysql依赖 12yum -y install libaiosu - mysqladmin 初始化mysql 12345678910sudo su - mysqladminbin/mysqld \\--defaults-file=/etc/my.cnf \\--user=mysqladmin \\--basedir=/usr/local/mysql/ \\--datadir=/usr/local/mysql/data/ \\--initialize# 等待初始化完成后 eC%gbl&gt;g3KUJcat data/hostname.err|grep password 启动mysql并修改密码 1234567891011121314/usr/local/mysql/bin/mysqld_safe --defaults-file=/etc/my.cnf &amp;mysql -u root -p# 修改密码,赋权alter user root@localhost identified by 'root@1234';GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root@1234' ;create database cmf default character set utf8;GRANT ALL PRIVILEGES ON cmf.* TO 'cmf'@'%' IDENTIFIED BY 'cmf@1234' ;create database amon default character set utf8;GRANT ALL PRIVILEGES ON amon.* TO 'amon'@'%' IDENTIFIED BY 'amon@1234' ;flush privileges; 上传mysql jar 12345mkdir /usr/share/javacd /usr/share/javarzscp ./mysql-connector-java-5.1.47.jar hadoop002:/usr/share/javascp ./mysql-connector-java-5.1.47.jar hadoop003:/usr/share/java 安装CM 解压 12mkdir /opt/cmtar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cm 配置agent 1234cd /opt/cm/cm-5.16.1/etc/cloudera-scm-agentvi config.ini # 配置cmd_server server_host=hadoop001 配置cm-server 123456cd /opt/cm/cm-5.16.1/etc/cloudera-scm-servervi db.properties com.cloudera.cmf.db.host=hadoop001 com.cloudera.cmf.db.name=cmf com.cloudera.cmf.db.password=cmf@1234 com.cloudera.cmf.db.setupType=EXTERNAL 添加用户 123456useradd --system \\--home=/opt/cm/cm-5.16.1/run/cloudera-scm-server \\--no-create-home \\--shell=/bin/false \\--comment='ClouderManager User' \\cloudmanager 修改权限 1chown -R cloudmanager:cloudmanager /opt/cm Parcel部署 parcel将hdfs,zk,kafka等组件做了自己的版本兼容修改,打成自己的包 校验 记得校验一下是否下载完整了,相同就是下载完成了,否则安装时会继续下载 123mv CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.shacat CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.shasha1sum CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel 部署准备 1234mkdir -p /opt/cloudera/parcel-repomv CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel* /opt/cloudera/parcel-repomv manifest.json /opt/cloudera/parcel-repochown -R cloudmanager:cloudmanager /opt/cloudera 正式集群部署 创建目录和用户 所有节点创建软件的安装目录,用户和用户组 12mkdir -p /opt/cloudera/parcelschown -R cloudmanager:cloudmanager /opt/cloudera/ 启动 123456# 先启动mysqlsudo su - mysqladminservice mysql restartexitcd /opt/cm/cm-5.16.1/etc/init.d./cloudera-scm-server start 发现报错了…. 12install: invalid user ‘cloudera-scm’Starting cloudera-scm-server: [FAILED] 还是老实用人家的默认用户cloudera-scm 123456789useradd --system \\--home=/opt/cm/cm-5.16.1/run/cloudera-scm-server \\--no-create-home \\--shell=/bin/false \\--comment='ClouderManager User' \\cloudera-scmchown -R cloudera-scm:cloudera-scm /opt/cmchown -R cloudera-scm:cloudera-scm /opt/cloudera 继续启动,发现还是报错了,查看一下日志 1tailf -200 /opt/cm/cm-5.16.1/log/cloudera-scm-server/cloudera-scm-server.log 123chmod 755 /usr/share/java/mysql-connector-java-5.1.47.jar# 这个一定要做,去掉版本号,否则不认mv /usr/share/java/mysql-connector-java-5.1.47.jar /usr/share/java/mysql-connector-java.jar 安全组配置 开放7180 待程序启动后,访问7180端口,就可以看到cm界面了 默认账号密码就是admin/admin 启动agent 所有的agent启动 12/opt/cm/cm-5.16.1/etc/init.d/cloudera-scm-agent starttailf -200 /opt/cm/cm-5.16.1/log/cloudera-scm-agent/cloudera-scm-agent.log 配置cm 选择免费版 选择自己下载好的parcel版本,不然会重新下载,很慢 去掉警告 12echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled 选择自定义服务,按需安装hdfs,zk,yarn 关闭集群 web界面关闭cdh和cms 关闭agent 1/opt/cm/cm-5.16.1/etc/init.d/cloudera-scm-agent stop 关闭server 1/opt/cm/cm-5.16.1/etc/init.d/cloudera-scm-server stop 关闭mysql 12su - mysqladminservice mysql stop 关闭实例 为实例创建镜像(需要购买OSS快照存储),释放实例","categories":[{"name":"CDH","slug":"CDH","permalink":"https://lurongjiang.github.io/categories/CDH/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://lurongjiang.github.io/tags/CDH/"},{"name":"CM","slug":"CM","permalink":"https://lurongjiang.github.io/tags/CM/"}]},{"title":"SparkStreaming基础-SparkStreaming的tranform和与Kakfa集成","slug":"Spark-SparkStreaming-2","date":"2019-06-07T06:57:53.000Z","updated":"2019-06-08T20:12:28.000Z","comments":true,"path":"2019/06/07/Spark-SparkStreaming-2/","link":"","permalink":"https://lurongjiang.github.io/2019/06/07/Spark-SparkStreaming-2/","excerpt":"SparkStreaming基础-SparkStreaming的tranform操作和与kafka集成","text":"SparkStreaming-2 tranform Spark Streaming接收到数据得到的是一个DStream,如果需要DStream和RDD进行关联,此时并没有DStream和RDD关联的API,所以需要tranform算子来进行DStream和RDD的关联 例如,我们只要统计指定的单词 123456789101112131415161718192021222324252627object TransformApp &#123; def main(args: Array[String]): Unit = &#123; def createStreamContext() = &#123; val conf = new SparkConf().setAppName(\"transformApp\").setMaster(\"local[2]\") val sc = new SparkContext(conf) //只统计hello,word val wordRDD = sc.parallelize(List(\"hello\", \"word\")).map((_, 1)) val ssc = new StreamingContext(sc, Seconds(5)) val stream = ssc.socketTextStream(\"hadoop001\", 9000) def updateState = (n: Seq[Int], o: Option[Int]) =&gt; &#123; val sum = n.sum + o.getOrElse(0) Some(sum) &#125; stream.flatMap(_.split(\",\")).map((_, 1)).transform(rdd =&gt; &#123; rdd.join(wordRDD) &#125;).map(t =&gt; (t._1, t._2._1)).updateStateByKey(updateState).print() ssc.checkpoint(\"checkpoint\") ssc &#125; val ssc = StreamingContext.getOrCreate(\"checkpoint\", createStreamContext) ssc.start() ssc.awaitTermination() &#125;&#125; Window 滑窗有两个重要的概念 window length - 窗口的间隔 sliding interval - 每次滑移的间隔 这两个值必须是DStream的间隔时间的倍数,例如DStream可能为每5秒钟一次,window length=30s,则相当于每次的窗口数据包含了6个DStream间隔,silding interval=10s,那么就是10秒滑动一次 例如我们想,每隔10s统计30s中以内的wc 123456789101112131415161718192021222324object WindowApp &#123; def main(args: Array[String]): Unit = &#123; def createStreamContext() = &#123; val conf = new SparkConf().setAppName(\"WindowApp\").setMaster(\"local[2]\") val sc = new SparkContext(conf) val ssc = new StreamingContext(sc, Seconds(5)) val stream = ssc.socketTextStream(\"hadoop001\", 9000) def updateState = (seq: Seq[Int], oldValue: Option[Int]) =&gt; &#123; Some(seq.sum) &#125; stream.flatMap(_.split(\",\")).map((_, 1)) .reduceByKeyAndWindow((a:Int,b:Int)=&gt;a+b, Seconds(30), Seconds(10)) .updateStateByKey(updateState).print() ssc.checkpoint(\"checkpoint\") ssc &#125; val ssc = StreamingContext.getOrCreate(\"checkpoint\", createStreamContext) ssc.start() ssc.awaitTermination() &#125;&#125; 这样运行后就可以看到,不在时间范围内的就不会被统计 Output Operations on DStreams 输出算子 SparkStreaming作为流处理,一般不会输出到文件系统如hdfs,或者保存为文件,因为这样可能会导致大量的小文件问题,一般常用的算子为 print foreachRDD 使用最多的算子,在Streaming中,一般全都要使用foreachRDD来操作 1234567891011121314151617181920212223242526272829object ForeachRDDApp &#123; def main(args: Array[String]): Unit = &#123; def createStreamContext() = &#123; val conf = new SparkConf().setAppName(\"ForeachRDDApp\").setMaster(\"local[2]\") val sc = new SparkContext(conf) val ssc = new StreamingContext(sc, Seconds(5)) val stream = ssc.socketTextStream(\"hadoop001\", 9000) def updateState = (seq: Seq[Int], oldValue: Option[Int]) =&gt; &#123; Some(seq.sum) &#125; stream.flatMap(_.split(\",\")).map((_, 1)) .reduceByKeyAndWindow((a: Int, b: Int) =&gt; a + b, Seconds(30), Seconds(10)) .updateStateByKey(updateState).foreachRDD((rdd, time) =&gt; &#123; //也可以写到数据库,这里打印方便点 val array = rdd.collect() val str = array.mkString(\",\") println(s\"time:$time----&gt;$str\") &#125;) ssc.checkpoint(\"checkpoint\") ssc &#125; val ssc = StreamingContext.getOrCreate(\"checkpoint\", createStreamContext) ssc.start() ssc.awaitTermination() &#125;&#125; 运行效果 Design Patterns for using foreachRDD foreachRDD可以将数据写到外部系统中,但是使用不当会导致一些错误.常见错误如,创建连接,如何写到外部,但是由于连接之类的很少实现了序列化,所以会导致不能序列化的错误 错误例子 连接创建在Driver端,但是使用是在worker端,这时需要序列化,所以这种用法是错误的 123456dstream.foreachRDD &#123; rdd =&gt; val connection = createNewConnection() // executed at the driver rdd.foreach &#123; record =&gt; connection.send(record) // executed at the worker &#125;&#125; 另一种常见的错误用法是,每个rdd都创建连接,这样增加系统的负荷 1234567dstream.foreachRDD &#123; rdd =&gt; rdd.foreach &#123; record =&gt; val connection = createNewConnection() connection.send(record) connection.close() &#125;&#125; 正确的方式是使用连接池,用完归还 12345678dstream.foreachRDD &#123; rdd =&gt; rdd.foreachPartition &#123; partitionOfRecords =&gt; // ConnectionPool is a static, lazily initialized pool of connections val connection = ConnectionPool.getConnection() partitionOfRecords.foreach(record =&gt; connection.send(record)) ConnectionPool.returnConnection(connection) // return to the pool for future reuse &#125;&#125; 连接池的连接最好做成懒加载的模式,不使用一段时间后进行销毁,避免占用资源 Other points to remember DStream只有遇到输出算子才会执行(和RDD的懒加载一样,只有遇到action才执行),DStream内部RDD的action会强制DStream处理接收到的数据,如果你的应用没有设置输出算子,或者只有foreachRDD算子而内部没有RDD的action算子,那么不会有任何输出 Input DStreams and Receivers Basic Sources Advanced Sources Kafka Source Spark Streaming和Kafka集成的 0.10版本,Kafka的partition和SparkStreaming的partition是一致的,保持1:1 添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_$&#123;scala.tool.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt; Creating a Direct Stream 12345678910111213141516171819202122232425262728293031323334353637object KafkaStreamApp &#123; def main(args: Array[String]): Unit = &#123; def createStreamContext() = &#123; val conf = new SparkConf().setAppName(\"KafkaStreamApp\").setMaster(\"local[2]\") val sc = new SparkContext(conf) val ssc = new StreamingContext(sc, Seconds(5)) val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; \"hadoop001:9091,hadoop001:9092,hadoop001:9093\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"myGroup\", \"auto.offset.reset\" -&gt; \"latest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"testA\") val stream = KafkaUtils.createDirectStream[String, String]( ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams) ) def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = &#123; Some(newValues.sum) &#125; stream.map(record =&gt; (record.value,1)) .reduceByKeyAndWindow((a:Int,b:Int)=&gt;a+b,Seconds(10),Seconds(5)) .updateStateByKey(updateFunction).print() ssc.checkpoint(\"checkpoint\") ssc &#125; val ssc = StreamingContext.getOrCreate(\"checkpoint\", createStreamContext) ssc.start() ssc.awaitTermination() &#125;&#125; 创建消费者 12345678910111213141516object MyKafkaProducer &#123; def main(args: Array[String]): Unit = &#123; val props = new Properties(); props.put(\"bootstrap.servers\", \"hadoop001:9091,hadoop001:9092,hadoop001:9093\") props.put(\"acks\", \"all\") props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") val producer = new KafkaProducer[String, String](props) for (x &lt;- 0 to 1000) &#123; val a = 'A' val word = (Random.nextInt(23) + a).toChar+\"\" producer.send(new ProducerRecord[String, String](\"testA\", x%3, x+\"\", word)) TimeUnit.MICROSECONDS.sleep(200) &#125; &#125;&#125; 输出 Obtaining Offsets获取偏移量 获取消费的偏移量 1234567stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd.foreachPartition &#123; iter =&gt; val o: OffsetRange = offsetRanges(TaskContext.get.partitionId) println(s\"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;\") &#125;&#125; Storing Offsets存储偏移量 123456stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges // some time later, after outputs have completed stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)&#125; 设置kafka数据的偏移量 1234567891011121314151617181920212223val fromOffsets = selectOffsetsFromYourDatabase.map &#123; resultSet =&gt; new TopicPartition(resultSet.string(\"topic\"), resultSet.int(\"partition\")) -&gt; resultSet.long(\"offset\")&#125;.toMapval stream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Assign[String, String](fromOffsets.keys.toList, kafkaParams, fromOffsets))stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges val results = yourCalculation(rdd) // begin your transaction // update results // update offsets where the end of existing offsets matches the beginning of this batch of offsets // assert that offsets were updated correctly // end your transaction&#125;","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkStreaming","slug":"SparkStreaming","permalink":"https://lurongjiang.github.io/tags/SparkStreaming/"},{"name":"SparkStreaming-Kakfa","slug":"SparkStreaming-Kakfa","permalink":"https://lurongjiang.github.io/tags/SparkStreaming-Kakfa/"}]},{"title":"SparkStreaming基础-SparkStreaming的基本使用","slug":"Spark-SparkStream","date":"2019-05-20T12:08:29.000Z","updated":"2019-05-24T11:55:21.000Z","comments":true,"path":"2019/05/20/Spark-SparkStream/","link":"","permalink":"https://lurongjiang.github.io/2019/05/20/Spark-SparkStream/","excerpt":"SparkStreaming基础-SparkStreaming的基本使用,HelloWorld","text":"SparkStream-1 SparkStream概述 SparkCore的拓展 高吞吐量,容错在线流数据处理 数据源诸如:Kafka,Flume,TCP sockets 可以使用高阶函数来进行复炸的操作 可以输出到文件系统,数据库等 数据按时间间隔切分成批次交由Spark引擎处理 DStream概述 基本抽象DStream,代表着连续的数据流 DStream可由Kafka,Flume,TCP socket产生 WordCount 首先创建SparkStream入口StreamContext 123val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"wc\")val sc = new SparkContext(conf)val ssc = new StreamingContext(sc, Seconds(5)) socketTextStream 创建DStream 1val stream = ssc.socketTextStream(\"hadoop001\", 9000) wc逻辑 12345stream.flatMap(x =&gt; x.split(\",\")).map((_, 1)).reduceByKey(_ + _).print()//启动streamssc.start()ssc.awaitTermination() 注意 socketTextStream底层会创建socket连接进行监听,所以core必须大于等于receiver数量,否则没有可用的core来处理数据,导致stream无限挂起 123456789101112131415161718192021222324252627class SocketReceiver[T: ClassTag]( host: String, port: Int, bytesToObjects: InputStream =&gt; Iterator[T], storageLevel: StorageLevel ) extends Receiver[T](storageLevel) with Logging &#123; private var socket: Socket = _ def onStart() &#123; logInfo(s\"Connecting to $host:$port\") try &#123; socket = new Socket(host, port) &#125; catch &#123; case e: ConnectException =&gt; restart(s\"Error connecting to $host:$port\", e) return &#125; logInfo(s\"Connected to $host:$port\") // Start the thread that receives data over a connection new Thread(\"Socket Receiver\") &#123; setDaemon(true) override def run() &#123; receive() &#125; &#125;.start()&#125; 注意点: streaming.start()之后,任何stream的操作都没用了,所以所有的额streaming操作都要在start之前 一旦stream stop,无法重启 DStream DStream是一系列的RDD,按时间间隔切分 对DStream的转换操作就是对DStream代表的所以RDD进行相同的操作 Input DStreams and Receivers SparkStream提供了两种内置的DStream数据源 基本的数据源,如fileSystem,socket 高级的数据源,如kafka,flume Points to remember 本地运行,如果使用基于receiver的DStream时,不能使用local/local[1],因为这样只有一个core,而这个core用于接收数据了,就没用剩余的core来处理数据了 拓宽到集群,core的数量也必须大于receiver的数量 但是不基于receiver的DStream就没有这个限制了,如textFileStream SparkStreaming只能监听到启动之后进来的数据,之前的数据是没法监听到的 DStream的count是按行计算的记录数,如果需要知道元素的个数,需要在遍历每一行进行计算 Transformations on DStreams DStream除了可以使用高阶函数(如map,filter,flatMap等),还提供了updateStateByKey算子,用于更新DStream的状态,例如每批数据进来之后对wc中相同的key进行累加操作 UpdateStateByKey Operation 为了使用这个算子,需要: 定义State,可为任意类型 定义更新操作函数,处理旧值和新值 1234567891011121314151617181920object WcStateApp &#123; def main(args: Array[String]): Unit = &#123; //1.创建StreamingContext val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"wc\") val sc = new SparkContext(conf) val ssc = new StreamingContext(sc, Seconds(5)) def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = &#123; val newCount = newValues.sum + runningCount.getOrElse(0) Some(newCount) &#125; //2.创建DStream,这里监听socket val stream = ssc.socketTextStream(\"hadoop001\", 9000) //指定一个checkpoint目录,这样streaming就可以获取到以前的统计了 ssc.checkpoint(\"checkpoint\") stream.flatMap(x =&gt; x.split(\",\")).map((_, 1)).updateStateByKey(updateFunction).print() ssc.start() ssc.awaitTermination() &#125; 此时,再次启动wc程序,已经发现可以对批次累加了 但是这里还是有问题的,因为只要stream重启,以前的统计值就没法拿到了,我们应该从checkpoint中创建StreamingContext,这个才checkponit中做了相关的论述 需要对程序做一下修改 1234567891011121314151617181920212223242526object WcStateApp &#123; def main(args: Array[String]): Unit = &#123; //1.创建StreamingContext def functionToCreateContext(): StreamingContext = &#123; def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = &#123; val newCount = newValues.sum + runningCount.getOrElse(0) Some(newCount) &#125; val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"wc\") val sc = new SparkContext(conf) val ssc = new StreamingContext(sc, Seconds(5)) //2.创建DStream,这里监听socket val stream = ssc.socketTextStream(\"hadoop001\", 9000) stream.flatMap(x =&gt; x.split(\",\")).map((_, 1)).updateStateByKey(updateFunction).print() ssc.checkpoint(\"checkpoint\") ssc &#125; // Get StreamingContext from checkpoint data or create a new one val ssc = StreamingContext.getOrCreate(\"checkpoint\", functionToCreateContext _) ssc.start() ssc.awaitTermination() &#125;&#125; 再启动时,可以发现历史的已经可以从checkpoint中都出来了,但是历史闪的速度很快,并不是我们指定duration mapWithState 新版提供了mapWithState算子来管理state,与updateStateByKey不同的是,每接收到元素就会进行mappingFunction,而不是一批一批的mappingFunction,所以会导致同一批的统计中有很多值,如,一条记录是这样的: 1,5,4,2,1,4,1,41,5,4,2,1,4,1,41,5,4,2,1,4,1,41,5,4,2,1,4,1 此时,控制台的输出将是 可以看到,同一个key出现了多次,我们需要取最高值,而不是之前的值,感觉还不如updateStateByKey 1234567891011121314151617181920212223242526272829303132object WcMapWithStateApp &#123; def main(args: Array[String]): Unit = &#123; //1.创建StreamingContext def functionToCreateContext(): StreamingContext = &#123; val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"wc\") val sc = new SparkContext(conf) val ssc = new StreamingContext(sc, Seconds(5)) //2.创建DStream,这里监听socket val stream = ssc.socketTextStream(\"hadoop001\", 9000) def mappingFunction = (key: String, value: Option[Int], state: State[Int]) =&gt; &#123; // Use state.exists(), state.get(), state.update() and state.remove() // to manage state, and return the necessary string val sum = value.getOrElse(0) + state.getOption().getOrElse(0) state.update(sum) (key, sum) &#125; //新版提供了StateSpec.function(mappingFunction)进行mapWithState stream.flatMap(x =&gt; x.split(\",\")).map((_, 1)).mapWithState(StateSpec.function(mappingFunction)).print() ssc.checkpoint(\"checkpoint\") ssc &#125; // Get StreamingContext from checkpoint data or create a new one val ssc = StreamingContext.getOrCreate(\"checkpoint\", functionToCreateContext _) ssc.start() ssc.awaitTermination() &#125;&#125;","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkStreaming","slug":"SparkStreaming","permalink":"https://lurongjiang.github.io/tags/SparkStreaming/"}]},{"title":"SparkSQL基础-Spark SQL和RDD的交互","slug":"Spark-SparkSQL-和RDD的交互","date":"2019-05-04T18:11:54.000Z","updated":"2019-05-15T11:15:06.000Z","comments":true,"path":"2019/05/05/Spark-SparkSQL-和RDD的交互/","link":"","permalink":"https://lurongjiang.github.io/2019/05/05/Spark-SparkSQL-%E5%92%8CRDD%E7%9A%84%E4%BA%A4%E4%BA%92/","excerpt":"SparkCore-Spark SQL和RDD的交互","text":"SparkSQL-和RDD的交互 RDD到DataSet 使用反射的方式来推导SparkSQL的schema(如:已知case class) programmatic 反射方式 这种预先知道并定义了case class 准备数据 123Justin,25Andy,35Michael,14 程序 123456789101112131415161718object RddToDfTest &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(\"UdfTest\").master(\"local[2]\").getOrCreate() byCaseClass(spark) spark.close() &#125; case class Person(name:String,age:Int) def byCaseClass(spark: SparkSession): Unit =&#123; val rdd = spark.sparkContext.textFile(\"input/info.txt\") val person = rdd.map(str =&gt; &#123; val words = str.split(\",\") Person(words(0), words(1).toInt) &#125;) //导入转换,否则没有toDF import spark.implicits._ person.toDF().printSchema() &#125;&#125; output 123root |-- name: string (nullable &#x3D; true) |-- age: integer (nullable &#x3D; false) Row+Schema 获取RDD[String] RDD[String]转成RDD[Row] 定义schema 使用RDD和schema创建df program 123456789101112131415161718def byRow(spark: SparkSession): Unit =&#123; val rdd = spark.sparkContext.textFile(\"input/info.txt\") // 1.将RDD[String]=&gt;RDD[Row] val rowRDD = rdd.map(str =&gt; &#123; val words = str.split(\",\") Row(words(0),words(1).toInt) &#125;) // 2.定义schema val struct=StructType( StructField(\"name\",StringType):: StructField(\"age\",IntegerType)::Nil ) // 3.将row作用schema转成df val df = spark.createDataFrame(rowRDD, struct) df.printSchema() df.show()&#125; output 1234567891011root |-- name: string (nullable &#x3D; true) |-- age: integer (nullable &#x3D; true)+-------+---+| name|age|+-------+---+| Justin| 25|| Andy| 35||Michael| 14|+-------+---+","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"https://lurongjiang.github.io/tags/SparkSQL/"},{"name":"RDD","slug":"RDD","permalink":"https://lurongjiang.github.io/tags/RDD/"}]},{"title":"SparkSQL基础-Spark SQL-聚合和UDF","slug":"Spark-SparkSQL-聚合","date":"2019-05-01T17:59:02.000Z","updated":"2019-05-10T15:56:19.000Z","comments":true,"path":"2019/05/02/Spark-SparkSQL-聚合/","link":"","permalink":"https://lurongjiang.github.io/2019/05/02/Spark-SparkSQL-%E8%81%9A%E5%90%88/","excerpt":"SparkCore-Spark SQL常用的聚合和UDF操作","text":"SparkSQL-聚合和UDF 准备数据 12345678910111213141516subject,userId,sex,scoreSubjectA,user1,male,512SubjectA,user2,female,521SubjectA,user3,female,152SubjectA,user4,male,542SubjectA,user5,male,542SubjectB,user11,male,652SubjectB,user21,male,562SubjectB,user31,female,562SubjectB,user41,female,2344SubjectB,user51,male,133SubjectC,user111,male,455SubjectC,user211,female,4553SubjectC,user311,male,4553SubjectC,user411,female,455SubjectC,user511,male,4552 分组求和 1234567891011def groupBy(spark: SparkSession): Unit =&#123; import spark.implicits._ val df = spark.read .option(\"sep\", \",\") .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .format(\"csv\").load(\"input/groupby.txt\") val dataFrame = df.select($\"subject\", $\"sex\", $\"score\") import org.apache.spark.sql.functions._ dataFrame.groupBy(\"subject\",\"sex\").agg(sum($\"score\")).show()&#125; API方式需要导入org.apache.spark.sql.functions已经定义好的函数 output 12345678910+--------+------+----------+| subject| sex|sum(score)|+--------+------+----------+|SubjectC|female| 5008||SubjectC| male| 9560||SubjectB| male| 1347||SubjectB|female| 2906||SubjectA|female| 673||SubjectA| male| 1596|+--------+------+----------+ 分组topN 1234567891011121314def groupTopN(spark: SparkSession): Unit =&#123; val df = spark.read .option(\"sep\", \",\") .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .format(\"csv\").load(\"input/groupby.txt\") df.createTempView(\"subjects\") spark.sql( \"\"\" |select t.* from ( |select subject,sex,score,row_number() over (partition by subject,sex order by score desc) rn from subjects)t |where t.rn&lt;=2 |\"\"\".stripMargin).show()&#125; output 12345678910111213141516+--------+------+-----+---+| subject| sex|score| rn|+--------+------+-----+---+|SubjectC|female| 4553| 1||SubjectC|female| 455| 2||SubjectC| male| 4553| 1||SubjectC| male| 4552| 2||SubjectB| male| 652| 1||SubjectB| male| 562| 2||SubjectB|female| 2344| 1||SubjectB|female| 562| 2||SubjectA|female| 521| 1||SubjectA|female| 152| 2||SubjectA| male| 542| 1||SubjectA| male| 542| 2|+--------+------+-----+---+ UDF 准备数据 1234&#123;&quot;name&quot;:&quot;zhangsan&quot;,&quot;age&quot;:25,&quot;sex&quot;:0&#125;&#123;&quot;name&quot;:&quot;lisi&quot;,&quot;age&quot;:41,&quot;sex&quot;:1&#125;&#123;&quot;name&quot;:&quot;wangwu&quot;,&quot;age&quot;:32,&quot;sex&quot;:1&#125;&#123;&quot;name&quot;:&quot;zhaoliu&quot;,&quot;age&quot;:41,&quot;sex&quot;:0&#125; 程序 123456789101112131415161718object UdfTest &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(\"UdfTest\").master(\"local[2]\").getOrCreate() val sexFun = spark.udf.register(\"aa\", (str: String) =&gt; &#123; val toInt = str.toInt if (toInt == 0) &#123; \"male\" &#125; else &#123; \"female\" &#125; &#125;) val df = spark.read.json(\"input/people.txt\") import spark.implicits._ df.select($\"name\", $\"age\", sexFun($\"sex\").as(\"sex\")).show spark.close() &#125;&#125; register进行注册,后面调用一下就可以了 output 12345678+--------+---+------+| name|age| sex|+--------+---+------+|zhangsan| 25| male|| lisi| 41|female|| wangwu| 32|female|| zhaoliu| 41| male|+--------+---+------+","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"https://lurongjiang.github.io/tags/SparkSQL/"},{"name":"Agg","slug":"Agg","permalink":"https://lurongjiang.github.io/tags/Agg/"},{"name":"UDF","slug":"UDF","permalink":"https://lurongjiang.github.io/tags/UDF/"}]},{"title":"SparkSQL基础-Spark SQL-Source","slug":"Spark-SparkSQL-2","date":"2019-04-27T21:14:35.000Z","updated":"2019-04-29T20:43:04.000Z","comments":true,"path":"2019/04/28/Spark-SparkSQL-2/","link":"","permalink":"https://lurongjiang.github.io/2019/04/28/Spark-SparkSQL-2/","excerpt":"SparkCore-Spark SQL常用的数据源操作","text":"SparkSQL-Source JSON文件 读取 sparkSession.read.format(“json”).load(path) sparkSession.read.json(path) 准备一份数据 1234&#123;\"name\":\"zhangsan\",\"age\":18,\"sex\":\"male\"&#125;&#123;\"name\":\"lisi\",\"age\":32,\"sex\":\"female\"&#125;&#123;\"name\":\"wangwu\",\"age\":45,\"sex\":\"male\"&#125;&#123;\"name\":\"zhaoliu\",\"age\":87,\"sex\":\"male\"&#125; 程序 read.format(“json”).load(path) 12345678910111213141516171819object JsonSourceTest &#123; def main(args: Array[String]): Unit = &#123; //创建SparkSession val spark = SparkSession.builder() .appName(\"JsonSource\") .master(\"local\") .getOrCreate() //readJson1(spark) readJson2(spark) spark.close() &#125; def readJson1(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"json\").load(\"input/json.txt\") df.printSchema() &#125; &#125; sparkSession.read.json(path) 1234def readJson2(spark:SparkSession): Unit =&#123; val df = spark.read.json(\"input/json.txt\") df.printSchema()&#125; 输出 1234root |-- age: long (nullable &#x3D; true) |-- name: string (nullable &#x3D; true) |-- sex: string (nullable &#x3D; true) 部分字段 select 123456def readJson3(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"json\").load(\"input/json.txt\") df.printSchema() //选择部分列 df.select(\"name\",\"sex\").show(false)&#125; 输出 12345678+--------+------+|name |sex |+--------+------+|zhangsan|male ||lisi |female||wangwu |male ||zhaoliu |male |+--------+------+ 部分字段Select-DSL 1234567def readJson4(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"json\").load(\"input/json.txt\") //需要添加隐式转换才能使用DSL语法 import spark.implicits._ //选择部分列 df.select($\"name\",$\"sex\").show(false)&#125; 过滤-filter 123456789101112 def readJsonFilter(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"json\").load(\"input/json.txt\") import spark.implicits._ //选择部分列 val dataFrame = df.select($\"name\", $\"sex\") //1.字符串 dataFrame.filter(\"sex='female'\").show(false) //2.df方式 dataFrame.filter(df(\"sex\")===\"female\").show() //3.DSL dataFrame.filter('sex===\"female\").show()&#125; 输出 123456+----+------+|name|sex |+----+------+|lisi|female|+----+------+... 复杂对象 准备数据 1&#123;\"id\":0,\"name\":\"admin\",\"users\":[&#123;\"id\":2,\"name\":\"guest\"&#125;,&#123;\"id\":3,\"name\":\"root\"&#125;]&#125; 程序 1234567def readJsonComplex(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"json\").load(\"input/complexJson.txt\") df.printSchema() import spark.implicits._ //选择部分列 df.select($\"name\", $\"users.id\",$\"users.name\".as(\"users.name\")).show()&#125; 输出 12345678910111213root |-- id: long (nullable &#x3D; true) |-- name: string (nullable &#x3D; true) |-- users: array (nullable &#x3D; true) | |-- element: struct (containsNull &#x3D; true) | | |-- id: long (nullable &#x3D; true) | | |-- name: string (nullable &#x3D; true) +-----+------+-------------+| name| id| users.name|+-----+------+-------------+|admin|[2, 3]|[guest, root]|+-----+------+-------------+ 输出 df.write.format(“json”).mode(SaveMode.Overwrite).save(path) df.write.mode(SaveMode.Overwrite).json(path) 1234567def writeJson(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"json\").load(\"input/json.txt\") import spark.implicits._ //选择部分列 val dataFrame = df.select($\"name\", $\"sex\") dataFrame.write.format(\"json\").mode(SaveMode.Overwrite).save(\"output/write.json\")&#125; 发现这个写出去和hdfs操作一样,output/write.json只是一个目录,真正的数据是partxxx Text文本文件 读取 read.format(“text”).load(path) read.text(path) 准备数据 12313429100031 22552 8 2013-03-11 08:55:19.151754088 571 571 282 57113429100082 22540 8 2013-03-11 08:58:20.152622488 571 571 270 57113429100082 22691 8 2013-03-11 08:56:37.149593624 571 571 103 571 read.format(“text”).load(path) 程序 12345def readText(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"text\").load(\"input/text.txt\") df.show() df.printSchema()&#125; 输出 12345678910+--------------------+| value|+--------------------+|13429100031 22552...||13429100082 22540...||13429100082 22691...|+--------------------+root |-- value: string (nullable &#x3D; true) map 12345678def map(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"text\").load(\"input/text.txt\") import spark.implicits._ df.map(row=&gt;&#123; val strings = row.getString(0).split(\"\\t\") (strings(0),strings(1)) &#125;).show()&#125; output 1234567+-----------+-----+| _1| _2|+-----------+-----+|13429100031|22552||13429100082|22540||13429100082|22691|+-----------+-----+ rdd 1234567def rdd(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"text\").load(\"input/text.txt\") df.rdd.map(row=&gt;&#123; val strings = row.getString(0).split(\"\\t\") (strings(0),strings(1)) &#125;).foreach(println)&#125; output 123(13429100031,22552)(13429100082,22540)(13429100082,22691) textFile textFile直接返回一个DataSet而不是一个DataFrame 12345678def textFile(spark: SparkSession): Unit = &#123; val ds = spark.read.textFile(\"input/text.txt\") import spark.implicits._ ds.map(row =&gt; &#123; val strings = row.split(\"\\t\") (strings(0), strings(1)) &#125;).show()&#125; output 1234567+-----------+-----+| _1| _2|+-----------+-----+|13429100031|22552||13429100082|22540||13429100082|22691|+-----------+-----+ 输出 write.text(path) write.format(“text”).save(path) 123456789def writeTextFile(spark: SparkSession): Unit = &#123; val ds = spark.read.textFile(\"input/text.txt\") import spark.implicits._ val value = ds.map(row =&gt; &#123; val strings = row.split(\"\\t\") (strings(0), strings(1)) &#125;) value.write.mode(SaveMode.Overwrite).text(\"output/text\")&#125; 此时会报异常 1org.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.; 多列数据想存储为text格式的时候,需要合并成一列 123456789def writeTextFile(spark: SparkSession): Unit = &#123; val df = spark.read.text(\"input/text.txt\") import spark.implicits._ val value = df.map(row =&gt; &#123; val strings = row.getString(0).split(\"\\t\") (strings(0), strings(1)) &#125;).map(x =&gt; x._1 +\",\"+ x._2) value.write.mode(SaveMode.Overwrite).text(\"output/text\")&#125; output 12313429100031,2255213429100082,2254013429100082,22691 compress 要压缩保存,可以使用option指定compression 123456789def writeCompressionTextFile(spark: SparkSession): Unit = &#123; val df = spark.read.text(\"input/text.txt\") import spark.implicits._ val value = df.map(row =&gt; &#123; val strings = row.getString(0).split(\"\\t\") (strings(0), strings(1)) &#125;).map(x =&gt; x._1 +\",\"+ x._2) value.write.mode(SaveMode.Overwrite).option(\"compression\",\"bzip2\").text(\"output/text\")&#125; csv文件 读取 read.format(“csv”).load(path) read.csv(path) 准备数据 1234name|age|sexzhangsan|20|malelisi|15|femalewangwu|25|male read.format(“csv”).load(path) 1234567def readCsv(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"csv\") .option(\"header\",\"true\") .option(\"sep\", \"|\").load(\"input/csv.txt\") df.printSchema() df.show()&#125; output 123456789101112root |-- name: string (nullable &#x3D; true) |-- age: string (nullable &#x3D; true) |-- sex: string (nullable &#x3D; true) +--------+---+------+| name|age| sex|+--------+---+------+|zhangsan| 20| male|| lisi| 15|female|| wangwu| 25| male|+--------+---+------+ 可以发现,csv好像不进行类型推断了,那是因为这个option没开启 123456def readCsvInfer(spark:SparkSession): Unit =&#123; val df = spark.read.format(\"csv\") .option(\"header\",\"true\").option(\"inferSchema\",\"true\") .option(\"sep\", \"|\").load(\"input/csv.txt\") df.printSchema()&#125; output 1234root |-- name: string (nullable &#x3D; true) |-- age: integer (nullable &#x3D; true) |-- sex: string (nullable &#x3D; true) Source的option 每个Source对应的option可以在 org.apache.spark.sql.execution包下找到相应的xxOption类 JDBC 读取 1234567891011def readMysql(spark: SparkSession): Unit =&#123; val df = spark.read.format(\"jdbc\") .option(\"url\", \"jdbc:mysql:///test?serverTimezone=Asia/Shanghai\") .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") .option(\"dbtable\", \"t_order\") .option(\"user\", \"lrj\") .option(\"password\", \"123456\") .load() df.printSchema() df.show()&#125; output 1234567891011121314root |-- id: integer (nullable &#x3D; true) |-- name: string (nullable &#x3D; true) |-- number: integer (nullable &#x3D; true) |-- product_id: integer (nullable &#x3D; true) |-- total: double (nullable &#x3D; true) +---+-----+------+----------+------+| id| name|number|product_id| total|+---+-----+------+----------+------+| 1|订单1| 2| 1| 355.0|| 2|订单1| 2| 2| 45.0|| 3|订单2| 5| 1|1000.0|+---+-----+------+----------+------+ 输出 123456789101112131415def writeMysql(spark: SparkSession): Unit =&#123; val df = spark.read.format(\"jdbc\") .option(\"url\", \"jdbc:mysql:///test?serverTimezone=Asia/Shanghai\") .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") .option(\"dbtable\", \"t_order\") .option(\"user\", \"lrj\") .option(\"password\", \"123456\") .load() import spark.implicits._ df.filter('total&lt;500) .write.option(\"url\", \"jdbc:mysql:///test?serverTimezone=Asia/Shanghai\") .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") .option(\"dbtable\", \"t_order_filter\") .option(\"user\", \"lrj\") .option(\"password\", \"123456\").mode(SaveMode.Overwrite).format(\"jdbc\").save()&#125; 这个还是挺骚的,不需要提前创建表 SaveMode write的SaveMode需要注意,如果不指定,当输出路径存在时会报错","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"https://lurongjiang.github.io/tags/SparkSQL/"}]},{"title":"SparkSQL基础-Spark SQL","slug":"Spark-SparkSQL-1","date":"2019-04-20T23:52:31.000Z","updated":"2019-04-27T12:25:41.000Z","comments":true,"path":"2019/04/21/Spark-SparkSQL-1/","link":"","permalink":"https://lurongjiang.github.io/2019/04/21/Spark-SparkSQL-1/","excerpt":"SparkCore-Spark SQL的基本使用,HelloSparkSQL","text":"SparkSQL-1 为什么要使用SQL 传统SQL手段,对于RDBMS在数据量大之后,很难满足需求,或者性能很低,需要将原来的SQL方式转为大数据方向,也就是云化.但是最好能保留原有的SQL方式,不会影响原来的业务. 常见的SQL On Hadoop框架 Hive Hive的出现已经很久了,技术成熟,主要处理离线业务,时延比较高 Impala Cloudera公司基于Parquet实现 Presto Shark 基于Hive,将SQL运行在Spark之上,但是对执行计划优化比较弱,过于依赖Hive,维护升级难 Hive On Spark Hive默认使用MR作业方式运行,可以通过设置set =spark转换Hive为Spark作业,语法支持丰富 可以通过set hive.execution.engine=spark Spark SQL语法解析,执行计划优化全都由Spark来实现,逐步支持各种SQL Drill Phoenix HBase SQL化 SparkSQL Spark处理结构化数据的一个模块 SparkSQL 特点 使用Spark编程无缝对接SQL查询 使用SQL或者DataFrameAPI查询处理结构化数据 统一数据访问 使用DataFrame和SQL可以访问多种数据源,包括Hive,Avro,Parquet,ORC,JSON,JDBC等 Hive集成 支持HiveQL语法,UDF等 标准连接 支持JDBC,ODBC连接 Spark SQL强调的是结构化数据,而不是SQL Spark SQL支持SQL,DataFrame,DataSet 总的来说: Spark SQL is not about SQL Spark SQL is about more than SQL DataSet &amp; DataFrame DataSet 特殊的RDD,也是分布式数据集 spark1.6引入的 强类型,可以使用lamda函数 可以从JVM对象中使用转换函数转换而来 DataFrame DataFrame是一个特殊的DataSet 等同于关系型数据库的一张表 DataFrame可以结构化数据文件,Hive,外部数据源或者已存在的RDD中构造出来 DF=DataSet[Row] Hello World 1234567891011121314151617181920object HelloWorldApp &#123; def main(args: Array[String]): Unit = &#123; //创建SparkSession对象,这个是SparkSQL的入口 val spark = SparkSession.builder() .appName(\"helloWorldApp\") .master(\"local[2]\").getOrCreate() //读取json,传入json文件路径 val df = spark.read.json(\"input/access.json\") df.printSchema() //将读到的json创建一个临时表,这样后面就可以使用sql了,传入表名 df.createOrReplaceTempView(\"json\") spark.sql(\"select ip,ispname,cityname from json\").show(false) spark.close() &#125;&#125; 注意 Spark RDD中的cache是lazy的,Spark1.6开始,Spark SQL中的cache/uncache是eager,默认存储级别内存无序列化","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"https://lurongjiang.github.io/tags/SparkSQL/"}]},{"title":"SparkCore基础-Spark监控","slug":"Spark-Spark监控","date":"2019-04-18T07:14:54.000Z","updated":"2019-04-19T16:10:06.000Z","comments":true,"path":"2019/04/18/Spark-Spark监控/","link":"","permalink":"https://lurongjiang.github.io/2019/04/18/Spark-Spark%E7%9B%91%E6%8E%A7/","excerpt":"SparkCore-Spark的监控,监控Spark程序的生命周期","text":"Spark监控 spark自带的Monitor 4040端口生命周期可见 生命周期结束要看,需要配置 spark-default.properties 123spark.master=localspark.eventLog.enable=truespark.eventLog.dir=xx spark-env.sh 1export SPARK_HISTORY_OPTS=\"-Dspark.history.fs.logDirectory=xx -Dspark.history.ui.port=xx\" 自定义实现 SparkLister","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkCore","slug":"SparkCore","permalink":"https://lurongjiang.github.io/tags/SparkCore/"}]},{"title":"SparkCore基础-Spark作业提交","slug":"Spark-SparkCore-SumbmitApplication","date":"2019-04-17T01:32:22.000Z","updated":"2019-04-19T03:58:52.000Z","comments":true,"path":"2019/04/17/Spark-SparkCore-SumbmitApplication/","link":"","permalink":"https://lurongjiang.github.io/2019/04/17/Spark-SparkCore-SumbmitApplication/","excerpt":"SparkCore-Spark作业提交","text":"SparkCore-SumbmitApplication hdfs:HADOOP_CONF_DIR 1234567spark-submit \\--class com.lrj.spark.core.offline.WordCountApp \\--name WCApp \\--master local[2] \\--jars /usr/software/spark/lib/spark-utils-1.0-SNAPSHOT.jar \\/usr/software/spark/lib/spark-demo-1.0-SNAPSHOT.jar \\input/wc.txt output/wc yarn:YARN_CONF_DIR 1234567spark-submit \\--class com.lrj.spark.core.offline.WordCountApp \\--name WCApp \\--master yarn \\--jars /usr/software/spark/lib/spark-utils-1.0-SNAPSHOT.jar \\/usr/software/spark/lib/spark-demo-1.0-SNAPSHOT.jar \\input/wc.txt output/wc 作业提交之后,先Accept,等到得到资源之后才会开始运行,如果资源不够可能回导致等待时间很长 不同模式的Spark任务 client模式 本地启动Driver (new SparkContext(xx).setMaster(xx) ) Driver和RM通信,请求启动AM RM找一台NM启动AM AM向RM申请资源 RM分配一批资源给AM,AM根据资源分布与NM通信,启动Executor容器 Executor反向注册到Driver Executor和Driver必须时不时的通信,可能会出现网络激增 关于spark-submit的说明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]Usage: spark-submit --kill [submission ID] --master [spark://...]Usage: spark-submit --status [submission ID] --master [spark://...]Usage: spark-submit run-example [options] example-class [example args]Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, k8s://https://host:port, or local (Default: local[*]). --deploy-mode DEPLOY_MODE Whether to launch the driver program locally (\"client\") or on one of the worker machines inside the cluster (\"cluster\") (Default: client). --class CLASS_NAME Your application's main class (for Java / Scala apps). --name NAME A name of your application. --jars JARS Comma-separated list of jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. --files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName). --conf PROP=VALUE Arbitrary Spark configuration property. --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). --driver-java-options Extra Java options to pass to the driver. --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h Show this help message and exit. --verbose, -v Print additional debug output. --version, Print the version of current Spark. Cluster deploy mode only: --driver-cores NUM Number of cores used by the driver, only in cluster mode (Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise If given, restarts the driver on failure. --kill SUBMISSION_ID If given, kills the driver specified. --status SUBMISSION_ID If given, requests the status of the driver specified. Spark standalone and Mesos only: --total-executor-cores NUM Total cores for all executors. Spark standalone and YARN only: --executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode, or all available cores on the worker in standalone mode) YARN-only: --queue QUEUE_NAME The YARN queue to submit to (Default: \"default\"). --num-executors NUM Number of executors to launch (Default: 2). If dynamic allocation is enabled, the initial number of executors will be at least NUM. --archives ARCHIVES Comma separated list of archives to be extracted into the working directory of each executor. --principal PRINCIPAL Principal to be used to login to KDC, while running on secure HDFS. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically.","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkCore","slug":"SparkCore","permalink":"https://lurongjiang.github.io/tags/SparkCore/"}]},{"title":"SparkCore基础-Spark On Yarn","slug":"Spark-Spark On Yarn","date":"2019-04-14T17:29:49.000Z","updated":"2019-04-17T09:38:42.000Z","comments":true,"path":"2019/04/15/Spark-Spark On Yarn/","link":"","permalink":"https://lurongjiang.github.io/2019/04/15/Spark-Spark%20On%20Yarn/","excerpt":"SparkCore-Spark On Yarn的两种模式","text":"Spark On Yarn Launching Spark on YARN Spark On Yarn务必配置HADOOP_CONF_DIR或者YARN_CONF_DIR指向Hadoop的配置目录($HADOOP_HOME/etc/hadoop),否则报错 Spark On Yarn的两种模式 client client模式中,Driver进程运行再客户端的进程中,所以客户端提交任务后必须等待任务执行完成,适合交互性任务 cluster cluster模式中,Driver是运行在Yarn管理应用程序主进程中,客户端提交完任务后就可以退出了,不适合交互性任务 Yarn模式和其他cluster模式不一样 其他模式的–master参数必须指定到master的地址 yarn模式–master只需要写一个yarn就行,yarn会从配置文件中获取到ResourceManager的地 启动spark集群命令 1spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] &lt;app jar&gt; [app options] 例如 123456789spark-submit --class org.apache.spark.examples.SparkPi \\ --master yarn \\ --deploy-mode cluster \\ --driver-memory 4g \\ --executor-memory 2g \\ --executor-cores 1 \\ --queue thequeue \\ examples/jars/spark-examples*.jar \\ 10 如果需要使用yarn client模式,只需把–deploy-mode设置为client 123456789spark-submit --class org.apache.spark.examples.SparkPi \\ --master yarn \\ --deploy-mode cluster \\ --driver-memory 4g \\ --executor-memory 2g \\ --executor-cores 1 \\ --queue thequeue \\ examples/jars/spark-examples*.jar \\ 10 Adding Other JARs 集群模式中,Driver不是运行在client端,所以使用SparkContext.addJar并没有用,如果需要添加额外的依赖包,需要使用–jars传入,多个jar包使用逗号分隔 Preparations 为了让Spark能在Yarn上运行,需要指定spark.yarn.archive或者spark.yarn.jars属性,如果两个都没指定,Spark每次任务运行的时候就把$SPARK_HOME/jars下的所有jar包上传到分布式缓存中 没设置时 设置之后 spark-default.conf 1spark.yarn.jars hdfs:&#x2F;&#x2F;&#x2F;spark&#x2F;*.jar 需要把jar包上传到/spark目录,这样就不会每次都上传jar包到hdfs了 Debugging your Application 如果开启了yarn日志收集功能(配置了yarn.log-aggregation-enable=true),日志会拷贝到yarn上,然后删除本地日志,如果要看日志,需要使用命令 1yarn logs -applicationId &lt;app ID&gt; 这个命令会打印出指定appId的所有日志信息 你也可以通过hdfs shell和相关api获取 你可以通过yarn.nodemanager.remote-app-log-dir and yarn.nodemanager.remote-app-log-dir-suffix来定位你的日志到底是哪一个 这些日在在SparkUI中也可以查看到,在Executor页签中 你需要保证Spark日志服务和Yarn的日志服务都是启动的,并且 yarn-site.xml中的yarn.log.server.url属性配置正确 如果没有开启日志收集功能,则日志会存在本地,你可以在 $HADOOP_HOME/logs/user中查看到 为了检查看每个Container的环境,你需要配置yarn.nodemanager.delete.debug-delay-sec多久删除本地的日志,这样你就可以进入事先配置的yarn.nodemanager.local-dirs目录下查看每个容器的环境情况,下面包含了任务执行的script, JARs和所有的环境变量 为了使用自己的log4j来打印日志,可以在spark-submit脚本中,把自己的log4j.properties通过–files传入 或者在spark.driver.extraJavaOptions中设置’-Dlog4j.configuration=&lt; location of configuration file&gt;’ 当然你可以直接修改$SPARK_HOME/conf/log4j.properties Important notes Yarn配置什么调度器就使用什么方式调度 在cluster模式中,executor和driver使用的本地目录可以通过yarn.nodemanager.local-dirs来指定 如果p配置了spark.local.dir,则会忽略掉yarn的配置 client模式可以指定spark.local.dir,因为Driver并不是运行在yarn集群中的 --files 和--archives选项都可以使用类似Hadoop中#的用法(和别名类似,不需要每次写全路径) cluster模式中使用--jars来指定本地的jars包路径,但是如果使用的是HDFS, HTTP, HTTPS, or FTP的文件则不需要","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkCore","slug":"SparkCore","permalink":"https://lurongjiang.github.io/tags/SparkCore/"}]},{"title":"SparkCore基础-Action-2","slug":"Spark-SparkCore-2","date":"2019-04-08T09:46:09.000Z","updated":"2019-04-11T08:12:26.000Z","comments":true,"path":"2019/04/08/Spark-SparkCore-2/","link":"","permalink":"https://lurongjiang.github.io/2019/04/08/Spark-SparkCore-2/","excerpt":"SparkCore的常用Action","text":"SparkCore-2 新建工程 新建maven父工程 新建Module&gt;maven core streaming ml hbase SparkCore Transformation map 迭代每一个元素 mapPartition 迭代每个分区 mapPartitionWithIndex grom filter sample 取样 zip 拉链 注意点 spark和scala不同,spark中必须保证: 元素数目相同 分区数相同 zipWithIndex union intersection 交集,去重 substract 差集,不去重 cartesian 卡笛尔集 distinct 去重 分区数不变 sortBy sorkByKey KV类型 groupBy 这个出来的是(key,Interator),需要对iteration的内容进行进一步的处理 groupByKey 出来(k,v) mapValues reduceByKey 先本地聚合,再shuffle join inner join leftOuterJoion rightOuterJoin fullOutJoin coalesce 由多变少 指定第二个参数true之后,可以实现少变多,重新分区1 repartition 遇到shuffle就切分statge","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkCore","slug":"SparkCore","permalink":"https://lurongjiang.github.io/tags/SparkCore/"}]},{"title":"SparkCore基础-Action","slug":"Spark-SparkCore-action","date":"2019-04-07T05:18:58.000Z","updated":"2019-04-09T14:56:45.000Z","comments":true,"path":"2019/04/07/Spark-SparkCore-action/","link":"","permalink":"https://lurongjiang.github.io/2019/04/07/Spark-SparkCore-action/","excerpt":"SparkCore的常用Action","text":"Spark Core-Action Action take take使用需要慎重,避免冲爆Driver端内存 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * 先扫描一个分区,再估算还需要扫描多少个其他分区才能满足 * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`. */def take(num: Int): Array[T] = withScope &#123; val scaleUpFactor = Math.max(conf.getInt(\"spark.rdd.limit.scaleUpFactor\", 4), 2) if (num == 0) &#123; new Array[T](0) &#125; else &#123; val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size &lt; num &amp;&amp; partsScanned &lt; totalParts) &#123; // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L //还剩多少没取 val left = num - buf.size //如果不是第一次扫描 if (partsScanned &gt; 0) &#123; // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) &#123; //之前扫描的结果集是空的,说明之前都没扫到,扫描范围扩充为原来的N倍 numPartsToTry = partsScanned * scaleUpFactor &#125; else &#123; // As left &gt; 0, numPartsToTry is always &gt;= 1 //如果之前扫描到了,但数量还不够,采用插值法,增加50%的扫描范围 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) &#125; &#125; //计算要扫描第几个分区 val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) =&gt; it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size &#125; buf.toArray &#125;&#125; count 123456789101112//返回RDD中的元素//Utils.getIteratorSize就是把集合的迭代器数一遍def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sumdef getIteratorSize(iterator: Iterator[_]): Long = &#123; var count = 0L while (iterator.hasNext) &#123; count += 1L iterator.next() &#125; count&#125; top和takeOrdered 取topN个元素,这个是全局的,所以注意只能在数据量小的时候使用,否则可能会冲爆Driver端内存 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/**This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver's memory.sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1)// returns Array(12)sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2)// returns Array(6, 5)*/def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope &#123; //使用柯里化的方式,第一个是要取多少个,第二按降序排列 takeOrdered(num)(ord.reverse)&#125;/*** sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1)* // returns Array(2)** sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2)* // returns Array(2, 3)* &#125;&#125;&#125;** @note This method should only be used if the resulting array is expected to be small* as all the data is loaded into the driver's memory.*/def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope &#123; //取0个,直接返回空 if (num == 0) &#123; Array.empty &#125; else &#123; //每个partition取topN val mapRDDs = mapPartitions &#123; items =&gt; // Priority keeps the largest elements, so let's reverse the ordering. //构建有界优先队列(N个长度) val queue = new BoundedPriorityQueue[T](num)(ord.reverse) //将数据加进去 queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) &#125; if (mapRDDs.partitions.length == 0) &#123; Array.empty &#125; else &#123; //归并一下,因为是有界优先队列,后面大的数据会替换掉小的 mapRDDs.reduce &#123; (queue1, queue2) =&gt; queue1 ++= queue2 queue1 &#125;.toArray.sorted(ord) &#125; &#125;&#125; 可以看出top方法的takeOrdered传入的是一个隐式转换:降序,所以如果需要升序,可以直接takeOrdered 1234567891011121314object ActionTes &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setMaster(\"local\") conf.setAppName(\"app\") val sc = new SparkContext(conf) val list = List(1, 2, 3) val value = sc.parallelize(list) //升序 value.takeOrdered(2) //降序 value.takeOrdered(2)(Ordering.by(-_)) &#125;&#125; reduce 两两合并 countByKey 针对PairRDDFunction 计算key出现的次数 使用场景:数据倾斜key检查 123def countByKey(): Map[K, Long] = self.withScope &#123; self.mapValues(_ =&gt; 1L).reduceByKey(_ + _).collect().toMap&#125; foreach 对集合中的每一个元素作用一个function foreachPartition 针对每个分区作用一个函数,也就是说一批一批的处理,一般效率会比foreach效率要高 值得注意的是,foreachPartition本身是没有返回值的,但是使用了sc.runJob,这个是有返回值的 如果分区的内容很大,可能会出现OOM 12345678910111213141516171819/** * Applies a function f to each partition of this RDD. */def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withScope &#123; val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =&gt; cleanF(iter))&#125;/*** Run a job on all partitions in an RDD and return the results in an array.** @param rdd target RDD to run tasks on* @param func a function to run on each partition of the RDD* @return in-memory collection with a result of the job * (each collection element will contain a result from one partition)* 返回一个任务结果的集合到内存中*/def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] =&gt; U): Array[U] = &#123;runJob(rdd, func, 0 until rdd.partitions.length)&#125; DataSource saveTextFile 将rdd作为文本按分区写出去 先检查输出路径,存在报错 使用hadoop的output检查 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204def saveAsTextFile(path: String): Unit = withScope &#123; // https://issues.apache.org/jira/browse/SPARK-2075 // // NullWritable is a `Comparable` in Hadoop 1.+, so the compiler cannot find an implicit // Ordering for it and will use the default `null`. However, it's a `Comparable[NullWritable]` // in Hadoop 2.+, so the compiler will call the implicit `Ordering.ordered` method to create an // Ordering for `NullWritable`. That's why the compiler will generate different anonymous // classes for `saveAsTextFile` in Hadoop 1.+ and Hadoop 2.+. // // Therefore, here we provide an explicit Ordering `null` to make sure the compiler generate // same bytecodes for `saveAsTextFile`. // 将输出以&lt;NullWritable,Text&gt;的形式写出去 val nullWritableClassTag = implicitly[ClassTag[NullWritable]] val textClassTag = implicitly[ClassTag[Text]] val r = this.mapPartitions &#123; iter =&gt; val text = new Text() iter.map &#123; x =&gt; text.set(x.toString) (NullWritable.get(), text) &#125; &#125; //RDD.rddToPairRDDFunctions生成PairRDDFunctions, //采用hdfs默认的FileOutputFormat实现TextOutputFormat RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null).saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)&#125;//底层抵用hadoop的读写进行保存文件/*** Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class* supporting the key and value types K and V in this RDD.*/def saveAsHadoopFile[F &lt;: OutputFormat[K, V]]( path: String)(implicit fm: ClassTag[F]): Unit = self.withScope &#123; saveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[Class[F]])&#125;def saveAsHadoopFile( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit = self.withScope &#123; // Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038). val hadoopConf = conf //设置hdfs的输出K,V类型 hadoopConf.setOutputKeyClass(keyClass) hadoopConf.setOutputValueClass(valueClass) //其实就是TextOutputFormat conf.setOutputFormat(outputFormatClass) for (c &lt;- codec) &#123; //如果设置了压缩 hadoopConf.setCompressMapOutput(true) hadoopConf.set(\"mapreduce.output.fileoutputformat.compress\", \"true\") hadoopConf.setMapOutputCompressorClass(c) hadoopConf.set(\"mapreduce.output.fileoutputformat.compress.codec\", c.getCanonicalName) hadoopConf.set(\"mapreduce.output.fileoutputformat.compress.type\", CompressionType.BLOCK.toString) &#125; // Use configured output committer if already set if (conf.getOutputCommitter == null) &#123; hadoopConf.setOutputCommitter(classOf[FileOutputCommitter]) &#125; // When speculation is on and output committer class name contains \"Direct\", we should warn // users that they may loss data if they are using a direct output committer. val speculationEnabled = self.conf.getBoolean(\"spark.speculation\", false) val outputCommitterClass = hadoopConf.get(\"mapred.output.committer.class\", \"\") if (speculationEnabled &amp;&amp; outputCommitterClass.contains(\"Direct\")) &#123; val warningMessage = s\"$outputCommitterClass may be an output committer that writes data directly to \" + \"the final location. Because speculation is enabled, this output committer may \" + \"cause data loss (see the case in SPARK-10063). If possible, please use an output \" + \"committer that does not have this behavior (e.g. FileOutputCommitter).\" logWarning(warningMessage) &#125; //设置输出目录 FileOutputFormat.setOutputPath(hadoopConf, SparkHadoopWriterUtils.createPathFromString(path, hadoopConf)) saveAsHadoopDataset(hadoopConf)&#125;/*** Output the RDD to any Hadoop-supported storage system, using a Hadoop JobConf object for* that storage system. The JobConf should set an OutputFormat and any output paths required* (e.g. a table name to write to) in the same way as it would be configured for a Hadoop* MapReduce job.*/def saveAsHadoopDataset(conf: JobConf): Unit = self.withScope &#123;val config = new HadoopMapRedWriteConfigUtil[K, V](new SerializableJobConf(conf)) SparkHadoopWriter.write( rdd = self, config = config)&#125;/** Basic work flow of this command is:* 1. Driver side setup, prepare the data source and hadoop configuration for the write job to* be issued.* 2. Issues a write job consists of one or more executor side tasks, each of which writes all* rows within an RDD partition.* 3. If no exception is thrown in a task, commits that task, otherwise aborts that task; If any* exception is thrown during task commitment, also aborts that task.* 4. If all tasks are committed, commit the job, otherwise aborts the job; If any exception is* thrown during job commitment, also aborts the job.*/def write[K, V: ClassTag]( rdd: RDD[(K, V)], config: HadoopWriteConfigUtil[K, V]): Unit = &#123; // Extract context and configuration from RDD. val sparkContext = rdd.context val commitJobId = rdd.id //获取jobId // Set up a job. val jobTrackerId = createJobTrackerID(new Date()) //创建上下文 val jobContext = config.createJobContext(jobTrackerId, commitJobId) //设置Output Class config.initOutputFormat(jobContext) //判断K,V类型,输出目录是否存在 // Assert the output format/key/value class is set in JobConf. config.assertConf(jobContext, rdd.conf) val committer = config.createCommitter(commitJobId) committer.setupJob(jobContext) // Try to write all RDD partitions as a Hadoop OutputFormat. try &#123; val ret = sparkContext.runJob(rdd, (context: TaskContext, iter: Iterator[(K, V)]) =&gt; &#123; // SPARK-24552: Generate a unique \"attempt ID\" based on the stage and task attempt numbers. // Assumes that there won't be more than Short.MaxValue attempts, at least not concurrently. val attemptId = (context.stageAttemptNumber &lt;&lt; 16) | context.attemptNumber executeTask( context = context, config = config, jobTrackerId = jobTrackerId, commitJobId = commitJobId, sparkPartitionId = context.partitionId, sparkAttemptNumber = attemptId, committer = committer, iterator = iter) &#125;) committer.commitJob(jobContext, ret) logInfo(s\"Job $&#123;jobContext.getJobID&#125; committed.\") &#125; catch &#123; case cause: Throwable =&gt; logError(s\"Aborting job $&#123;jobContext.getJobID&#125;.\", cause) committer.abortJob(jobContext) throw new SparkException(\"Job aborted.\", cause) &#125;&#125;//检查输出override def assertConf(jobContext: NewJobContext, conf: SparkConf): Unit = &#123; val outputFormatInstance = getOutputFormat() val keyClass = getConf.getOutputKeyClass val valueClass = getConf.getOutputValueClass //这个基本不可能,他自己带过来的TextOutputFormat if (outputFormatInstance == null) &#123; throw new SparkException(\"Output format class not set\") &#125; //这个就是NullWritable if (keyClass == null) &#123; throw new SparkException(\"Output key class not set\") &#125; //这个是Text if (valueClass == null) &#123; throw new SparkException(\"Output value class not set\") &#125; SparkHadoopUtil.get.addCredentials(getConf) logDebug(\"Saving as hadoop file of type (\" + keyClass.getSimpleName + \", \" + valueClass.getSimpleName + \")\") if (SparkHadoopWriterUtils.isOutputSpecValidationEnabled(conf)) &#123; // FileOutputFormat ignores the filesystem parameter val ignoredFs = FileSystem.get(getConf) //检查输出路径是否存在文件,存在就报错 getOutputFormat().checkOutputSpecs(ignoredFs, getConf) &#125;&#125;public void checkOutputSpecs(FileSystem ignored, JobConf job) throws FileAlreadyExistsException, InvalidJobConfException, IOException &#123; Path outDir = getOutputPath(job); if (outDir == null &amp;&amp; job.getNumReduceTasks() != 0) &#123; throw new InvalidJobConfException(\"Output directory not set in JobConf.\"); &#125; else &#123; if (outDir != null) &#123; FileSystem fs = outDir.getFileSystem(job); outDir = fs.makeQualified(outDir); setOutputPath(job, outDir); TokenCache.obtainTokensForNamenodes(job.getCredentials(), new Path[]&#123;outDir&#125;, job); if (fs.exists(outDir)) &#123; //如果目录存在就报错 throw new FileAlreadyExistsException(\"Output directory \" + outDir + \" already exists\"); &#125; &#125; &#125;&#125; saveAsTextFile 以压缩的形式写出去 saveAsObjectFile 保存对象,这个是需要序列化的,可以使用自带序列化的case class objectFile 读取对象文件,反序列化 Application=1Driver+n Executor Driver =&gt; main中创建sc Task=&gt;最小执行单位,map,filter,… WorkNode=&gt;NM Job=&gt;一个action就是一个job,action in stage Stage=&gt;遇到shuffle就是一个Stage","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkCore","slug":"SparkCore","permalink":"https://lurongjiang.github.io/tags/SparkCore/"}]},{"title":"Flume稍复杂的一些玩意","slug":"Flume-稍复杂的一些玩意","date":"2019-04-06T07:36:48.000Z","updated":"2019-04-06T02:36:45.000Z","comments":true,"path":"2019/04/06/Flume-稍复杂的一些玩意/","link":"","permalink":"https://lurongjiang.github.io/2019/04/06/Flume-%E7%A8%8D%E5%A4%8D%E6%9D%82%E7%9A%84%E4%B8%80%E4%BA%9B%E7%8E%A9%E6%84%8F/","excerpt":"Flume稍复杂的一些玩意,例如多个agent,sink,source,高可用,负载均衡等的基本配置和使用","text":"Flume稍复杂的一些玩意 Flume的稍复杂配置 两个agent的传递 agent A -&gt;发送数据到agent B a.conf 12345678910111213141516171819202122232425# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F ~/usr/software/test.txt# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44441# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 b.conf 12345678910111213141516171819202122# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44441# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动 注意启动顺序 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/b.conf 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/a.conf 两个source 采集多个渠道的日志 two_source.conf 123456789101112131415161718192021222324252627# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1 r2a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txta1.sources.r2.type = netcata1.sources.r2.bind = 0.0.0.0a1.sources.r2.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sources.r2.channels = c1a1.sinks.k1.channel = c1 启动 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/two_source.conf 之后可以telnet 44444端口和写文件进行测试 两个Sink 上游下来一份数据,写到两个位置 two_sink.conf 1234567891011121314151617181920212223242526# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txt# Describe the sinka1.sinks.k1.type = loggera1.sinks.k2.type = hdfsa1.sinks.k2.hdfs.path = hdfs://hadoop001:9000/flume/test# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1a1.sinks.k2.channel = c1 启动 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/two_sink.conf 但是启动之后会发现,有一些数据在hdfs上,有的在logger中,由此可见,flume默认同一份你数据只会发一次 两个channel和两个Sink 前面的两个Sink似乎并不是我们想要的结果,我们希望写到两个位置,上面虽然写道了两个位置,但是都不是完整的,所以还需要调整一下 c2s2.conf 12345678910111213141516171819202122232425# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txt# Describe the sinka1.sinks.k1.type = loggera1.sinks.k2.type = hdfsa1.sinks.k2.hdfs.path = hdfs://hadoop001:9000/flume/test# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c2.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2 启动 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/c2s2.conf 多个个Channel其实涉及到一个ChannelSelector的概念,但是ChannelSelector的策略默认是replicating复制策略,正是我们需要的,所以就不用指定了 a1.sources.r1.selector.type=…. a1.sources.r1.selector.optional=…. Multiplexing Channel Selector 有时候日志是从不同的地方传过来的,携带了不同的标识信息header,需要存放不同的地方或者归类 因为需要添加header,所以需要增加Interceptor sink1.conf 1234567891011121314151617181920212223242526# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44441# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44444# Use a channel which buffers events in memorya1.channels.c1.type = memory# 添加CN Headera1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = statica1.sources.r1.interceptors.i1.key = headera1.sources.r1.interceptors.i1.value = CN# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 sink2.conf 1234567891011121314151617181920212223242526# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txt# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44444# Use a channel which buffers events in memorya1.channels.c1.type = memory# 添加EN Headera1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = statica1.sources.r1.interceptors.i1.key = headera1.sources.r1.interceptors.i1.value = EN# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 sink3.conf 12345678910111213141516171819202122# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = TAILDIRa1.sources.r1.filegroups = f1a1.sources.r1.filegroups.f1 = /usr/software/test/.*.log# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44444# Use a channel which buffers events in memorya1.channels.c1.type = memory#不加Header# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 接收端的sink-all.conf 123456789101112131415161718192021222324252627282930313233343536373839# Name the components on this agenta1.sources = r1a1.sinks = k1 k2 k3a1.channels = c1 c2 c3# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://hadoop001:9000/flume/cna1.sinks.k1.hdfs.writeFormat = Texta1.sinks.k1.hdfs.fileType = DataStreama1.sinks.k2.type = hdfsa1.sinks.k2.hdfs.path = hdfs://hadoop001:9000/flume/ena1.sinks.k2.hdfs.writeFormat = Texta1.sinks.k2.hdfs.fileType = DataStreama1.sinks.k3.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c2.type = memorya1.channels.c3.type = memorya1.sources.r1.selector.type = multiplexinga1.sources.r1.selector.header = headera1.sources.r1.selector.mapping.CN = c1a1.sources.r1.selector.mapping.EN = c2a1.sources.r1.selector.default = c3# Bind the source and sink to the channela1.sources.r1.channels = c1 c2 c3a1.sinks.k1.channel = c1a1.sinks.k1.channel = c2a1.sinks.k1.channel = c3 启动 注意启动顺序 先启动sink-all 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/sink-all.conf 启动sink 1,2,3 1234567891011121314151617# 使用telnet 44441端口测试flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/sink1.conf# 监听的是 /usr/software/test.txtflume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/sink2.conf# 监听的是/usr/software/test/.*.logflume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/sink-3.conf 使用对应的方式进行测试,即可 Sink Processor FailOver配置 容错配置和两个Sink类似,但是正常情况下,只会用优先级高的Sink,当另一个Sink不可用的时候才会使用备用的Sink first.conf 123456789101112131415161718192021# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44441# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 second.conf 123456789101112131415161718192021# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44442# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 failover.conf 1234567891011121314151617181920212223242526272829303132333435# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txt# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44441a1.sinks.k2.type = avroa1.sinks.k2.hostname = 0.0.0.0a1.sinks.k2.port = 44442# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1a1.sinks.k2.channel = c1# 设置sink的优先级,sink process为容错机制a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.g1.processor.type = failovera1.sinkgroups.g1.processor.priority.k1 = 5a1.sinkgroups.g1.processor.priority.k2 = 10a1.sinkgroups.g1.processor.maxpenalty = 10000 启动 启动k1,k2 12345678910flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/first.confflume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/second.conf 启动failover.conf 1234flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/failover.conf 然后追加内容到/usr/software/test.txt就可以看到变化了,当两个都可用的时候,44442端口优先使用,当关闭44442对应的agent,则会自动切换到44441的agent 可以发现,优先的值越高,优先级越高 LoadBalance配置 load1.conf 123456789101112131415161718192021# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44441# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 load2.conf 123456789101112131415161718192021# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44442# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 load.conf 12345678910111213141516171819202122232425262728293031323334# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /usr/software/test.txt# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = 0.0.0.0a1.sinks.k1.port = 44441a1.sinks.k2.type = avroa1.sinks.k2.hostname = 0.0.0.0a1.sinks.k2.port = 44442# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1a1.sinks.k2.channel = c1# 配置负载均衡为随机数机制a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.g1.processor.type = load_balancea1.sinkgroups.g1.processor.backoff = truea1.sinkgroups.g1.processor.selector = random 启动 启动load1,load2 123456789flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/load1.confflume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/load2.conf 启动load.conf 12345flume-ng agent \\--name 'a1' \\--conf $FLUEM_HOME/conf \\--conf-file $FLUME_HOME/myconf/load.conf 之后往/usr/software/test.txt添加内容可以看到两个都可以看到内容的变化,实现负载均衡. 负载均衡还有一个机制:轮询","categories":[{"name":"Flume","slug":"Flume","permalink":"https://lurongjiang.github.io/categories/Flume/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://lurongjiang.github.io/tags/Flume/"}]},{"title":"Azkaban配置代理用户执行任务","slug":"Azkaban-配置代理用户执行任务","date":"2019-04-06T05:15:54.000Z","updated":"2019-04-06T08:45:23.000Z","comments":true,"path":"2019/04/06/Azkaban-配置代理用户执行任务/","link":"","permalink":"https://lurongjiang.github.io/2019/04/06/Azkaban-%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86%E7%94%A8%E6%88%B7%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1/","excerpt":"简单的配置,实现登录的Azkaban用户和执行任务用户不同的需求","text":"Azkaban配置代理用户执行任务 有时候我们希望启动azkaban的用户和启动任务脚本使用不同的用户 这时候我们可以使用azkaban的execute-as-user插件完成,参考文档Plugin Configurations 1234567891011# 这里修正一下,execute-as-user.c的位置官网写成azkaban-common的目录# 但是3.81.0我在github在azkaban-util下面找到的scp ./az-exec-util/src/main/c/execute-as-user.c# 编译C文件,需要提前安装gccgcc execute-as-user.c -o execute-as-user# 设置权限为rootchown root execute-as-user# 这个权限比较有意思,---Sr-s---,不懂意思,很特殊的权限chmod 6050 execute-as-user 配置一下plugin $AZKABAN_HOME/plugins/jobtypes/commonprivate.properties 1234execute.as.user=trueazkaban.native.lib=/home/lurongjiang/azkaban-3.81.0proxy.user=azkaban.should.proxy=true 这样在project的.flow中就可以设置user.to.proxy参数来实现用户代理执行了 注意 要代理的用户记得要在linux中存在 默认使用的是azkaban用户组,所以需要groupadd,或者参考文档中的另外一个参数 记得赋予proxy用户执行脚本的权限,否则百搭 test.flow 1234567891011121314151617config: time: \"\" user.to.proxy: lurongjiangnodes: - name: jobA type: command config: command: /home/lurongjiang/test/a.sh $&#123;time&#125; - name: jobB type: command config: command: /home/lurongjiang/test/b.sh $&#123;time&#125; - name: jobC type: command config: command: /home/lurongjiang/test/c.sh $&#123;time&#125;","categories":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://lurongjiang.github.io/categories/Azkaban/"}],"tags":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://lurongjiang.github.io/tags/Azkaban/"}]},{"title":"SparkCore基础-1","slug":"Spark-SparkCore-RDD","date":"2019-04-06T03:20:03.000Z","updated":"2019-04-08T09:46:09.000Z","comments":true,"path":"2019/04/06/Spark-SparkCore-RDD/","link":"","permalink":"https://lurongjiang.github.io/2019/04/06/Spark-SparkCore-RDD/","excerpt":"SparkCore的基础,RDD的五大特点是什么......","text":"SparkCore-RDD 弹性分布式数据集 不可变的 这个集合可分区(拆开的) 数据集的元素可以并行操作 spark中,RDD是一个抽象类 RDD实现了序列化接口和Logging RDD五大特点 有多个分区 对RDD的计算操作就是对每一个分区的计算操作 RDD之间有依赖关系 KV类型的RDD有Partitioner分区器的概念 数据位置有优先 SparkContext Spark上下文对象 一个JVM只能有一个上下文对象.启动一个,必须关掉另一个 SparkConf 传递自定义参数,必须是spark.开头 SparkContext必须设置master和appName,也就是conf必须包含(spark.master和spark.app.name) 对于master和appName不要硬编码,而是通过脚本传进去 RDD 1. RDD的创建 parallelize 将集合转成RDD对象 parallelize第二个参数就是split数 parallelize默认使用core数作为第二个默认参数,例如local[2],每次collect就是2 2. 外部数据源 textFile 读取文件作为RDD对象(MapPartitionRDD) 跨节点(如standalone),这种方式必须保证所有节点的相同路径下都要有这个文件 支持目录和通配符路径 默认块大小128M wholeTextFiles 读取整个文件,返回一个(Int,String)的元组RDD 3. 通过RDD转换获取RDD 这个就是RDD五大特点的特点3 RDD操作 RDD的操作包括两种: transformation: 从RDD转换成另一个RDD action: 把结果返回到Driver端 transformation 如map,filter 所有的transformation都是lazy模式的,不会立即执行的 遇到action才会执行 操作 map 作用在集合的每一个元素 filter mapPartition 作用在集合的每一个partition mapPartitionWithIndex 带分区id对每一个分区进行操作 mapValues 对K,V类型的集合中的每一个V进行操作 flatMap 对两层的结构压平成一层 action 如reduce persist/cache 持久化或者缓存RDD到磁盘","categories":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://lurongjiang.github.io/tags/Spark/"},{"name":"SparkCore","slug":"SparkCore","permalink":"https://lurongjiang.github.io/tags/SparkCore/"}]},{"title":"Flume使用教程","slug":"Flume-基本使用","date":"2019-04-04T07:14:54.000Z","updated":"2019-04-04T10:45:45.000Z","comments":true,"path":"2019/04/04/Flume-基本使用/","link":"","permalink":"https://lurongjiang.github.io/2019/04/04/Flume-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"Flume的简单使用教程,分别列了几个常见的Flume配置","text":"Flume Flume是高可靠,高可用的,分布式的海量日志收集,聚合和传输的系统, 数据流模型 A Flume event is defined as a unit of data flow having a byte payload and an optional set of string attributes. A Flume agent is a (JVM) process that hosts the components through which events flow from an external source to the next destination (hop). A Flume source consumes events delivered to it by an external source like a web server. The external source sends events to Flume in a format that is recognized by the target Flume source. For example, an Avro Flume source can be used to receive Avro events from Avro clients or other Flume agents in the flow that send events from an Avro sink. A similar flow can be defined using a Thrift Flume Source to receive events from a Thrift Sink or a Flume Thrift Rpc Client or Thrift clients written in any language generated from the Flume thrift protocol.When a Flume source receives an event, it stores it into one or more channels. The channel is a passive store that keeps the event until it’s consumed by a Flume sink. The file channel is one example – it is backed by the local filesystem. The sink removes the event from the channel and puts it into an external repository like HDFS (via Flume HDFS sink) or forwards it to the Flume source of the next Flume agent (next hop) in the flow. The source and sink within the given agent run asynchronously with the events staged in the channel. Flume中,数据流的单位称为事件(event) 它具有一个字节的有效载荷,还可以携带一系列的字符串属性. Flume中Agent是一个JVM进程 Agent由一系列的组件组成,通过这些组件,将事件从外部数据源传到另一个位置 Flume源端部分(source)消费着来至外部数据源的事件,例如web服务端的 外部数据源需要以目标flume源端能够识别的形式发出.例如，Avro类型的flume源端可用于接收从Avro客户端发出的事件,或从其他的Avro类型的Flume发送端(sink)发出的事件 flume源端接收事件后,会将事件存入一个或者多个管道(Channel) Flume的管道存储所源端接收到的事件,直到它Flume发送端消费了这些事件.File Channel就是一个例子,它将事件备份在本地文件系统中 flume的发送端消费着Channel中的事件,并把它推送HDFS等外部仓库中(通过HDFS类型的Sink发送) Agent的三个组件 Source 数据源 Channel 通道,缓冲 数据存在哪里 Sink 数据输出到哪里 下载 1wget http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2.tar.gz 配置 所有的配置都参考官网的配置,只要配置agent,source,channel,sink即可 http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#setting-up-an-agent 其中所有的粗体都是必须要配置的,其他普通的属性是可选的 样例 为了便于配置,可以先把这个模板copy到notepad,根据需要改一下就可以 1234567891011121314151617181920212223242526272829303132333435# example.conf: A single-node Flume configuration# Name the components on this agent# agent a1的 source 的名字设置为r1a1.sources = r1 # agent a1的 sinks 的名字设置为k1a1.sinks = k1# agent a1的 channels 的名字设置为c1a1.channels = c1# Describe/configure the source# agent a1的 source r1 的类型设置为netcata1.sources.r1.type = netcat# agent a1的 source r1 的绑定的host和porta1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sink# agent a1的 sinks k1 的类型为logger类型,也就是控制台输出a1.sinks.k1.type = loggerc# Use a channel which buffers events in memory# agent a1的 channels c1 的类型为memory类型,也就是基于内存进行缓冲,所以flume对内存还是由一定要求的a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel# agent a1的 source,channel,sink如何连接# 这里配置了agent a1的source r1的channel为c1# 也就是说,事件从在被sink消费之前被保存在c1的channel中a1.sources.r1.channels = c1# 这里配置了agent a1的sink k1的channel为c1,# 也就是说agent a1的sink k1要从c1 channel消费事件a1.sinks.k1.channel = c1 参数说明 a1 这个是agent的名字,启动flume必须指定agent的名字,独一无二的即可,因为agent是一个独立jvm进程 netcat样例 netcat.conf 12345678910111213141516171819myConsoleAgent.sources = source1myConsoleAgent.sinks = sink1myConsoleAgent.channels = channel1# 数据从哪里来myConsoleAgent.sources.source1.type = netcatmyConsoleAgent.sources.source1.bind = localhostmyConsoleAgent.sources.source1.port = 44444# 输出到哪里去myConsoleAgent.sinks.sink1.type = logger# 读取数据之后先放到哪里myConsoleAgent.channels.channel1.type = memory# 将source,channel,sink连接起来myConsoleAgent.sources.source1.channels = channel1myConsoleAgent.sinks.sink1.channel = channel1 启动agent 1234$FLUME_HOME/bin/flume-ng \\--conf $FLUME_HOME \\--conf-file ./netcat.conf \\--name myConsoleAgent exec 命令行输入 exec.conf 1234567891011121314151617181920212223myConsoleAgent.sources = source1myConsoleAgent.sinks = sink1myConsoleAgent.channels = channel1# 数据从哪里来myConsoleAgent.sources.source1.type = execmyConsoleAgent.sources.source1.command = tail -F /usr/software/flume-1.6.0-cdh5.16.2/input/exec.log # 输出到哪里去myConsoleAgent.sinks.sink1.type = hdfsmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/execmyConsoleAgent.sinks.sink1.hdfs.batchSize = 10myConsoleAgent.sinks.sink1.hdfs.fileType = DataStream myConsoleAgent.sinks.sink1.writeFormat = Text # 读取数据之后先放到哪里myConsoleAgent.channels.channel1.type = memory# 将source,channel,sink连接起来myConsoleAgent.sources.source1.channels = channel1myConsoleAgent.sinks.sink1.channel = channel1 启动agent 1234$FLUME_HOME/bin/flume-ng \\--conf $FLUME_HOME \\--conf-file ./exec.conf \\--name myConsoleAgent spooldir 监听文件夹 spool.conf 12345678910111213141516171819202122232425262728myConsoleAgent.sources = source1myConsoleAgent.sinks = sink1myConsoleAgent.channels = channel1# 数据从哪里来myConsoleAgent.sources.source1.type = spooldirmyConsoleAgent.sources.source1.spoolDir = /usr/software/flume-1.6.0-cdh5.16.2/inputmemory.sources.source1.includePattern = *.log# 输出到哪里去myConsoleAgent.sinks.sink1.type = hdfsmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/spool/%Y%m%d%H%MmyConsoleAgent.sinks.sink1.hdfs.batchSize = 10myConsoleAgent.sinks.sink1.hdfs.fileType = DataStream myConsoleAgent.sinks.sink1.writeFormat = Text myConsoleAgent.sinks.sink1.hdfs.rollInterval=30myConsoleAgent.sinks.sink1.hdfs.rollSize=1024myConsoleAgent.sinks.sink1.hdfs.hdfs.rollCount=100myConsoleAgent.sinks.sink1.hdfs.useLocalTimeStamp =true# 读取数据之后先放到哪里myConsoleAgent.channels.channel1.type = memory# 将source,channel,sink连接起来myConsoleAgent.sources.source1.channels = channel1myConsoleAgent.sinks.sink1.channel = channel1 启动agent 1234$FLUME_HOME/bin/flume-ng \\--conf $FLUME_HOME \\--conf-file ./spool.conf \\--name myConsoleAgent taildir 监听文件夹(最常用的) 这个监听最常用,也是必须掌握的,因为spoolDir并没有偏移量的概念,使用有很大的局限性 spooldir的缺点 每次采集完成之后,会在文件结尾设置Complete的后缀,而且之后不能再使用相同的文件名,否则报错 采集完之后的文件不能再次写入,否则报错 而taildir很好的使用了偏移量的概念,记录在一个json文件中,可以实现断点还原 taildir.conf 123456789101112131415161718192021222324252627282930313233343536myConsoleAgent.sources = source1myConsoleAgent.sinks = sink1myConsoleAgent.channels = channel1# 数据从哪里来myConsoleAgent.sources.source1.type = TAILDIRmyConsoleAgent.sources.source1.filegroups = f1 f2myConsoleAgent.sources.source1.filegroups.f1 = input/taildir/test1/hello.txtmyConsoleAgent.sources.source1.headers.f1.headerKey1 = value1myConsoleAgent.sources.source1.filegroups.f2 = input/taildir/test2/.*.logmyConsoleAgent.sources.source1.headers.f2.headerKey1 = value2-1myConsoleAgent.sources.source1.headers.f2.headerKey2 = value2-2myConsoleAgent..sources.source1.maxBatchCount = 1000myConsoleAgent.sources.source1.fileHeader = true # 输出到哪里去myConsoleAgent.sinks.sink1.type = hdfsmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/tailDir/%Y%m%d%H%MmyConsoleAgent.sinks.sink1.hdfs.batchSize = 100myConsoleAgent.sinks.sink1.hdfs.fileType = DataStream myConsoleAgent.sinks.sink1.writeFormat = Text myConsoleAgent.sinks.sink1.hdfs.rollInterval=30myConsoleAgent.sinks.sink1.hdfs.rollSize=10240myConsoleAgent.sinks.sink1.hdfs.hdfs.rollCount=100myConsoleAgent.sinks.sink1.hdfs.useLocalTimeStamp =true# 读取数据之后先放到哪里myConsoleAgent.channels.channel1.type = memory# 将source,channel,sink连接起来myConsoleAgent.sources.source1.channels = channel1myConsoleAgent.sinks.sink1.channel = channel1 启动agent 1234$FLUME_HOME/bin/flume-ng \\--conf $FLUME_HOME \\--conf-file ./taildir.conf \\--name myConsoleAgent","categories":[{"name":"Flume","slug":"Flume","permalink":"https://lurongjiang.github.io/categories/Flume/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://lurongjiang.github.io/tags/Flume/"}]},{"title":"Azkaband的安装和使用","slug":"Azkaban-的安装和使用","date":"2019-04-03T06:24:04.000Z","updated":"2019-04-05T01:27:49.000Z","comments":true,"path":"2019/04/03/Azkaban-的安装和使用/","link":"","permalink":"https://lurongjiang.github.io/2019/04/03/Azkaban-%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","excerpt":"工作流调度框架Azkaban的安装和简单使用教程","text":"Azkaband的安装和使用 Azkaban可以设计很复杂的工作流,解决任务之间的依赖关系,还是很方便的 Azkaban是一个由LinkedIn公司开发的分布式工作流管理框架,主要是为了解决Hadoop工作之间的依赖关系.例如,我们需要有顺序地执行任务,把ETL任务的数据导入到RMBS中,以提供数据分析支持. Azkaban是一个开源的工作流管理框架 Azkaban是由LinkedIn(领英,还有一个比较出名的产品Kafka)公司创建,用来管理批处理工作流的任务调度 Azkaban提供了WebUI接口来维护工作流 Feature Compatible with any version of Hadoop 兼容Hadoop各个版本 Easy to use web UI 提供易用的WebUI Simple web and http workflow uploads web和http方式上传工作流(这个被诟病了,应该提供拖拉拽的,写了配置还要上传,确实不方便) Project workspaces Scheduling of workflows 工作流调度 Modular and pluginable 模块化,可插拔 Authentication and Authorization 提供认证和授权 Tracking of user actions Email alerts on failure and successes 任务失败和成功邮件告警 SLA alerting and auto killing SLA告警和自动结束任务 Retrying of failed jobs 任务失败重试 版本选择 2.x 基本可以不用看了 3.x 当前推荐的 模式 stand alone mode(or solo-server) 单个 web server和executor server运行在同一个进程里 数据存储的数据库是内嵌的,不需要自己安装 主要使用在小规模场景中 Multiple executor mode 多用于生产 存储的DB应该是一个主从结构的MySQL web server和executor server运行在不同的host上,这样升级和维护时,互不影响 下载和编译 Azkban不直接提供安装包,需要自己从源码编译 Azkaban是Gradle构建的,Java版本需要1.8以上 1234wget https://github.com/azkaban/azkaban/archive/3.81.0.tar.gztar -zxvf ./3.81.0.tar.gz -C /usr/softwarecd /usr/software/azkaban-3.81.0./gradlew build installDist -x test 这个需要git,如果不安装会报错Failed to apply plugin [id ‘com.cinnober.gradle.semver-git’] 所以先得安装git 1sudo yum install -y git 执行有报错….缺少g++:Could not find Linker ‘g++’ in system path 好吧在安装g++ 12sudo yum install g++sudo yum install -y gcc-c++* 部署 编译完成后会出现3个目录,一个是azkaban-solo-server,azkaban-web-server,azkaban-exec-server,分别对应了单机版的azkaban-executor-server,azkaban-web-server和多实例的azkaban-executor-server 我们只需要把他们目录下的build/distributions的压缩包拷贝出来就行 单机版 1tar -zxvf ./azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz 修改azkban的配置文件azkaban.properties,用默认也可以 1vi conf/azkaban.properties 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# Azkaban Personalization Settings# 设置azkban的名称,这个是UI首页左上角,图标右侧上方的文字azkaban.name=LRJ# UI首页左上角,图标右侧下方的文字azkaban.label=MyAzkabanazkaban.color=#FF3601azkaban.default.servlet.path=/indexweb.resource.dir=web/# 修改一下时区,免得调度时间不对default.timezone.id=Asia/Shanghai# Azkaban UserManager classuser.manager.class=azkaban.user.XmlUserManageruser.manager.xml.file=conf/azkaban-users.xml# Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projectsdatabase.type=h2h2.path=./h2h2.create.tables=true# Velocity dev modevelocity.dev.mode=false# Azkaban Jetty server properties.jetty.use.ssl=falsejetty.maxThreads=25# Web端口,端口占用的时候改一改jetty.port=8081# Azkaban Executor settingsexecutor.port=12321# mail settings# 邮件服务器设置mail.sender=mail.host=# 开启SSL设置# User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.# enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081# when this parameters set then these parameters are used to generate email links.# if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.# azkaban.webserver.external_hostname=myazkabanhost.com# azkaban.webserver.external_ssl_port=443# azkaban.webserver.external_port=8081# 全局邮箱设置job.failure.email=job.success.email=lockdown.create.projects=falsecache.directory=cache# JMX statsjetty.connector.stats=trueexecutor.connector.stats=true# Azkaban plugin settings# 插件设置azkaban.jobtype.plugin.dir=plugins/jobtypes# Number of executions to be displayedazkaban.display.execution_page_size=16azkaban.use.multiple.executors=true# Azkaban Ramp Feature Configuration#Ramp Feature Relatedazkaban.ramp.enabled=trueazkaban.ramp.status.polling.enabled=trueazkaban.ramp.status.polling.interval.min=30azkaban.ramp.status.push.interval.threshold=15azkaban.ramp.status.pull.interval.threshold=100 添加个用户名和密码,也可以使用默认的 12345678&lt;azkaban-users&gt; &lt;user groups=\"azkaban\" password=\"azkaban\" roles=\"admin\" username=\"azkaban\"/&gt; &lt;!--添加自己的用户--&gt; &lt;user groups=\"lurongjiang\" password=\"lurongjiang\" roles=\"admin\" username=\"lurongjiang\"/&gt; &lt;user password=\"metrics\" roles=\"metrics\" username=\"metrics\"/&gt; &lt;role name=\"admin\" permissions=\"ADMIN\"/&gt; &lt;role name=\"metrics\" permissions=\"METRICS\"/&gt;&lt;/azkaban-users&gt; 启动 123./bin/start-solo.sh# 查看一下jps -m 访问webUI端口就可以显示了,可以看到我们改的两个位置 输入用户名和密码就就可以进去了 使用 参考Create Flows 创建一个名字为flow20.project的文件,写入内容: 1azkaban-flow-version: 2.0 创建一个basic.flow的文件,写入内容: 12345nodes: - name: jobA type: command config: command: echo \"This is an echoed text.\" 把两个文件打个zip包 在WebUI中创建工程 上传打包好的附件 这样project就有任务了 点击运行后就有任务执行历史了 依赖型任务 依赖型可以使用dependsOn来进行关联 12345678910111213141516171819202122232425262728nodes: - name: jobC type: noop # jobC depends on jobA and jobB dependsOn: - jobA - jobB - name: jobA type: command config: command: echo \"This is an echoed text.\" - name: jobB type: command config: command: pwd - name: jobD type: command config: command: echo 'I am D job' - name: jobE type: command dependsOn: - jobC - jobD config: command: echo 'I am E' 这样就配置三个任务,其中A,B,D任务互不干扰,同时运行,jobC必须要等任务A,B执行完了才执行,E任务需要任务C,D都执行完才执行,依赖关系 注意: 文件和yml文件格式类似,但是千万不要用tab,这个不识别\\t 不同的project,需要多次创建project,同一个project,后一次上传的会覆盖掉前一次上传的 .flow文件的文件名就是project中显示的flow的name 点击job进去,可以对参数进行设置 定时调度 点击Schedule可以设置定时任务的周期, 这样就可在Scheduling下看到定时任务列表,等待下一个时钟来临就会执行 如果需要移除,可以RemoveSchedule","categories":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://lurongjiang.github.io/categories/Azkaban/"}],"tags":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://lurongjiang.github.io/tags/Azkaban/"}]},{"title":"Scala基础-隐式转换","slug":"scala基础-隐式转换","date":"2019-02-26T02:09:42.000Z","updated":"2019-03-01T07:45:26.000Z","comments":true,"path":"2019/02/26/scala基础-隐式转换/","link":"","permalink":"https://lurongjiang.github.io/2019/02/26/scala%E5%9F%BA%E7%A1%80-%E9%9A%90%E5%BC%8F%E8%BD%AC%E6%8D%A2/","excerpt":"Scala隐式转换","text":"scala基础-隐式转换 隐式转换:A=&gt;B 发现A不满足的情况下,自动转成B 通过隐式转换可以对原来的类进行增强 大家可能经常在博客上看到这样的例子,File本身没有read方法,这时可以通过隐式转换来对File的宫娥能进行增强 隐式类型转换 例子1:”人”会飞 普通的人是不会飞的,超人会飞 所以怎么才能把人转成超人呢,看例子 1234567891011121314151617181920212223/** * 人只有一个name属性,并没有任何方法 * * @param name name */class Man(val name: String) &#123;&#125;class SuperMan(val name: String) &#123; def fly(): Unit = &#123; println(s\"$&#123;name&#125; can fly.....\") &#125;&#125;object TestMan2SuperMan &#123; def main(args: Array[String]): Unit = &#123; // 定义一个从Man到SuperMan的隐式转换,implicit修饰 implicit def man2SuperMan(man: Man): SuperMan = &#123; new SuperMan(man.name) &#125; //new一个人,你可以发现\"人\"可以飞了 new Man(\"Zhangsan\").fly() &#125;&#125; 可以发现,scala在执行时,发现Man是没用fly方法的,会尝试找一个隐式转换,接收Man类型,并且有fly方法的隐式转换来进行隐式的转成目标类型,执行指定的方法 隐式转换的公式 implicit def x2y(x):y=new y 函数接收一个普通的x,转成另一个东西的y 例子2:File.read File本身没有read方法,那么我们可以使用隐式转换来增强File,使它”具有”read功能 1234567891011121314151617181920class HenceFile(val file: File) &#123; def read() = &#123; var source: BufferedSource = null try &#123; source = Source.fromFile(file.getPath) source.mkString &#125; finally &#123; source.close() &#125; &#125;&#125;object FileHenceTest &#123; def main(args: Array[String]): Unit = &#123; //定义一个方法,接收普通的File,返回HenceFile,并用implicit修饰 implicit def read(file: File): HenceFile = new HenceFile(file) //这样普通的File就可以read了 println(new File(\"input/test.txt\").read()) &#125;&#125; 可以看的出,隐式转换的流程: 使用implicit关键字定义一个函数 这个函数接受你要转换的普通类A,返回你想要的类B,这样在A没有的功能,而B有的,A也可以使用,从而达到对A类的增强 隐式参数 隐式参数,当函数发现找不到参数时会去找对应的隐式参数来填充 12345678910111213object ImplicitParamTest &#123; def main(args: Array[String]): Unit = &#123; implicit val start =0 def add(num:Int)(implicit step:Int) =&#123; num+step &#125; //sum使用了柯里化,但是第二个参数是隐式参数 //scala在运行时发现不带第二个参数,会去找有没有隐式的Int类型参数 //如果发现存在,就使用那个隐式参数来作为第二个值 println(add(25)) //25 println(add(23)(24)) //47 &#125;&#125; 注意隐式参数必须放在最后面,如果有多个隐式参数,只需要写一个implicit,参数合起来 12345678910//如果只有一个隐式参数,隐式参数必须是最后面的,也就是implicit修饰最后一个// 否则报错,如add(x: Int)(implicit y: Int)(z: Int)就是错的//标识y,z都是隐式参数,但是只能写一个implicit// add(x: Int)(implicit y: Int, z: Int)implicit val num = 25def add(x: Int)(implicit y: Int, z: Int) = x + y + zprintln(add(25)) //75,因为y,z都是25println(add(25)(10,14)) //49 隐式类 不需要new新的类型了,直接对增强类进行implicit进行修饰,这样就可以粗暴的进行增强 12345678object AddTest &#123; implicit class AddClass(str: String) &#123; def add(num: Int) = num + str.toInt &#125; def main(args: Array[String]): Unit = &#123; println(\"12\".add(15)) //27 &#125;&#125; 字符串本来是没用add方法的,但是通过隐式类,很方便的进行类型转换和计算 但是这个implicit class只能定义在object内,外面会报错","categories":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/tags/Scala/"}]},{"title":"scala基础-高阶函数","slug":"scala基础-高阶函数","date":"2019-02-17T07:10:52.000Z","updated":"2019-02-25T12:29:57.000Z","comments":true,"path":"2019/02/17/scala基础-高阶函数/","link":"","permalink":"https://lurongjiang.github.io/2019/02/17/scala%E5%9F%BA%E7%A1%80-%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0/","excerpt":"Scala的高阶函数","text":"scala基础-高阶函数 “_” 的使用 将函数赋值给变量 123456789def describe(firstWorker:String,secondWorker:String)=&#123; println(\"the first worker is \"+firstWorker) println(\"the second worker is \"+secondWorker)&#125;val hello1 = describe _ val hello2 = describe(_) hello1(\"Zhangsan1\",\"Lisi1\")hello2(\"Zhangsan2\",\"Lisi2\") 匿名函数 12val fun1=(x:Int,y:Int)=&gt;x+yfun1(5,4) map 对每一个元素作用一个函数 123456789val list=List(1,2,3,4,5)// _代表迭代的每一个元素val list1=list.map(_*2)val list2=list.map(_+2)list.mkString(\",\") //1,2,3,4,5list1.mkString(\",\") //2,4,6,8,10list2.mkString(\",\") //3,4,5,6,7 foreach 遍历每一个元素 12val arr= 1 to 10arr.foreach(println) filter 过滤符合要求的元素 12val arr=1 to 10arr.filter(_%2==0).mkString(\",\") //2,4,6,8,10 reduce 合并结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849val arr=1 to 10arr.reduce(_+_) //55arr.reduce((a,b)=&gt;&#123; println(a,b) a-b&#125;) // -53/*(1,2)(-1,3)(-4,4)(-8,5)(-13,6)(-19,7)(-26,8)(-34,9)(-43,10)*/arr.reduceLeft((a,b)=&gt;&#123; println(a,b) a-b&#125;) // -53/*(1,2)(-1,3)(-4,4)(-8,5)(-13,6)(-19,7)(-26,8)(-34,9)(-43,10)*/arr.reduceRight((a,b)=&gt;&#123; println(a,b) a-b&#125;) // -5/*(9,10)(8,-1)(7,9)(6,-2)(5,8)(4,-3)(3,7)(2,-4)(1,6)*/ fold 柯里化的sum 123456789101112131415161718val arr=1 to 10arr.fold(0)(_+_) //55arr.fold(0)((a,b)=&gt;&#123; println(a,b) a-b&#125;) //-55/*(0,1)(-1,2)(-3,3)(-6,4)(-10,5)(-15,6)(-21,7)(-28,8)(-36,9)(-45,10)*/ zip 拉链,只zip能对应的,缺的直接丢弃 123val a=List(\"A\",\"B\",\"C\",\"D\")val b=List(\"a\",\"b\",\"c\",\"d\",\"e\",\"f\")a.zip(b) //List((A,a), (B,b), (C,c), (D,d)) 注意,这个和Spark中不一样 Spark要求两边必须保持相同的个数 flattern 压平 12345val list=List(List(1,2,3),List(4,5,6),List(7,8,9))list.flatten //List(1, 2, 3, 4, 5, 6, 7, 8, 9)val list=List(Array(1,2,3),Array(4,5,6),Array(7,8,9))list.flatten //List(1, 2, 3, 4, 5, 6, 7, 8, 9) flatMap 等于map+flatten 12val list=List(List(1,2,3),List(4,5,6),List(7,8,9))list.flatMap(_.map(_*2)) //List(2, 4, 6, 8, 10, 12, 14, 16, 18) groupBy 12val list=List((\"math\",65),(\"english\",75),(\"math\",84),(\"physic\",75))val group=list.groupBy(_._1) mapValues 12val list=List((\"math\",65),(\"english\",75),(\"math\",84),(\"physic\",75))val group=list.groupBy(_._1).mapValues(x=&gt;x.map(_._2).sum) sortBy 数值按大小 字符串按字典顺序 123456val list=List(45,15,32,78,14)list.sortBy(-_) //78, 45, 32, 15, 14val list=List((\"math\",65),(\"english\",75),(\"math\",84),(\"physic\",75))val group=list.groupBy(_._1).mapValues(x=&gt;x.map(_._2).sum)group.toList.sortBy(-_._2) diff 1234567val a= 5 to 10val b=7 to 12a.diff(b) //Vector(5, 6)b.diff(a) //Vector(11, 12)a.union(b) //Vector(5, 6, 7, 8, 9, 10, 7, 8, 9, 10, 11, 12)a.union(b).distinct //Vector(5, 6, 7, 8, 9, 10, 11, 12) 柯里化-currying 将接收多个参数的函数变成接受单个参数的函数 12345def next(num:Int)(step:Int=1)=&#123; step+num&#125;next(15)(5)next(15)() 偏函数-PartialFunction 和模式匹配类似,只是没有了match部分,只包含了大括号和里面的case代码片段 偏函数就是对部分数据进行处理,因为只有符合case的才会被相应的处理函数处理 集合的collect方法可以接收一个偏函数 1234567891011121314151617181920val arr=List(1,2,3,4.5,\"AA\",'A')//筛选数值类型的数据进行相应的处理val result = arr collect &#123; case x:Int=&gt;x+1 case x:Double=&gt;x*x&#125;//List[Double] = List(2.0, 3.0, 4.0, 20.25)val arr=List(1,2,3,4.5,6,7)//星期转换val result = arr collect &#123; case 1 =&gt; \"Monday\" case 2=&gt;\"Tuesday\" case 3 =&gt; \"Wednesday\" case 4=&gt;\"Thursday\" case 5 =&gt; \"Friday\" case 6=&gt;\"Saturday\" case _ =&gt; \"Sunday\"&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/tags/Scala/"}]},{"title":"Scala基础-模式匹配","slug":"scala基础-模式匹配","date":"2019-02-04T13:41:21.000Z","updated":"2019-02-06T06:16:06.000Z","comments":true,"path":"2019/02/04/scala基础-模式匹配/","link":"","permalink":"https://lurongjiang.github.io/2019/02/04/scala%E5%9F%BA%E7%A1%80-%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D/","excerpt":"Scala模式匹配","text":"scala基础-模式匹配 match case 标准调用格式 123456变量 match&#123; case 条件1=&gt;逻辑1 case 条件2=&gt;逻辑2 ... _ =&gt;逻辑xxx&#125; 常量枚举 1234567val array=Array(1,5,6,8,11)val index=scala.util.Random.nextInt(5)index match &#123; case 0 =&gt; println(\"less than 4\") case 1 =&gt; println(\"the num is\"+array(index)) case _ =&gt; println(\"default condition\")&#125; 类型匹配 12345678910111213val tuple=(1,\"AB\",Array(\"Zhangsan\"),Array(\"Zhangsan\",\"male\"))def hello(obj:Any)=&#123; obj match &#123; //==Array(\"Zhangsan\") case Array(\"Zhangsan\")=&gt;println(\"I am Zhangsan\") //Zhangsan 开头的Array case Array(\"Zhangsan\",_*)=&gt;println(\"Hello,Zhangsan\") case _:Int=&gt; println(\"This is Int number:\"+obj) case _=&gt; println(\"Something else\") &#125;&#125;hello(tuple.productElement(scala.util.Random.nextInt(4))) 异常捕获 123456try&#123; val value=1/0&#125;catch&#123; case e:ArithmeticException=&gt;println(\"divided num cannot be 0\") case _:Exception=&gt; println(\"Other Exception\")&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/tags/Scala/"}]},{"title":"scala基础-数组和集合","slug":"scala基础-集合","date":"2019-01-29T03:57:29.000Z","updated":"2019-02-04T21:53:51.000Z","comments":true,"path":"2019/01/29/scala基础-集合/","link":"","permalink":"https://lurongjiang.github.io/2019/01/29/scala%E5%9F%BA%E7%A1%80-%E9%9B%86%E5%90%88/","excerpt":"Scala的数组和集合","text":"scala基础-数组和集合 定长数组-Array new Array()需要声明类型和长度,否则泛型是Nothing,无法进行后续赋值 123456val array=new Array[Int](5)for (x&lt;-0 until 5)&#123; array(x)=x&#125;array.foreach(println)//输出:0,1,2,3,4 Array有apply方法,可以不需要new 12val array=Array(\"a\",\"b\",\"c\")array.foreach(println) 取数据,直接下标取,注意下标是从0开始的 1234val array=Array(\"a\",\"b\",\"c\")println(array(0),array(2))//输出(a,c),//array(3)会报错java.lang.ArrayIndexOutOfBoundsException 遍历 123456val array=Array(\"a\",\"b\",\"c\")for(x&lt;-array)&#123; print(x) print(\",\")&#125;array.foreach(println) 反转 reverse 12val array=Array(\"a\",\"b\",\"c\")array.foreach(println) 合并 ++ 1234val array1=Array(\"a\",\"b\")val array2=Array(1,2,3)val arr=array1 ++ array2arr.foreach(println) 针对数值类型 12val array=Array(1,2,3,4,5)println(array.min,array.max,array.sum) mkString 1234val array=Array(1,2,3,4,5)println(array.mkString(\",\"))//带首尾println(array.mkString(\"[\",\",\",\"]\")) 变长数组-ArrayBuffer 初始化 1234import scala.collection.mutable.ArrayBuffer//可以传入初始容量ArrayBuffer[String](2),默认是16val array=ArrayBuffer[String]() 添加单个元素 += 元素 12array+=\"a\"array+=\"b\" 加数组 ++ 数组 1234//但是这个返回的是新的数组,并不是再array的基础上加的val result=array ++ Array(\"1\",\"2\")println(array.mkString(\",\"))println(result.mkString(\",\")) 指定位置插入 insert 123//第一个是要插入的下标,后面为可变参数array.insert(0,\"a\",\"b\",\"c\")println(array.mkString(\",\")) 删除指定下标的元素,remove(index) 12array.remove(0)println(array.mkString(\",\")) 从某个位置开始,删除N个,remove(index,count) 1234val array=ArrayBuffer(\"a\",\"b\",\"c\",\"d\")array.remove(0,3)println(array.mkString(\",\"))//输出 d 从尾部开始删除 trimEnd(count) 1234val array=ArrayBuffer(\"a\",\"b\",\"c\",\"d\")array.trimEnd(2)println(array.mkString(\",\"))//输出 a,b 变长到定长, toArray 12val array=ArrayBuffer(\"a\",\"b\",\"c\",\"d\")array.toArray.mkString(\",\") 定长到变长 12val array=Array(\"a\",\"b\",\"c\",\"d\")array.toBuffer.mkString(\",\") 遍历 1234val array=ArrayBuffer(\"a\",\"b\",\"c\",\"d\")for(x&lt;-array)&#123; println(x)&#125; 定长集合-List List有两个子样例类,Nil,scala.::Nil 是List为空的情形, scala.collection.immutable.Nil = List[Nothing]() 初始化 1val list=List(1,2,3,4) 第一个元素 head 12list.head//输出 1 tail 1234list.tail//List(2,3,4)list.tail.tail//List(3,4) 最后加一个元素 :+ 12345val list1=list :+ 4list.mkString(\",\")list1.mkString(\",\")//可以感受到 :+ 返回的是一个新的List,也就是list和list1不一样 再List前面加单个元素 +: 12345678val list1=list :+ 4val list2= 4 +: listlist.mkString(\",\") // 1,2,3,4list1.mkString(\",\") // 1,2,3,4,4list2.mkString(\",\") // 4,1,2,3,4//这个也是返回新的List,原来的没法变 ::Nil添加 1234567891::2::3::Nil //List(1,2,3)val list1 = 1::listval list2 = 1::2::listlist.mkString(\",\") // 1,2,3,4list1.mkString(\",\") // 1,1,2,3,4list2.mkString(\",\") // 1,2,1,2,3,4//这个也是返回新的List,原来的不变,总之定长List永远是不变的 变长集合-ListBuffer 初始化 123import scala.collection.mutable.ListBufferval list=ListBuffer[Int]() 加单个元素 += 123list+=2list+=3list.mkString(\",\") //2,3 加定长集合 ++ 12345val list1=list ++ List(1,2,3,4)list.mkString(\",\") //2,3list1.mkString(\",\") //2,3,1,2,3,4//这个返回的也是新的 ListBuffer,原来的不变 删除元素 -= ,注意这里并不是下标,而是元素 12list1 -= 2 //3,1,2,3,4list1 -= 2 //3,1,3,4 删除多个元素 12val list1= ListBuffer(1,6,2,7,1,2,1,3,4,3) list1 --= List(1,2,3) //6, 7, 1, 2, 1, 4, 3 head 12val list=ListBuffer(5)list.head //5 tail 123val list=ListBuffer(5)list.tail //ListBuffer()//注意这里并不是Nil,Nil是List的子样例类 toList 12val list=ListBuffer(5,7,8)list.toList //List(5,7,8) isEmpty 12val list=ListBuffer(5,7,8)list.isEmpty","categories":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/tags/Scala/"}]},{"title":"scala基础-Queue,Set,Tuple,Map","slug":"scala基础-队列","date":"2019-01-26T10:01:35.000Z","updated":"2019-02-02T16:27:22.000Z","comments":true,"path":"2019/01/26/scala基础-队列/","link":"","permalink":"https://lurongjiang.github.io/2019/01/26/scala%E5%9F%BA%E7%A1%80-%E9%98%9F%E5%88%97/","excerpt":"Scala的Queue,Set,Tuple,Map等","text":"scala基础-Queue,Set,Tuple,Map Queue 初始化 12import scala.collection.mutable.Queueval queue=new Queue[Int]() 添加单个元素 += 1queue +=10 添加多个元素 ++ 12345678910val q=new Queue[Int]()q += 1val q1=q ++ List(1,2,3)val q2=q ++ Array(1,2,3,4)q.mkString(\",\") //1q1.mkString(\",\") // 1,1,2,3q2.mkString(\",\") // 1,1,2,3,4//很明显,++依旧是返回新的Queue 入队 enqueue 12val q=new Queue[Int]()q.enqueue(2,3,4) //Queue(2, 3, 4) 出队 dequeue 12345val q=new Queue[Int]()q.enqueue(2,3,4) //Queue(2, 3, 4)//先进先出q.dequeue // 2q.dequeue // 3 Set set也是有两种,一种是mutable,一种是immutable 这里介绍mutable类型的 初始化 12import scala.collection.mutable.Setval set=Set(1,2,3) 添加单个元素 12set += 1 //不变,Set是去重的set += 5 //加 (1,2,3,5) 添加多个元素 12345678val set=Set(1,2,3)set += (6,7,8) //(1, 2, 6, 3, 7, 8)val s1=set ++ Array(4,5,6)set.mkString(\",\") //6,3,7,8s1.mkString(\",\") //5,6,3,7,4,8//很明显,++还是返回新的Set,原来的set并不会发生变化 移除元素 1234val set=Set(1,2,3)set += (6,7,8)set.remove(1) //true,set=(2, 6, 3, 7, 8)set -=2 //(6, 3, 7, 8) Tuple 元组,并不关系数据类型,随便放什么 注意,Tuple最多只能放22个 1234567891011121314// 实例化Tuple的方法1val tuple=(1,'A',\"a\")//Tuple的取数是从1开始数的tuple._1 // 1tuple._2 // A// 实例化Tuple的方法2val tuple1=new Tuple3(1,2,3)tuple1._1// 实例化Tuple的方法3val tuple2 = 'a'-&gt; 'A' //(a,A)val tuple3 = 'a'-&gt; 'A'-&gt; 97 -&gt; 65 // (((a,A),97),65) 遍历 1234val tuple=(1,2,'A',\"B\",5)for(i&lt;- 0 until tuple.productArity)&#123; println(tuple.productElement(i))&#125; swap-针对Tuple2 123456val tuple=(\"zhangsan\",18)val t=tuple.swapprintln(t) //(18,zhangsan)println(tuple) //(zhangsan,18)//很显然swap返回的是一个新的Tuple2 Map Map分mutable和immutable,默认是immutable类型 immutable 1234val map=Map(\"name\"-&gt;\"zhangsan\",\"age\"-&gt;8,\"sex\"-&gt;\"male\")//immutable的Map是没法修改的//如果 map(\"name\")=\"lisi\",会报错 mutable 123456789101112131415161718192021222324252627282930val map=scala.collection.mutable.Map(\"name\"-&gt;\"zhangsan\",\"age\"-&gt;8,\"sex\"-&gt;\"male\")map.mkString(\",\") // age -&gt; 8,name -&gt; zhangsan,sex -&gt; male//修改map(\"name\")=\"lisi\"map.mkString(\",\") //age -&gt; 8,name -&gt; lisi,sex -&gt; male//添加新的key,map(\"key\")=1map.mkString(\",\") //key -&gt; 1,age -&gt; 8,name -&gt; lisi,sex -&gt; male//添加map += (\"k1\"-&gt;\"v1\") //key -&gt; 1, k1 -&gt; v1, age -&gt; 8, name -&gt; lisi, sex -&gt; male//删除map -= \"k1\"map --= Array(\"key\",\"sex\") //(age -&gt; 8, name -&gt; lisi)//遍历for ((k,v)&lt;-map)&#123; println(k+\":\"+v)&#125;for (k&lt;-map.keySet)&#123; println(k+\":\"+map.getOrElse(k,\"\"))&#125;for(v &lt;-map.values)&#123; println(v)&#125; 注意 Map的get方法返回的是一个Option,所以要获得值,还需要再get一次,但这个get当不存在的时候会报错java.util.NoSuchElementException: None.get 为了避免这种情况,一般推荐使用Option.getOrElse来获取最终的结果","categories":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/tags/Scala/"}]},{"title":"Scala基础-构造器","slug":"scala基础-构造器","date":"2019-01-24T04:13:50.000Z","updated":"2019-01-25T12:29:57.000Z","comments":true,"path":"2019/01/24/scala基础-构造器/","link":"","permalink":"https://lurongjiang.github.io/2019/01/24/scala%E5%9F%BA%E7%A1%80-%E6%9E%84%E9%80%A0%E5%99%A8/","excerpt":"Scala中的构造器","text":"Scala基础-构造器 构造器 Scala的构造器分为两种: 主构造器 附属构造器 一个类没有显示定义主构造器的类,会有一个默认的无参构造器作为主构造器 1234567891011121314151617class ConstructNoTest &#123; /** * 两个附属构造器 * @param name */ def this(name: String, age: Int) &#123; //def this定义的附属构造器第一行必须调用主构造器 this &#125; /** * 两个附属构造器 * @param name */ def this(name:String)&#123; this &#125;&#125; 主构造器定义在类的后面 附属构造器第一句必须先调用主构造器 1234567891011121314151617181920212223/** * 主构造器定义在类名的后面 * * @param name * @param age */ class ConstructTest(name: String, age: Int) &#123; // _占位的必须指定数据类型 var sex: String = _ /** * def this 附属构造器 * 第一句必须先调用主构造器 * @param name * @param age * @param sex */ def this(name: String, age: Int, sex: String) &#123; this(name, age) this.sex = sex &#125; &#125; 主构造器中使用val声明可以将参数设置为类的属性 1234567891011121314/** * 主构造器上声明val,可以将参数自动设置为类的属性 * * @param name */class ConstructPro(val name: String) &#123; var age: Int = _ def this(name: String, age: Int) &#123; //显式创建主构造器之后,无参构造器没了,不能this()了 this(name) this.age = age &#125;&#125; 1234567891011object ConstructNoTest &#123; def main(args: Array[String]): Unit = &#123; val zhangsan = new ConstructNoTest(\"zhangsan\") println(zhangsan) val lisi = new ConstructTest(\"lisi\", 18) println(lisi.sex) val wangwu = new ConstructPro(\"wangwu\") //可以直接使用name属性 println(wangwu.name + \":\" + wangwu.age) &#125;&#125; 构造器的执行顺序 先执行父类构造器,再执行子类构造器java 12345678910111213141516171819202122class OrderClass(val name: String, age: Int) &#123; println(\"parent class start,age\" + age) def this(name: String) &#123; this(name, 10) println(\"parent this(name:String,age:Int)\") &#125; println(\"parent class end\")&#125;class OrderClassChild(name: String, val sex: String) extends OrderClass(name) &#123; println(\"child class start\") def this(name: String) &#123; this(name, \"male\") println(\"child this(name:String,age:Int)\") &#125; println(\"child class end\")&#125;object OrderClassTest &#123; def main(args: Array[String]): Unit = &#123; val zhangsan = new OrderClassChild(\"zhangsan\") println(zhangsan.name + \":\" + zhangsan.sex) &#125;&#125; 输出 1234567parent class start,age10parent class endparent this(name:String,age:Int)child class startchild class endchild this(name:String,age:Int)zhangsan:male 可以看出,先执行父类中的一些方法和属性,然后执行父类的构造器,退回来执行子类的属性和方法,最后是子类的构造方法 继承 父类的属性可以被继承,可修改的属性和方法,子类也可以覆盖和重写,必须使用override关键字 12345678910111213class OverrideTest(var name: String) &#123; val age = 45 def play(): Unit = &#123; println(\"parent is playing\") &#125;&#125;class OverrideChild(parentName: String) extends OverrideTest(parentName) &#123; override name = \"zhangsan\" override def play(): Unit = &#123; println(\"child is playing\") &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/tags/Scala/"}]},{"title":"Scala基础-类","slug":"scala基础-类","date":"2019-01-19T11:19:46.000Z","updated":"2019-01-20T01:42:59.000Z","comments":true,"path":"2019/01/19/scala基础-类/","link":"","permalink":"https://lurongjiang.github.io/2019/01/19/scala%E5%9F%BA%E7%A1%80-%E7%B1%BB/","excerpt":"Scala中类的介绍和使用包括抽象类,类的伴生,接口等","text":"Scala基础-类 抽象类 abstract 抽象类可包含未实现的方法和未初始化的属性 不可直接new,否则报错 1234567891011abstract class People &#123; def eat() def play() //抽象类可以具有普通方法 def work(): Unit = &#123; println(name + \"is working.....\") &#125; //不初始化,必须指定类型,否则报错 val name: String&#125; 具体实现交给子类实现 123456789101112class Worker extends People&#123; override def eat(): Unit = &#123; println(name + \"is eating.....\") &#125; override def play(): Unit = &#123; println(name + \"is playing.....\") &#125; override val name: String = \"Zhangsan\"&#125; 使用时直接new具体的实现类即可 1234567object PeopleTest &#123; def main(args: Array[String]): Unit = &#123; val worker = new Worker() worker.play() worker.work() &#125;&#125; 伴生类和伴生对象-Companion 其实在Scala中是不存在静态的属性和方法的概念 但是scala中,object却是和java中类似的用法,不需要new对象,可以直接使用 12345678910111213object ObjectTest &#123; val name = \"zhangsan\" def play(name: String=\"War3\"): Unit = &#123; println(\"play game :\" + name) &#125; def main(args: Array[String]): Unit = &#123; //不需要new直接用 ObjectTest.play() ObjectTest.play(\"FIFA 2019\") &#125;&#125; 名称相同的Class和Object互为伴生 Class称为Object的伴生类 Object称为Class的伴生对象 但是有一个很特殊的方法apply() 12345678910111213141516171819202122232425262728class CompanionTest&#123; println(\"class start----\") def apply(): Unit = &#123; println(\"apply in class...\") &#125; println(\"class end----\")&#125;object CompanionTest &#123; println(\"object start----\") def apply(): Unit = &#123; println(\"apply in object...\") new CompanionTest() &#125; println(\"object end----\")&#125;object Companion &#123; def main(args: Array[String]): Unit = &#123; val test = new CompanionTest() //对象()调用的是class中的apply方法 test() //不需要new的是调用object中的apply方法 CompanionTest() &#125;&#125; 因为object就是类的伴生对象,所以直接类()也相当于对一个具体的对象进行调用apply() apply()方法很神奇,不同的类都提供了不同的实现,只需要看懂这种用法就行 枚举类-Enumeration 枚举类对于有限个数的类型十分有用 需要通过继承Enumeration 123456789101112131415161718object WeekDay extends Enumeration &#123; val Mon = Value(1, \"星期一\") val Tue = Value(2, \"星期二\") val Wed = Value(3, \"星期三\") val Thu = Value(4, \"星期四\") val Fri = Value(5, \"星期五\") val San = Value(6, \"星期六\") val Sun = Value(7, \"星期日\") def main(args: Array[String]): Unit = &#123; println(WeekDay.Mon) println(WeekDay(2)) println(WeekDay.withName(\"星期三\")) for (elem &lt;- WeekDay.values) &#123; println(elem) &#125; &#125;&#125; 但是枚举类使用不存在的值时会报错 样例类-case class scala中case class称为样例类 使用时不需要new,当然,想加也是ok的 在sparkSQL中大量使用了case class case class已经实现了序列化,不需要实现序列化了 1234567case class CaseClass(name: String, age: Int)object CaseClassTest &#123; def main(args: Array[String]): Unit = &#123; println(CaseClass(\"Jack\", 22)) println(new CaseClass(\"拉布拉多\", 15)) &#125;&#125; 当然了有case class也有case object 需要注意的是 case object不能加参数,否则报错 12345678910111213case class CaseClass(name: String, age: Int)case object CaseClass&#123; def apply(): Unit =&#123; println(\"case object\") &#125;&#125;object CaseClassTest &#123; def main(args: Array[String]): Unit = &#123; println(CaseClass(\"Jack\", 22)) println(new CaseClass(\"拉布拉多\", 15)) CaseClass() &#125;&#125; 接口-trait Scala中和Java接口概念对应的是trait trait的用法和抽象类的用法类似 实现也用extends关键字 多实现,第二个trait开始,使用with进行连接 12345678910111213141516171819202122trait TraitTest &#123;def play()&#125;trait TraitTest1 &#123; def play1()&#125;class TraitTestImpl extends TraitTest with TraitTest1&#123; override def play(): Unit = &#123; println(\"Hello,trait play.....\") &#125; override def play1(): Unit = &#123; println(\"Hello,trait1 play.....\") &#125;&#125;object TraitTestA&#123; def main(args: Array[String]): Unit = &#123; val impl = new TraitTestImpl() impl.play() &#125;&#125; 注意,实现的两个trait不能有相同的方法签名,否则,编译时报错,运行时过不去 例如,下面这个就运行不过去 123456789101112131415161718//错误的案例trait TraitTest &#123;def play()&#125;trait TraitTest1 &#123; def play()&#125;class TraitTestImpl extends TraitTest with TraitTest1&#123; override def play(): Unit = &#123; println(\"Hello,trait play.....\") &#125;&#125;object TraitTestA&#123; def main(args: Array[String]): Unit = &#123; val impl = new TraitTestImpl() impl.play() &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/tags/Scala/"}]},{"title":"Scala基础-循环","slug":"scala基础-循环","date":"2019-01-17T19:01:04.000Z","updated":"2019-01-18T09:45:54.000Z","comments":true,"path":"2019/01/18/scala基础-循环/","link":"","permalink":"https://lurongjiang.github.io/2019/01/18/scala%E5%9F%BA%E7%A1%80-%E5%BE%AA%E7%8E%AF/","excerpt":"Scala循环","text":"Scala基础-循环 to 1 to 5 11,2,3,4,5 1.to(5) 11,2,3,4,5 1 to 5 by 2 带步长 11,3,5 1.to(5).by(2) 11,3,5 Range Range(1,5) 11,2,3,4 Range(1,5,2) 带步长的 11,3 Range(10,4,-2) 倒着来 110,8,6 util 1.until(5) util=Range 11,2,3,4 1.until(5).by(2) 11,3 1 util 5 by 2 11,3 区别,Range是左闭右开的范围,[) to是全闭合的范围,[] 数组遍历 增强for 123456val array=Array(1,2,3,4)//不需要声明x为val还是var,默认是valfor(x&lt;-array)&#123; println(x)&#125;//输出1,2,3,4 下标遍历 12345val array=Array(1,2,3,4)for(x&lt;- 0 until array.length )&#123; println(array(x))&#125;//输出1,2,3,4 带条件的for–守卫 123456val array=Array(1,2,3,4,5,6,7)//以x%2==0作为守卫条件for(x&lt;- 0 until array.length if x%2==0 )&#123; println(array(x))&#125;//输出1,3,5,7 yield推导 1234val array=Array(1,2,3,4,5,6,7)val result=for (x&lt;- array if x%2==0) yield x*2result.foreach(println)//输出( 4, 8, 14) while 12345678//100以内求和var (num,sum)=(100,0)while(num&gt;0)&#123; sum = sum + num num = num-1&#125;println(num,sum)//(0,100) 方法的默认参数 12345678910def hello(name:String=\"zhangsan\")&#123; println(\"Hello,\"+name)&#125;//默认参数也是有参数的,不能省略括号,否则报错hello()hello(\"lisi\")//输出 Hello,zhangsan// Hello,lisi 方法的默认参数很有用,Spark大量使用了这种默认参数,当用户不传时,会执行默认逻辑,如分区数,存储级别 方法的可变参数 1234567891011121314def add(arr:Int*)=&#123; var sum=0 for(x&lt;-arr)&#123; sum=sum+x &#125; sum&#125;add(1,2,3)//scala特殊语法, :_*add(1 to 10 : _*)add(1 to 100 : _*)// 输出 6,55,5050 命名参数 12345def add(x:Int,y:Int)=&#123; x+y&#125;add(10,y=20)//输出 30","categories":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://lurongjiang.github.io/tags/Scala/"}]},{"title":"Hive的一些坑","slug":"Hive-一些坑","date":"2018-10-12T18:41:20.000Z","updated":"2018-10-14T13:50:15.000Z","comments":true,"path":"2018/10/13/Hive-一些坑/","link":"","permalink":"https://lurongjiang.github.io/2018/10/13/Hive-%E4%B8%80%E4%BA%9B%E5%9D%91/","excerpt":"总结一下在使用hive的时候遇到的一些坑","text":"Hive的一些坑 specified datastore driver(“com.mysql.jdbc.Driver”) was not found 这个是因为驱动不对,下载了个新的就行了 Unable to open a test connection to the given database. JDBC url = jdbc:mysql://hadoop001:3306/test?useSSL=true&amp;serverTimezone=GMT%2B8, username = lrj. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: 这个需要把ssl禁用了,在jdbcUrl上指定useSSL=false MetaException(message:Version information not found in metastore. ) 这个需要将hive-site.xml中的hive.metastore.schema.verification设置为false Required table missing : “VERSION“ in Catalog “” Schema “”. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable “datanucleus.autoCreateTables” 这个需要初始化一下schema,执行 schematool -dbType mysql -initSchema 之后就可以启动metastore + hiveserver2服务 12nohup hive --service metastore &gt; ~/metastore.log 2&gt;&amp;1 &amp;nohup hiveserver2 &gt; ~/hiveserver2.log 2&gt;&amp;1 &amp; 测试hiveserver2服务是否ok 1beeline 打印日志 1234567891011which: no hbase in (&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;sbin:&#x2F;mysql&#x2F;bin:&#x2F;home&#x2F;lurongjiang&#x2F;.local&#x2F;bin:&#x2F;home&#x2F;lurongjiang&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;hadoop-2.6.0-cdh5.16.2&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;hadoop-2.6.0-cdh5.16.2&#x2F;sbin:&#x2F;usr&#x2F;software&#x2F;jdk1.8.0_231&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;apache-maven-3.6.3&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;scala-2.11.12&#x2F;bin:&#x2F;usr&#x2F;software&#x2F;hive-1.1.0-cdh5.16.2&#x2F;bin)Beeline version 1.1.0-cdh5.16.2 by Apache Hive# 查看下数据库,此时发现没连接beeline&gt; show databases;No current connection# 尝试连接数据库,只需要输入用户名就行,不需要密码beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;defaultConnecting to jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;defaultEnter username for jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;default: lrjEnter password for jdbc:hive2:&#x2F;&#x2F;hadoop001:10000&#x2F;default: Error: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user&#x3D;lrj, access&#x3D;EXECUTE, inode&#x3D;&quot;&#x2F;tmp&quot;:lurongjiang:supergroup:drwx java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=lrj, access=EXECUTE, inode=”/tmp”:lurongjiang:supergroup:drwx—— 这个是没权限 hadoop fs -chmod -R 777 /tmp 再次启动就ok了.","categories":[{"name":"Hive","slug":"Hive","permalink":"https://lurongjiang.github.io/categories/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://lurongjiang.github.io/tags/Hive/"}]},{"title":"Hive UDF","slug":"Hive-UDF","date":"2018-10-10T06:27:21.000Z","updated":"2018-10-10T17:10:59.000Z","comments":true,"path":"2018/10/10/Hive-UDF/","link":"","permalink":"https://lurongjiang.github.io/2018/10/10/Hive-UDF/","excerpt":"Hive UDF的介绍和基本使用","text":"Hive UDF hive内置函数并不一定满足我们的业务要求,所以需要拓展,即用户自定义函数 UDF User Defined Function UDF (one-to-one) UDAF(many-to-one) UDTF(one-to-many) 创建UDF步骤 添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.cdh.version&#125;&lt;/version&gt;&lt;/dependency&gt; 创建自定义类,继承UDF","categories":[{"name":"Hive","slug":"Hive","permalink":"https://lurongjiang.github.io/categories/Hive/"}],"tags":[{"name":"hive","slug":"hive","permalink":"https://lurongjiang.github.io/tags/hive/"},{"name":"udf","slug":"udf","permalink":"https://lurongjiang.github.io/tags/udf/"},{"name":"user-defined-function","slug":"user-defined-function","permalink":"https://lurongjiang.github.io/tags/user-defined-function/"}]},{"title":"HDFS块损坏修复","slug":"Hadoop-HDFS块损坏修复","date":"2018-09-14T07:35:56.000Z","updated":"2018-09-18T14:49:30.000Z","comments":true,"path":"2018/09/14/Hadoop-HDFS块损坏修复/","link":"","permalink":"https://lurongjiang.github.io/2018/09/14/Hadoop-HDFS%E5%9D%97%E6%8D%9F%E5%9D%8F%E4%BF%AE%E5%A4%8D/","excerpt":"生产者,HDFS块损坏了怎么办?hdfs fsck帮助你修复.","text":"HDFS块损坏修复 准备 创建一个文件夹 1hdfs dfs -mkdir /blockrecover 准备一个文件 1echo \"hello world\" &gt;&gt; test.txt 上传文件到hdfs 1hdfs dfs -put test.txt/blockrecover 检查健康状态 1hdfs fsck / 查看一下block信息 在data目录查找block信息 1find ./data -name \"blk_1073741826_1002*\" 删除block和meta信息 找到之后cd进去吧meta和对应的block删除了,模拟块丢失 重启hdfs(因为默认fsck间隔时间是6个小时,这里重启) 再来检查 1hdfs fsck /blockrecover 手动修复 hdfs debug 123456789Usage: hdfs debug &lt;command&gt; [arguments]These commands are for advanced users only.Incorrect usages may result in data loss. Use at your own risk.verifyMeta -meta &lt;metadata-file&gt; [-block &lt;block-file&gt;]computeMeta -block &lt;block-file&gt; -out &lt;output-metadata-file&gt;recoverLease -path &lt;path&gt; [-retries &lt;num-retries&gt;] 这个命令是隐藏的,可能是为了防止滥用,只让专业人员知道 修复命令 1hdfs debug recoverLease -path /blockrecover/test.txt -retries 3 这个可能会成功,可能会失败,所以多试几次 注意这个必须要指定到文件,目录是不行的 这样再次检查,就发现是ok了 block也恢复了","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/tags/Hadoop/"}]},{"title":"Hadoop HA部署完整文档","slug":"Hadoop-HA部署文档","date":"2018-09-09T16:32:17.000Z","updated":"2018-09-10T09:47:37.000Z","comments":true,"path":"2018/09/10/Hadoop-HA部署文档/","link":"","permalink":"https://lurongjiang.github.io/2018/09/10/Hadoop-HA%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/","excerpt":"Hadoop HA With JQM 部署文档,按照要求就可以实现.","text":"Hadoop HA部署完整文档 主机规划 公网 内网 hostname role 47.108.63.184 172.24.144.125 hadoop001 NN,ZKFC,JN,DN,JobHistory,NM,QuorumPeerMain 47.108.92.139 172.24.144.126 hadoop002 NN,ZKFC,JN,DN,JobHistory,NM,QuorumPeerMain 47.108.51.164 172.24.144.127 hadoop003 DN,JN,NM 目录规划 $HAOOP_HOME /home/hadoop/app/hadoop Data $HADOOP_HOME/data Log $HADOOP_HOME/logs hadoop.tmp.dir $HADOOP_HOME/tmp 需要手工创建,权限 777,root:root $ZOOKEEPER_HOME /home/hadoop/app/zookeeper 三台同时操作 1mkdir -p /usr/java 配置host 172.24.144.125 hadoop001172.24.144.126 hadoop002172.24.144.127 hadoop003 123ehco \"172.24.144.125 hadoop001\" &gt;&gt; /etc/hostsehco \"172.24.144.126 hadoop002\" &gt;&gt; /etc/hostsehco \"172.24.144.127 hadoop003\" &gt;&gt; /etc/hosts 添加hadoop用户 zk和hadoop只需要hadoop用户安装就可以了,先新建一个hadoop用户 三台同时操作 123useradd hadoopsu - hadoopmkdir app hadoop用户免密配置 生成密钥,一路enter,三台同时操作 12su - hadoopssh-keygen 因为hadoop用户没用设置登录密码,所以无法外部登录,只能把公钥拷贝出来 1cat ~/.ssh/id_rsa.pub 123ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDTosbqrdcGhqcTc+VS4pU87bW0E0jAOxnnX0aqYVv0mWFl2qhm5EJwzserJ1ucpw91pBMt7iAYZ9Mi0SKVRdJ9rGKDtxKO2W5m06A1fY4&#x2F;FK2iuuoCsRW79Dyl5Y26&#x2F;+J6trxjWOI6uGL7PBLReoJQ6iO34R6lq+ejKRhoblUMtSwIVPr&#x2F;kGzlYLRrc5SaBe4d4LuaHA+&#x2F;jB5UDDarZmqTQambwevOPGl8IVfp0ute&#x2F;NZySFBb5+1VjwB7L&#x2F;GarNPY7eFEY5LhRgnlqd6f4chnzFMBQagyJTOOZYHTmkWYH+wjsTohwWgXBU0CemMNxY3Y0fG6&#x2F;8qOvCxDntX+Ma7F hadoop@hadoop001ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDmDr8N0Pg8&#x2F;X13NgEUOLOeiWy&#x2F;PjAI59uaREnNjj+jagpi6ED+LOpflzZwypKGiu0EecX7MrC2ct6ElJeGzvI6ZigiE6brt2ufWjGGy2IITxgl6lvAxbC84WvLOkfXqyaydmv&#x2F;omOrxtshyKbzjDgNyEqUb+3fGtYz59rW+MKIG9nekWvGBZcA4zo46g&#x2F;kUmra&#x2F;9&#x2F;UUdzGAe5CF0tKheeB62uP7JBmv4dIhrR2R7B5BCgc7KAuZrNn&#x2F;DAFLJOIb8P87MVABkLiFwFIHxJV41OKq4RMx7D64VAJdtemRKlTTiA9Zz1alTzcgipWadwRpjqJPpfzu+s2LFirmS&#x2F;l1Btj hadoop@hadoop002ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCys26gHCMPDMdoltsk3Y7cgVSBV0DxM89EpzFLmBejJh11xCtyL20BbTZfRefMN9qd6lYlh6HFPLIpPgXpbtZXwnKUP5gM4eHCFFvzPC5AQDvvuXOmKmv9gkEjr30bCGx8uUuMPuF42YDmgyumz+x1O&#x2F;OFck6PzQh5SorcxoXyB+u2XbXXqrTpvx0rTJPGxJv41t1BmcIqCK4xiYVwR8D+12QomX&#x2F;jGn83Lh3Jurds6+ywbxCzmMxRew4NIDDjBJaJwPtikkfhd+RPMNKf+bCy40D3RhOFWsJ3hjPgbUSHH0jBjo6GRjg6wmVJxiBv1sXHss4N&#x2F;vdwvaBh6F2rcVhT hadoop@hadoop003 12345touch authorized_keysvi authorized_keys#把三台的公钥复制进去,wq!# 修改权限chmod 0600 ~/.ssh/authorized_keys 接着ssh进行验证,第一次需要yes 123ssh hadoop001 datessh hadoop002 datessh hadoop003 date 上传安装包 jdk,zk,hadoop 123su rootyum install -y lrzszrz 解压 12345678910111213141516171819su rootchown -R root:root ./jdk-8u231-linux-x64.tar.gzchown -R hadoop:hadoop ./zookeeper-3.4.5-cdh5.16.2.tar.gzchown -R hadoop:hadoop ./hadoop-2.6.0-cdh5.16.2.tar.gztar -zxvf ./jdk-8u231-linux-x64.tar.gz -C /usr/java/su - hadooptar -zxvf ./zookeeper-3.4.5-cdh5.16.2.tar.gz -C ~/apptar -zxvf ./hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app# 软连接su hadoopln -s $HOME/app/zookeeper-3.4.5-cdh5.16.2 $HOME/app/zookeeperln -s $HOME/app/hadoop-2.6.0-cdh5.16.2 $HOME/app/hadoopsu rootln -s /usr/java/jdk1.8.0_231 /usr/java/java8# 权限chmod -R 777 /opt/software/hadoopchmod -R 777 /opt/software/zookeeper 配置环境变量 1234567891011121314su rootvi /etc/profile# 追加export JAVA_HOME=/usr/java/java8source /etc/profilesu hadoopvi ~/.bash_profileexport HADOOP_HOME=$HOME/app/hadoopexport ZOOKEEPER_HOME=$HOME/app/zookeeperexport PATH=$&#123;PATH&#125;:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$&#123;ZOOKEEPER_HOME&#125;/binsource ~/.bash_profile 安装Zookeeper 123cd $HOME/app/zookeeper/confcp zoo_sample.cfg zoo.cfgvi zoo.cfg 修改zk配置 12345dataDir=/home/hadoop/app/zookeeper/data#增加server.1=hadoop001:2888:3888server.2=hadoop002:2888:3888server.3=hadoop003:2888:3888 增加myid,分别在三台机器执行对应命令 123456cd ~/app/zookeeper/data# 这里注意,1,2,3,&gt;前后的空格echo 1 &gt; ./myidecho 2 &gt; ./myidecho 3 &gt; ./myid 启动zk,三台执行 123zkServer.sh start# 查看状态zkServer.sh status 部署Hadoop hdfs-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!--HDFS超级用户 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt; &lt;/property&gt; &lt;!--开启web hdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop/data/name&lt;/value&gt; &lt;description&gt; namenode 存放name table(fsimage)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;$&#123;dfs.namenode.name.dir&#125;&lt;/value&gt; &lt;description&gt;namenode粗放 transaction file(edits)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop/data/data&lt;/value&gt; &lt;description&gt;datanode存放block本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 块大小128M （默认128M） --&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt; &lt;!--======================================================================= --&gt; &lt;!--HDFS高可用配置 --&gt; &lt;!--指定hdfs的nameservice为ruozeclusterg7,需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;hadoopha&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置NameNode IDs 此版本最大只支持两个NameNode --&gt; &lt;name&gt;dfs.ha.namenodes.hadoopha&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.rpc-address.[nameservice ID] rpc 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.hadoopha.nn1&lt;/name&gt; &lt;value&gt;hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.hadoopha.nn2&lt;/name&gt; &lt;value&gt;hadoop002:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.http-address.[nameservice ID] http 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.hadoopha.nn1&lt;/name&gt; &lt;value&gt;hadoop001:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.hadoopha.nn2&lt;/name&gt; &lt;value&gt;hadoop002:50070&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode editlog同步 ===================== --&gt; &lt;!--保证数据恢复 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.http-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8480&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.rpc-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8485&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置JournalNode服务器地址，QuorumJournalManager 用于存储editlog --&gt; &lt;!--格式：qjournal://&lt;host1:port1&gt;;&lt;host2:port2&gt;;&lt;host3:port3&gt;/&lt;journalId&gt; 端口同journalnode.rpc-address --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/hadoopha&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--JournalNode存放数据地址 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop/data/jn&lt;/value&gt; &lt;/property&gt; &lt;!--==================DataNode editlog同步 =========================== --&gt; &lt;property&gt; &lt;!--DataNode,Client连接Namenode识别选择Active NameNode策略 --&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.hadoopha&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode fencing：====================== --&gt; &lt;!--Failover后防止停掉的Namenode启动，造成两个服务 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少milliseconds 认为fencing失败 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!--==========NameNode auto failover base ZKFC and Zookeeper============== --&gt; &lt;!--开启基于Zookeeper --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--动态许可datanode连接namenode列表 --&gt; &lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop/etc/hadoop/slaves&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.use.datanode.hostname&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; core-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!--Yarn 需要使用 fs.defaultFS 指定NameNode URI --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoopha&lt;/value&gt; &lt;/property&gt; &lt;!--==============================Trash机制======================================= --&gt; &lt;property&gt; &lt;!--多长时间创建CheckPoint NameNode截点上运行的CheckPointer 从Current文件夹创建CheckPoint;默认：0 由fs.trash.interval项指定 --&gt; &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少分钟.Trash下的CheckPoint目录会被删除,该配置服务器设置优先级大于客户端，默认：0 不删除 --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop临时目录, hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配 置namenode和datanode的存放位置，默认就放在这&gt;个路径中 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!--指定ZooKeeper超时间隔，单位毫秒 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- 配置 MapReduce Applications --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- JobHistory Server ============================================================== --&gt; &lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop001:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop001:19888&lt;/value&gt; &lt;/property&gt;&lt;!-- 配置 Map段输出的压缩,snappy--&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185&lt;?xml version=\"1.0\"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt; &lt;!-- nodemanager 配置 ================================================= --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.localizer.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23344&lt;/value&gt; &lt;description&gt;Address where the localizer IPC is.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.webapp.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23999&lt;/value&gt; &lt;description&gt;NM Webapp address.&lt;/description&gt; &lt;/property&gt; &lt;!-- HA 配置 =============================================================== --&gt; &lt;!-- Resource Manager Configs --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 使嵌入式自动故障转移。HA环境启动，与 ZKRMStateStore 配合 处理fencing --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群名称，确保HA选举时对应的集群 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarn-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!--这里RM主备结点需要单独指定,（可选） &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm2&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms&lt;/name&gt; &lt;value&gt;5000&lt;/value&gt; &lt;/property&gt; &lt;!-- ZKRMStateStore 配置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk.state-store.address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- Client访问RM的RPC地址 (applications manager interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23140&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23140&lt;/value&gt; &lt;/property&gt; &lt;!-- AM访问RM的RPC地址(scheduler interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23130&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23130&lt;/value&gt; &lt;/property&gt; &lt;!-- RM admin interface --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23141&lt;/value&gt; &lt;/property&gt; &lt;!--NM访问RM的RPC端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23125&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23125&lt;/value&gt; &lt;/property&gt; &lt;!-- RM web application 地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:10086&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:10086&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop001:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 初始化 启动zk,三台启动 12zkServer.sh startzkServer.sh status 启动jn,三台 1hadoop-daemon.sh start journalnode 初始化zkfc,一台 1hdfs zkfc -formatZK 格式化namenode,一台 123hdfs namenode -format# 复制namenode情况到另一个namenodescp -r ./data/name hadoop002:~/app/hadoop/data 启动hdfs 1start-dfs.sh 启动yarn 1start-yarn.sh jps检查进程情况 hadoop001: 13776 NameNode14179 ResourceManager13220 QuorumPeerMain14295 NodeManager14074 DFSZKFailoverController13852 DataNode13948 JournalNode14942 Jps hadoop002: 14100 NodeManager12420 QuorumPeerMain13814 DFSZKFailoverController13992 ResourceManager15101 Jps13437 DataNode13629 JournalNode hadoop003: 9155 JournalNode9059 DataNode9379 NodeManager9606 Jps8329 QuorumPeerMain hadoop HA遇到的问题 这个好像是我提前创建data目录和tmp目录造成的,我重新删除hadoop的data,tmp目录,以及zk的hadoop-ha的znode,删除zk的data/version-2,重新启动zk,启动jn,formatZK,format namenode就好了 测试 测试standby是否可以创建目录 发现不可以创建目录 测试standby是否可写 发现不可以写 测试standby是否可读 发现不可以读 测试故障是否能会转移 发现故障转移正常 几个有用的命令 hdfs haadmin hdfs getconf hdfs dfsadmin","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/tags/Hadoop/"}]},{"title":"Hadoop MapReduce编程核心","slug":"Hadoop-MapReduce编程核心","date":"2018-09-05T23:29:36.000Z","updated":"2018-09-09T16:40:14.000Z","comments":true,"path":"2018/09/06/Hadoop-MapReduce编程核心/","link":"","permalink":"https://lurongjiang.github.io/2018/09/06/Hadoop-MapReduce%E7%BC%96%E7%A8%8B%E6%A0%B8%E5%BF%83/","excerpt":"Hadoop MapReduce编程核心相关介绍","text":"Hadoop MapReduce编程核心 Partitioner 分区 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Partitions the key space. * * &lt;p&gt;&lt;code&gt;Partitioner&lt;/code&gt; controls the partitioning of the keys of the * intermediate map-outputs. The key (or a subset of the key) is used to derive * the partition, typically by a hash function. The total number of partitions * is the same as the number of reduce tasks for the job. Hence this controls * which of the &lt;code&gt;m&lt;/code&gt; reduce tasks the intermediate key (and hence the * record) is sent for reduction.&lt;/p&gt; * partitioner是控制中间map阶段输出结果的key的分区.key通常被hash,分发到各个分区 * 分区数一般和reduce job的个数相等, * @see Reducer */@InterfaceAudience.Public@InterfaceStability.Stablepublic interface Partitioner&lt;K2, V2&gt; extends JobConfigurable &#123; /** * Get the paritition number for a given key (hence record) given the total * number of partitions i.e. number of reduce-tasks for the job. * * &lt;p&gt;Typically a hash function on a all or a subset of the key.&lt;/p&gt; * 根据分区总数,例如reduce job个数,获取分区的编号.一般是对所有key或者key的一部分进行进行hash处理 * @param key the key to be paritioned. * @param value the entry value. * @param numPartitions the total number of partitions. * @return the partition number for the &lt;code&gt;key&lt;/code&gt;. */ int getPartition(K2 key, V2 value, int numPartitions);&#125;/*** hash分区的实现就是key取hashCode和reduce个数进行取模*/public class HashPartitioner&lt;K2, V2&gt; implements Partitioner&lt;K2, V2&gt; &#123; public void configure(JobConf job) &#123;&#125; /** Use &#123;@link Object#hashCode()&#125; to partition. */ public int getPartition(K2 key, V2 value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 需要注意的是 分区数一般和reduce job个数相等 如果分区数&lt;reduce job个数,将导致输出有很多无用的空文件 如果分区数&gt;reduce job个数,将导致有些map输出找不到hash路径,出现java.io.IOException: Illegal partition for xxx的异常 Combiner 局部汇总 Combiner是hadoop对map阶段输出结果进行本地局部聚合,提高后面reduce的效率,避免大量数据进行网络传输. 需要注意的是 并非所有的任务都适用于Combiner 求和等操作,局部聚合可以有效的提高后面reduce的效率 平均值等操作,这种并不适用,因为局部平均值和全局平均值还是有差异的","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/categories/Hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://lurongjiang.github.io/tags/hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://lurongjiang.github.io/tags/MapReduce/"}]},{"title":"MapReduce教程","slug":"Hadoop-MapReduce Tutorial","date":"2018-08-17T09:03:42.000Z","updated":"2018-08-20T15:42:54.000Z","comments":true,"path":"2018/08/17/Hadoop-MapReduce Tutorial/","link":"","permalink":"https://lurongjiang.github.io/2018/08/17/Hadoop-MapReduce%20Tutorial/","excerpt":"MapReduce官网教程文档翻译","text":"MapReduce Tutorial Overview Hadoop MapReduce是一个运行在集群上,并行处理大量数据(TB级别)的框架 MapReduce任务通常讲输入切分成多个独立的块,这些数据块被独立的map任务并行的处理 该框架会对map输出进行排序,作为reduce任务的输入 该框架负责调度任务,监视任务并重新执行失败的任务 通常,计算的节点和数据存储节点是同一个节点,也就是说,MapReduce框架和HDFS都运行在同一些列节点中.这个约束使得框架在数据已经存在的节点上有效地调度任务,从而产生跨集群的非常高的聚合带宽 MapReduce框架由一个ResourceManager,集群每个节点的NodeManager和每个应用程序的MRAppMaster组成 必须指定输入输出路径,实现指定的接口或者抽象类,覆写map和reduce方法 hadoop任务客户端提交任务和相关配置到ResouceManager,ResouceManager负责把任务/配置分发到其他的从节点,并调度和监控任务,给客户端提供任务的状态和诊断信息 hadoop stream允许用户使用任何可执行的程序来作为mapper/reducer任务 hadoop pipes工具可以使用C++ API来实现mapper/reducer Inputs and Outputs MapReduce框架只针对&lt;Key,Value&gt;键值对类型操作.也就是说,每个MapReduce任务的输入是&lt;Key,Value&gt;形式,输入也是&lt;Key,Value&gt;形式,输入输出类型可不相同 Key,Value的类型必须是可以被框架序列化的类型,因此他们必须实现Writable接口. Key的类型除了实现Writable接口之外,还需要实现WritableComparable接口,这样才能被排序 (input) &lt;k1,v1&gt; -&gt; map -&gt; &lt;k2,v2&gt; -&gt; combine -&gt; &lt;k2,v2&gt; -&gt; reduce -&gt; &lt;k3,v3&gt; (output) hadoop jar的一些参数 -files 可以使用逗号分隔,指定多个文件 -libjars 可以添加jar包到map和reduce类路径下 -archives 可以使用逗号分隔传入多个压缩包路径 MapReduce - User Interfaces 实现Mapper和Reducer接口吗,并提供map/reduce的实现是任务的核心 Mapper Mapper将输入的,Key/Value键值对类型映射成中间结果的Key/Value键值对类型 Maps是独立的任务,负责将输入转成中间结果 中间结果的类型无需和输入的类型一样 一个输入可能对应0,1,或者多个输出 每个InputSplit(由InputFormat产生)都有一个map任务 可以通过Job.setMapperClass(Class) 来传入Mapper的实现.框架将对每个键值对形式的InputSplit调用map(WritableComparable, Writable, Context) 方法.如果需要清理一些必要资源,可以覆写cleanup(Context)方法 map的输出可以通过调用context.write(WritableComparable, Writable)来收集 所有的中间结果会被框架分组,然后传给Reducer.用户使用 Job.setGroupingComparatorClass(Class)指定比较器Comparator来控制分组 Mapper的输出会被排序(sort)和打散(partitioner)分发给每一个Reducer.partitioner数目和reduce任务的数量相同.用户可以实现Partitioner接口来自定义打散规则,控制不同的Key分到对应的reduce任务中 用户可以使用Job.setCombinerClass(Class)对中间输出结果进行本地聚合,这可以减少从Mapper传到Reduce的传输量 中间结果都是以简单的 (key-len, key, value-len, value) 形式存储,也可通过Configuration设置对中间结果进行压缩 How Many Maps? map任务的通常是由输入数据的大小来决定的,也就是输入文件的块数 对于cpu轻量级任务来说,每个节点map的并行度可达300,但是一般情况下并行度在10-100之间.任务的启动需要一定的时间,所以map任务至少需要1min的执行时间 Reducer Reducer将相同key的中间结果集进行处理 reduce任务的个数是通过Job.setNumReduceTasks(int)来设置的 通过 Job.setReducerClass(Class)来设置Reducer的实现类.框架对每组&lt;key, (list of values)&gt;的输入进行调用reduce(WritableComparable, Iterable, Context) 方法进行处理,需要清理资源可以覆写cleanup(Context) Shuffle 传到Reducer的输入是经过排序后的mapper的输出.shuffle阶段,框架将通过http获取相关partition的mapper输出 Sort 排序阶段,框架将Reducer的输入进行按Key进行分组 shuffle和sort同时进行.在map输出被拉取时,他们进行合并 Secondary Sort 如果中间结果key的分组规则需要和进入reducer前的keys的分组规则不一样,那么可以通过Job.setSortComparatorClass(Class)来设置比较器.因为Job.setSortComparatorClass(Class)时用来控制中间结果的keys是怎么分组的,所以可以用这个来对值进行二次排序 Reduce reduce阶段,将对每一组&lt;key, (list of values)&gt;输入调用reduce(WritableComparable, Iterable&lt;Writable&gt;, Context)方法 reduce任务通过 Context.write(WritableComparable, Writable)将输出结果写入文件系统 输出结果并不会进行排序 How Many Reduces? 比较合理的reduce任务的个数计算公式是:0.95(或1.75)×节点数(注意,不是每个节点的最大container数) 0.95系数可以使得reduce任务在map任务的输出传输结束后同时开始运行 1.75系数可以使得计算快的节点在一批reduce任务计算结束之后开始计算第二批 reduce任务,实现负载均衡 增加reduce的数量虽然会增加负载，但是可以改善负载匀衡，降低任务失败带来的负面影响 放缩系数要比整数略小是因为要给推测性任务和失败任务预留reduce位置 Reducer NONE 如果不需要reduce任务,将reduce任务个数设置为0是合法的 这种情况下,map任务的输出会直接写入文件系统的指定输出路径FileOutputFormat.setOutputPath(Job, Path).在写入文件系统前,map的输出是进行排序的 Partitioner partitioner控制中间map输出的key的分区 可以按照key(或者key的一部分)来产生分区,默认是使用hash进行分区 分区数和reduce任务的个数相等 控制发送给reduce的任务个数 Counter Counter是一个公共基础工具,用来报告MapReduce应用的统计信息 Mapper和Reducer实现类都可以使用Counter来报告统计 Job Configuration Job就是MapReduce任务的job配置代表 一般MapReduce框架会严格按照Job的配置执行,但是有几种情况例外 某些配置参数被标记为final类型,所以是修改配置是没法达到目的的,例如1.1比例 某些配置虽然可以直接配置,但是还需要配合其他的参数一起配置才能生效 Job通常会指定Mapper,combiner(有必要的话),Partitioner,Reducer,InputFormat,OutputFormat的实现类 输入可以使用下列方式指定输入数据文件集 (FileInputFormat.setInputPaths(Job, Path…)/ FileInputFormat.addInputPath(Job, Path)) (FileInputFormat.setInputPaths(Job, String…)/ FileInputFormat.addInputPaths(Job, String) 输出可以使用(FileOutputFormat.setOutputPath(Path))来指定输出文件集 其他配置都是可选的,如Caparator的使用,将文件放置到DistributeCache,是否中间结果或者最终输出结果需要压缩,是否允许推测模式,最大任务重试次数等 可以通过Configuration.set(String, String)/ Configuration.get(String)来设置和获取任意需要的参数.但是对于大的只读数据集,还是要用DistributedCache Task Execution &amp; Environment MRAppMaster在独立的JVM中执行每个Mapper/Reducer任务(任务进程级别) 子任务继承了MRAppMaster的环境. 用户可以通过 mapreduce.{map|reduce}.java.opts 给子任务添加额外的参数 运行时非标准类库路径可以通过-Djava.library.path=&lt;&gt;指定 如果mapreduce.{map|reduce}.java.opts参数配置包含了@taskid@则在运行时被替换成taskId 显示JVM GC,JVM JMX无密代理(这样可以结合jconsole,查看内存,线程,线程垃圾回收),最大堆内存,添加其他路径到任务java.library.path的例子 123456789101112131415&lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt; -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt; &lt;value&gt; -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false &lt;/value&gt;&lt;/property&gt; Memory Management 用户可以通过mapreduce.{map|reduce}.memory.mb指定子任务的最大虚拟内存.注意这个设置是进程级别的 注意这个参数不要大于-Xmx的参数,否则VM可能会无法启动 mapreduce.{map|reduce}.java.opt只能配置MRAppMaster的子任务.配置守护线程的需要参考 Configuring the Environment of the Hadoop Daemons map/reduce任务的性能,可能会被并发数,写入磁盘的频率影响.检查文件系统的统计报告,尤其是从map进入reduce的字节数,这参数是非常宝贵的. Map Parameters map输出的记录会被序列化到缓冲区,元数据存储在统计缓冲区 当缓冲区或者元数据超过一定的阈值,缓冲区的内容会被排序然后存储和写入磁盘 如果缓冲区一直是满状态的,map线程将被阻塞 map结束后,没有写入磁盘的map输出记录继续写入. 磁盘上所有的map输出文件段会合并成单个文件 减少写入磁盘的次数,可以减少map的次数,但是加大缓存区会压缩mapper的可用内存 Name Type Description mapreduce.task.io.sort.mb int 序列化和map输出到缓冲区的记录预排序的累计大小,单位为MB mapreduce.map.sort.spill.percent float 序列化缓冲区spill阈值比例,超过会将缓冲区内容写入磁盘 spill之后,如果在写入磁盘过程中,map的输出没有超过spill阈值,则会继续收集到spill结束 如果是spill设置为0.33,在spill到磁盘的过程,缓冲区继续会被map的输出填充,下一次spill的时候再将这期间填充的内容写到磁盘 如果spill设置为0.66,则不会触发下一次spill.也就是说,spill可以触发,但是不会阻塞 一条记录大于缓冲区的会先触发spill,而且会被spill到一个单独的文件.无论这条记录有没有定义combiner,它都会被combiner传输 Shuffle/Reduce Parameters reduce将partitioner通过http指派给自己的map输出加载到内存,并定期合并输出到磁盘. 如果中间结果是压缩输出,那么输出也是被reduce压缩的读进内存中,减少了内存的压力 Name Type Description mapreduce.task.io.soft.factor int 每次合并磁盘上段的数目.如果超过这个设置会分多次进行合并 mapreduce.reduce.merge.inmem.thresholds int 在合并写入磁盘之前,将排序后的map输出加载到内存的map输出数目.这个值通常设置很大(1000)或者直接禁用(0),因为内存合并要比磁盘合并的代价小得多.这个阈值只影响shuffle期间内存中合并的频率 mapreduce.reduce.shuffle.merge.percent float 在内存合并之前,读取map输出的内存阈值,代表着用于存储map输出在内存中的百分比.因为map的输出并不适合存储在内存,所以设置很高会知道使得获取和合并的并行度下降.相反,设置为1可以使得内存运行的reduce更快.这个参数只影响shuffle期间的内存内合并频率 mapreduce.reduce.shuffle.input.buffer.percent float 在shuffle期间,可以分配来存储map输出的内存百分比,相对于mapreduce.reduce.java.opts指定的最大堆内存.把这个值设的大一点可以存储更多的map输出,但是也应该为框架预留一些内存 mapreduce.reduce.input.buffer.percent float 相当于reduce阶段,用于存储map输出的最大堆内存的内存百分比.reduce开始的时候,map的输出被合并到磁盘,知道map输出在一定的阈值之内.默认情况下,在reduce开始之前,map的输出都会被合并到磁盘,这样才能使得reduce充分的利用到内存.对于只要内存密集型的reduce任务,应该增加这个值,减少磁盘的的往返时间 Configured Parameters 这些参数都是局部的,每个任务的 Name Type Description mapreduce.job.id String The job id mapreduce.job.jar String job.jar location in job directory mapreduce.job.local.dir String The job specific shared scratch space mapreduce.task.id String The task id mapreduce.task.attempt.id String The task attempt id mapreduce.task.is.map boolean Is this a map task mapreduce.task.partition int The id of the task within the job mapreduce.map.input.file String The filename that the map is reading from mapreduce.map.input.start long The offset of the start of the map input split mapreduce.map.input.length long The number of bytes in the map input split mapreduce.task.output.dir String The task’s temporary output directory 在流任务执行过程中,这些参数会被转化.点(.)会被转成下划线(_),所以要想在流任务的mapper/reducer中获得这些值,需要使用下划线形式. Distributing Libraries DistributedCache分布式缓存可以分发jars和本地类库给map/reduce任务使用. child-jvm总将自己的工作目录添加到java.library.path和LD_LIBRARY_PATH 缓存中的类库可以通过System.loadLibrary或者System.load Job Submission and Monitoring Job是用户任务和ResourceManager交互的主要接口 Job的提交流程包括 检查输入输出路径 计算任务的InputSplit 有必要的话,设置必要的分布式缓存 拷贝任务的jar和配置到MapReduce系统目录 提交任务到ResourceManager.监控任务状态是可选的 任务的执行记录历史存放在 mapreduce.jobhistory.intermediate-done-dir 和mapreduce.jobhistory.done-dir Job Control 对于单个MapReduce任务无法完成的任务,用户可能需要执行MapReduce任务链,才能完成.这还是非常容易的,因为任务的输出一般是存储在分布式文件系统中,所以一个任务的输出可以作为另一个任务的输入.这也就使得判断任务是否完成,不管成功或者失败,都需要用户来控制.主要有两种控制手段 Job.submit() 提交任务到集群中,立即返回 Job.waitForCompletion(boolean) 提交任务到集群中,等待其完成 Job Input InputFormat描述了MapReduce任务的输入规范 InputFormat的职责是: 校验输入是否合法 将输入逻辑切分成InputSplit实例,之后将它们发送到独立的Mapper RecordReader 实现了从符合框架逻辑的InputSplit实例收集输入的记录,提供给Mapper进行处理 默认的InputFormat是基于输入文件的总字节大小,将输入文件切分成逻辑的InputSplit实例,例如FileInputFormat的子类.然而,文件系统的blocksize只是split的上限,下限需要通过mapreduce.input.fileinputformat.split.minsize来设置 压缩文件并不一定可以被切分,如.gz文件会把完整的文件交给一个mapper来处理 InputSplit InputSplit代表了一个独立Mapper处理的输入数据 通常InputSplit是面向字节的,把面向字节转为面向记录是RecordReader的职责 FileSplit是默认的InputSplit实现,它把输入设置成mapreduce.map.input.file 属性,用于进行逻辑分割 RecordReader RecordReader负责将InputSplit的面向字节的输入转换成面向记录,提供给Mapper实现去处理每一条记录.因此RecordReader承担了从记录中提取出键值对的任务 Job Output OutputFormat描述了MapReduce输出的规范 OutputFormat的职责: 校验任务的输出,例如输出目录是否存在 RecordWriter实现可以将任务的输出写入到文件,存储在文件系统中 TextOutputFormat是默认的OutputFormat实现","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://lurongjiang.github.io/categories/Hadoop/"},{"name":"MapReduce","slug":"Hadoop/MapReduce","permalink":"https://lurongjiang.github.io/categories/Hadoop/MapReduce/"}],"tags":[{"name":"Hadoop,MapReduce","slug":"Hadoop-MapReduce","permalink":"https://lurongjiang.github.io/tags/Hadoop-MapReduce/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-07-18T01:35:45.000Z","updated":"2018-07-19T15:45:14.000Z","comments":false,"path":"2018/07/18/hello-world/","link":"","permalink":"https://lurongjiang.github.io/2018/07/18/hello-world/","excerpt":"这是一段文章摘要，是通过 Front-Matter 的 excerpt 属性设置的。","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[{"name":"TestNest","slug":"TestNest","permalink":"https://lurongjiang.github.io/categories/TestNest/"},{"name":"test1","slug":"test1","permalink":"https://lurongjiang.github.io/categories/test1/"},{"name":"nest1","slug":"TestNest/nest1","permalink":"https://lurongjiang.github.io/categories/TestNest/nest1/"},{"name":"nest2","slug":"TestNest/nest2","permalink":"https://lurongjiang.github.io/categories/TestNest/nest2/"}],"tags":[{"name":"PlayStation","slug":"PlayStation","permalink":"https://lurongjiang.github.io/tags/PlayStation/"},{"name":"Games","slug":"Games","permalink":"https://lurongjiang.github.io/tags/Games/"}]}]}