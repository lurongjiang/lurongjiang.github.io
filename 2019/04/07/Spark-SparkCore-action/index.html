<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="keywords" content="LRJ, Blog"><meta name="description" content="个人博客,记录成长历程"><title>SparkCore基础-Action</title><link rel="icon" href="/images/icons/favicon-16x16.png?v=1.7.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=1.7.0" type="image/png" sizes="32x32"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=1.7.0"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  fontawesome: {"prefix":"fa"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  post_widget: {"end_text":true},
  night_mode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"ocean","word_wrap":false},
  reward: false,
  fancybox: false,
  zoom_image: {"enable":true,"mask_color":"rgba(0,0,0,0.6)"},
  gallery_waterfall: undefined,
  lazyload: undefined,
  pjax: undefined,
  external_link: {"icon":{"enable":true,"name":"external-link"}},
  shortcuts: undefined,
  prompt: {"copy_success":"Copy Success","copy_error":"Copy Error","creative_commons":"Creative Commons","copy_button":"Copy"}
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
</head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-btn fa fa-bars"></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/"><i class="fa fa-home"></i>Home</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/archives/"><i class="fa fa-folder-open"></i>Archives</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/categories/"><i class="fa fa-th"></i>Categories</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/tags/"><i class="fa fa-tags"></i>Tags</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/about/"><i class="fa fa-user"></i>About</a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-info"><div class="header-info-inner"><div class="header-info-title">Blog</div><div class="header-info-subtitle">个人博客,记录成长历程</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content"><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-header-title">SparkCore基础-Action</h1><div class="post-header-meta"><span class="post-header-meta-create"><i class="fa fa-calendar-o"></i><span>Posted </span><span>2019-04-07</span></span><span class="post-header-meta-update"><i class="fa fa-calendar-check-o"></i><span>updated </span><span>2019-04-09</span></span></div></header><div class="post-body">
        <h2   id="Spark-Core-Action" >
          <span class="heading-link">Spark Core-Action</span>
        </h2>
      
        <h2   id="Action" >
          <span class="heading-link">Action</span>
        </h2>
      
        <h3   id="take" >
          <span class="heading-link">take</span>
        </h3>
      <p>take使用需要慎重,避免冲爆Driver端内存</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Take the first num elements of the RDD. It works by first scanning one partition, and use the</span></span><br><span class="line"><span class="comment"> * results from that partition to estimate the number of additional partitions needed to satisfy</span></span><br><span class="line"><span class="comment"> * the limit.</span></span><br><span class="line"><span class="comment"> * 先扫描一个分区,再估算还需要扫描多少个其他分区才能满足</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note This method should only be used if the resulting array is expected to be small, as</span></span><br><span class="line"><span class="comment"> * all the data is loaded into the driver's memory.</span></span><br><span class="line"><span class="comment"> * @note Due to complications in the internal implementation, this method will raise</span></span><br><span class="line"><span class="comment"> * an exception if called on an RDD of `Nothing` or `Null`.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> scaleUpFactor = <span class="type">Math</span>.max(conf.getInt(<span class="string">"spark.rdd.limit.scaleUpFactor"</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">if</span> (num == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](<span class="number">0</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> buf = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">T</span>]</span><br><span class="line">    <span class="keyword">val</span> totalParts = <span class="keyword">this</span>.partitions.length</span><br><span class="line">    <span class="keyword">var</span> partsScanned = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (buf.size &lt; num &amp;&amp; partsScanned &lt; totalParts) &#123;</span><br><span class="line">      <span class="comment">// The number of partitions to try in this iteration. It is ok for this number to be</span></span><br><span class="line">      <span class="comment">// greater than totalParts because we actually cap it at totalParts in runJob.</span></span><br><span class="line">      <span class="keyword">var</span> numPartsToTry = <span class="number">1</span>L</span><br><span class="line">      <span class="comment">//还剩多少没取</span></span><br><span class="line">      <span class="keyword">val</span> left = num - buf.size</span><br><span class="line">      <span class="comment">//如果不是第一次扫描</span></span><br><span class="line">      <span class="keyword">if</span> (partsScanned &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// If we didn't find any rows after the previous iteration, quadruple and retry.</span></span><br><span class="line">        <span class="comment">// Otherwise, interpolate the number of partitions we need to try, but overestimate</span></span><br><span class="line">        <span class="comment">// it by 50%. We also cap the estimation in the end.</span></span><br><span class="line">        <span class="keyword">if</span> (buf.isEmpty) &#123;</span><br><span class="line">          <span class="comment">//之前扫描的结果集是空的,说明之前都没扫到,扫描范围扩充为原来的N倍</span></span><br><span class="line">          numPartsToTry = partsScanned * scaleUpFactor</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// As left &gt; 0, numPartsToTry is always &gt;= 1</span></span><br><span class="line">          <span class="comment">//如果之前扫描到了,但数量还不够,采用插值法,增加50%的扫描范围</span></span><br><span class="line">          numPartsToTry = <span class="type">Math</span>.ceil(<span class="number">1.5</span> * left * partsScanned / buf.size).toInt</span><br><span class="line">          numPartsToTry = <span class="type">Math</span>.min(numPartsToTry, partsScanned * scaleUpFactor)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//计算要扫描第几个分区</span></span><br><span class="line">      <span class="keyword">val</span> p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt)</span><br><span class="line">      <span class="keyword">val</span> res = sc.runJob(<span class="keyword">this</span>, (it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; it.take(left).toArray, p)</span><br><span class="line"></span><br><span class="line">      res.foreach(buf ++= _.take(num - buf.size))</span><br><span class="line">      partsScanned += p.size</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    buf.toArray</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h3   id="count" >
          <span class="heading-link">count</span>
        </h3>
      <figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//返回RDD中的元素</span></span><br><span class="line"><span class="comment">//Utils.getIteratorSize就是把集合的迭代器数一遍</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getIteratorSize</span></span>(iterator: <span class="type">Iterator</span>[_]): <span class="type">Long</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> count = <span class="number">0</span>L</span><br><span class="line">    <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">        count += <span class="number">1</span>L</span><br><span class="line">        iterator.next()</span><br><span class="line">    &#125;</span><br><span class="line">    count</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h3   id="top和takeOrdered" >
          <span class="heading-link">top和takeOrdered</span>
        </h3>
      <p>取topN个元素,这个是全局的,所以注意只能在数据量小的时候使用,否则可能会冲爆Driver端内存</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver's memory.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1)</span></span><br><span class="line"><span class="comment">// returns Array(12)</span></span><br><span class="line"><span class="comment">sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2)</span></span><br><span class="line"><span class="comment">// returns Array(6, 5)</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="comment">//使用柯里化的方式,第一个是要取多少个,第二按降序排列</span></span><br><span class="line">    takeOrdered(num)(ord.reverse)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*   sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1)</span></span><br><span class="line"><span class="comment">*   // returns Array(2)</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">*   sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2)</span></span><br><span class="line"><span class="comment">*   // returns Array(2, 3)</span></span><br><span class="line"><span class="comment">* &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* @note This method should only be used if the resulting array is expected to be small</span></span><br><span class="line"><span class="comment">* as all the data is loaded into the driver's memory.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="comment">//取0个,直接返回空</span></span><br><span class="line">    <span class="keyword">if</span> (num == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="type">Array</span>.empty</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">//每个partition取topN</span></span><br><span class="line">        <span class="keyword">val</span> mapRDDs = mapPartitions &#123; items =&gt; </span><br><span class="line">            <span class="comment">// Priority keeps the largest elements, so let's reverse the ordering.</span></span><br><span class="line">            <span class="comment">//构建有界优先队列(N个长度)</span></span><br><span class="line">            <span class="keyword">val</span> queue = <span class="keyword">new</span> <span class="type">BoundedPriorityQueue</span>[<span class="type">T</span>](num)(ord.reverse)</span><br><span class="line">            <span class="comment">//将数据加进去</span></span><br><span class="line">            queue ++= collectionUtils.takeOrdered(items, num)(ord)</span><br><span class="line">            <span class="type">Iterator</span>.single(queue)</span><br><span class="line">                                    &#125;</span><br><span class="line">        <span class="keyword">if</span> (mapRDDs.partitions.length == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="type">Array</span>.empty</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//归并一下,因为是有界优先队列,后面大的数据会替换掉小的</span></span><br><span class="line">            mapRDDs.reduce &#123; (queue1, queue2) =&gt;</span><br><span class="line">                queue1 ++= queue2</span><br><span class="line">                queue1</span><br><span class="line">                           &#125;.toArray.sorted(ord)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>可以看出top方法的takeOrdered传入的是一个隐式转换:降序,所以如果需要升序,可以直接takeOrdered</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ActionTes</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>)</span><br><span class="line">    conf.setAppName(<span class="string">"app"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">val</span> value = sc.parallelize(list)</span><br><span class="line">    <span class="comment">//升序</span></span><br><span class="line">    value.takeOrdered(<span class="number">2</span>)</span><br><span class="line">    <span class="comment">//降序</span></span><br><span class="line">    value.takeOrdered(<span class="number">2</span>)(<span class="type">Ordering</span>.by(-_))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h3   id="reduce" >
          <span class="heading-link">reduce</span>
        </h3>
      <p>两两合并</p>

        <h3   id="countByKey" >
          <span class="heading-link">countByKey</span>
        </h3>
      <p>针对PairRDDFunction</p>
<p>计算key出现的次数</p>
<p>使用场景:数据倾斜key检查</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByKey</span></span>(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Long</span>] = self.withScope &#123;</span><br><span class="line">  self.mapValues(_ =&gt; <span class="number">1</span>L).reduceByKey(_ + _).collect().toMap</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h3   id="foreach" >
          <span class="heading-link">foreach</span>
        </h3>
      <p>对集合中的每一个元素作用一个function</p>

        <h3   id="foreachPartition" >
          <span class="heading-link">foreachPartition</span>
        </h3>
      <p>针对每个分区作用一个函数,也就是说一批一批的处理,一般效率会比foreach效率要高</p>
<p>值得注意的是,foreachPartition本身是没有返回值的,但是使用了sc.runJob,这个是有返回值的</p>
<p>如果分区的内容很大,可能会出现OOM</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Applies a function f to each partition of this RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanF(iter))</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Run a job on all partitions in an RDD and return the results in an array.</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">* @param rdd target RDD to run tasks on</span></span><br><span class="line"><span class="comment">* @param func a function to run on each partition of the RDD</span></span><br><span class="line"><span class="comment">* @return in-memory collection with a result of the job </span></span><br><span class="line"><span class="comment">* (each collection element will contain a result from one partition)</span></span><br><span class="line"><span class="comment">* 返回一个任务结果的集合到内存中</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">runJob(rdd, func, <span class="number">0</span> until rdd.partitions.length)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h2   id="DataSource" >
          <span class="heading-link">DataSource</span>
        </h2>
      
        <h3   id="saveTextFile" >
          <span class="heading-link">saveTextFile</span>
        </h3>
      <p>将rdd作为文本按分区写出去</p>
<p>先检查输出路径,存在报错</p>
<p>使用hadoop的output检查</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">	<span class="comment">// https://issues.apache.org/jira/browse/SPARK-2075</span></span><br><span class="line">	<span class="comment">//</span></span><br><span class="line">	<span class="comment">// NullWritable is a `Comparable` in Hadoop 1.+, so the compiler cannot find an implicit</span></span><br><span class="line">	<span class="comment">// Ordering for it and will use the default `null`. However, it's a `Comparable[NullWritable]`</span></span><br><span class="line">	<span class="comment">// in Hadoop 2.+, so the compiler will call the implicit `Ordering.ordered` method to create an</span></span><br><span class="line">	<span class="comment">// Ordering for `NullWritable`. That's why the compiler will generate different anonymous</span></span><br><span class="line">	<span class="comment">// classes for `saveAsTextFile` in Hadoop 1.+ and Hadoop 2.+.</span></span><br><span class="line">	<span class="comment">//</span></span><br><span class="line">	<span class="comment">// Therefore, here we provide an explicit Ordering `null` to make sure the compiler generate</span></span><br><span class="line">	<span class="comment">// same bytecodes for `saveAsTextFile`.</span></span><br><span class="line">	<span class="comment">// 将输出以&lt;NullWritable,Text&gt;的形式写出去</span></span><br><span class="line">	<span class="keyword">val</span> nullWritableClassTag = implicitly[<span class="type">ClassTag</span>[<span class="type">NullWritable</span>]]</span><br><span class="line">	<span class="keyword">val</span> textClassTag = implicitly[<span class="type">ClassTag</span>[<span class="type">Text</span>]]</span><br><span class="line">	<span class="keyword">val</span> r = <span class="keyword">this</span>.mapPartitions &#123; iter =&gt;</span><br><span class="line">	  <span class="keyword">val</span> text = <span class="keyword">new</span> <span class="type">Text</span>()</span><br><span class="line">	  iter.map &#123; x =&gt;</span><br><span class="line">	    text.set(x.toString)</span><br><span class="line">	    (<span class="type">NullWritable</span>.get(), text)</span><br><span class="line">	  &#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//RDD.rddToPairRDDFunctions生成PairRDDFunctions,</span></span><br><span class="line">	<span class="comment">//采用hdfs默认的FileOutputFormat实现TextOutputFormat</span></span><br><span class="line">	<span class="type">RDD</span>.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, <span class="literal">null</span>).saveAsHadoopFile[<span class="type">TextOutputFormat</span>[<span class="type">NullWritable</span>, <span class="type">Text</span>]](path)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//底层抵用hadoop的读写进行保存文件</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class</span></span><br><span class="line"><span class="comment">* supporting the key and value types K and V in this RDD.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsHadoopFile</span></span>[<span class="type">F</span> &lt;: <span class="type">OutputFormat</span>[<span class="type">K</span>, <span class="type">V</span>]](</span><br><span class="line">                                             path: <span class="type">String</span>)(<span class="keyword">implicit</span> fm: <span class="type">ClassTag</span>[<span class="type">F</span>]): <span class="type">Unit</span> = self.withScope &#123;</span><br><span class="line">	saveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[<span class="type">Class</span>[<span class="type">F</span>]])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsHadoopFile</span></span>(</span><br><span class="line">      path: <span class="type">String</span>,</span><br><span class="line">      keyClass: <span class="type">Class</span>[_],</span><br><span class="line">      valueClass: <span class="type">Class</span>[_],</span><br><span class="line">      outputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">OutputFormat</span>[_, _]],</span><br><span class="line">      conf: <span class="type">JobConf</span> = <span class="keyword">new</span> <span class="type">JobConf</span>(self.context.hadoopConfiguration),</span><br><span class="line">      codec: <span class="type">Option</span>[<span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>]] = <span class="type">None</span>): <span class="type">Unit</span> = self.withScope &#123;</span><br><span class="line">    <span class="comment">// Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038).</span></span><br><span class="line">    <span class="keyword">val</span> hadoopConf = conf</span><br><span class="line">    <span class="comment">//设置hdfs的输出K,V类型</span></span><br><span class="line">    hadoopConf.setOutputKeyClass(keyClass)</span><br><span class="line">    hadoopConf.setOutputValueClass(valueClass)</span><br><span class="line">    <span class="comment">//其实就是TextOutputFormat</span></span><br><span class="line">    conf.setOutputFormat(outputFormatClass)</span><br><span class="line">    <span class="keyword">for</span> (c &lt;- codec) &#123;</span><br><span class="line">      <span class="comment">//如果设置了压缩</span></span><br><span class="line">      hadoopConf.setCompressMapOutput(<span class="literal">true</span>)</span><br><span class="line">      hadoopConf.set(<span class="string">"mapreduce.output.fileoutputformat.compress"</span>, <span class="string">"true"</span>)</span><br><span class="line">      hadoopConf.setMapOutputCompressorClass(c)</span><br><span class="line">      hadoopConf.set(<span class="string">"mapreduce.output.fileoutputformat.compress.codec"</span>, c.getCanonicalName)</span><br><span class="line">      hadoopConf.set(<span class="string">"mapreduce.output.fileoutputformat.compress.type"</span>,</span><br><span class="line">        <span class="type">CompressionType</span>.<span class="type">BLOCK</span>.toString)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Use configured output committer if already set</span></span><br><span class="line">    <span class="keyword">if</span> (conf.getOutputCommitter == <span class="literal">null</span>) &#123;</span><br><span class="line">      hadoopConf.setOutputCommitter(classOf[<span class="type">FileOutputCommitter</span>])</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// When speculation is on and output committer class name contains "Direct", we should warn</span></span><br><span class="line">    <span class="comment">// users that they may loss data if they are using a direct output committer.</span></span><br><span class="line">    <span class="keyword">val</span> speculationEnabled = self.conf.getBoolean(<span class="string">"spark.speculation"</span>, <span class="literal">false</span>)</span><br><span class="line">    <span class="keyword">val</span> outputCommitterClass = hadoopConf.get(<span class="string">"mapred.output.committer.class"</span>, <span class="string">""</span>)</span><br><span class="line">    <span class="keyword">if</span> (speculationEnabled &amp;&amp; outputCommitterClass.contains(<span class="string">"Direct"</span>)) &#123;</span><br><span class="line">      <span class="keyword">val</span> warningMessage =</span><br><span class="line">        <span class="string">s"<span class="subst">$outputCommitterClass</span> may be an output committer that writes data directly to "</span> +</span><br><span class="line">          <span class="string">"the final location. Because speculation is enabled, this output committer may "</span> +</span><br><span class="line">          <span class="string">"cause data loss (see the case in SPARK-10063). If possible, please use an output "</span> +</span><br><span class="line">          <span class="string">"committer that does not have this behavior (e.g. FileOutputCommitter)."</span></span><br><span class="line">      logWarning(warningMessage)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//设置输出目录</span></span><br><span class="line">    <span class="type">FileOutputFormat</span>.setOutputPath(hadoopConf,</span><br><span class="line">      <span class="type">SparkHadoopWriterUtils</span>.createPathFromString(path, hadoopConf))</span><br><span class="line">    saveAsHadoopDataset(hadoopConf)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Output the RDD to any Hadoop-supported storage system, using a Hadoop JobConf object for</span></span><br><span class="line"><span class="comment">* that storage system. The JobConf should set an OutputFormat and any output paths required</span></span><br><span class="line"><span class="comment">* (e.g. a table name to write to) in the same way as it would be configured for a Hadoop</span></span><br><span class="line"><span class="comment">* MapReduce job.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsHadoopDataset</span></span>(conf: <span class="type">JobConf</span>): <span class="type">Unit</span> = self.withScope &#123;</span><br><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">HadoopMapRedWriteConfigUtil</span>[<span class="type">K</span>, <span class="type">V</span>](<span class="keyword">new</span> <span class="type">SerializableJobConf</span>(conf))</span><br><span class="line">	<span class="type">SparkHadoopWriter</span>.write(</span><br><span class="line">	  rdd = self,</span><br><span class="line">	  config = config)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Basic work flow of this command is:</span></span><br><span class="line"><span class="comment">* 1. Driver side setup, prepare the data source and hadoop configuration for the write job to</span></span><br><span class="line"><span class="comment">*    be issued.</span></span><br><span class="line"><span class="comment">* 2. Issues a write job consists of one or more executor side tasks, each of which writes all</span></span><br><span class="line"><span class="comment">*    rows within an RDD partition.</span></span><br><span class="line"><span class="comment">* 3. If no exception is thrown in a task, commits that task, otherwise aborts that task;  If any</span></span><br><span class="line"><span class="comment">*    exception is thrown during task commitment, also aborts that task.</span></span><br><span class="line"><span class="comment">* 4. If all tasks are committed, commit the job, otherwise aborts the job;  If any exception is</span></span><br><span class="line"><span class="comment">*    thrown during job commitment, also aborts the job.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span></span>[<span class="type">K</span>, <span class="type">V</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)],</span><br><span class="line">      config: <span class="type">HadoopWriteConfigUtil</span>[<span class="type">K</span>, <span class="type">V</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Extract context and configuration from RDD.</span></span><br><span class="line">    <span class="keyword">val</span> sparkContext = rdd.context</span><br><span class="line">    <span class="keyword">val</span> commitJobId = rdd.id</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取jobId</span></span><br><span class="line">    <span class="comment">// Set up a job.</span></span><br><span class="line">    <span class="keyword">val</span> jobTrackerId = createJobTrackerID(<span class="keyword">new</span> <span class="type">Date</span>())</span><br><span class="line">    <span class="comment">//创建上下文</span></span><br><span class="line">    <span class="keyword">val</span> jobContext = config.createJobContext(jobTrackerId, commitJobId)</span><br><span class="line">    <span class="comment">//设置Output Class</span></span><br><span class="line">    config.initOutputFormat(jobContext)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//判断K,V类型,输出目录是否存在</span></span><br><span class="line">    <span class="comment">// Assert the output format/key/value class is set in JobConf.</span></span><br><span class="line">    config.assertConf(jobContext, rdd.conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> committer = config.createCommitter(commitJobId)</span><br><span class="line">    committer.setupJob(jobContext)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Try to write all RDD partitions as a Hadoop OutputFormat.</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> ret = sparkContext.runJob(rdd, (context: <span class="type">TaskContext</span>, iter: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)]) =&gt; &#123;</span><br><span class="line">        <span class="comment">// SPARK-24552: Generate a unique "attempt ID" based on the stage and task attempt numbers.</span></span><br><span class="line">        <span class="comment">// Assumes that there won't be more than Short.MaxValue attempts, at least not concurrently.</span></span><br><span class="line">        <span class="keyword">val</span> attemptId = (context.stageAttemptNumber &lt;&lt; <span class="number">16</span>) | context.attemptNumber</span><br><span class="line"></span><br><span class="line">        executeTask(</span><br><span class="line">          context = context,</span><br><span class="line">          config = config,</span><br><span class="line">          jobTrackerId = jobTrackerId,</span><br><span class="line">          commitJobId = commitJobId,</span><br><span class="line">          sparkPartitionId = context.partitionId,</span><br><span class="line">          sparkAttemptNumber = attemptId,</span><br><span class="line">          committer = committer,</span><br><span class="line">          iterator = iter)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">      committer.commitJob(jobContext, ret)</span><br><span class="line">      logInfo(<span class="string">s"Job <span class="subst">$&#123;jobContext.getJobID&#125;</span> committed."</span>)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> cause: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        logError(<span class="string">s"Aborting job <span class="subst">$&#123;jobContext.getJobID&#125;</span>."</span>, cause)</span><br><span class="line">        committer.abortJob(jobContext)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Job aborted."</span>, cause)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//检查输出</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">assertConf</span></span>(jobContext: <span class="type">NewJobContext</span>, conf: <span class="type">SparkConf</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> outputFormatInstance = getOutputFormat()</span><br><span class="line">    <span class="keyword">val</span> keyClass = getConf.getOutputKeyClass</span><br><span class="line">    <span class="keyword">val</span> valueClass = getConf.getOutputValueClass</span><br><span class="line">    <span class="comment">//这个基本不可能,他自己带过来的TextOutputFormat</span></span><br><span class="line">    <span class="keyword">if</span> (outputFormatInstance == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Output format class not set"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//这个就是NullWritable</span></span><br><span class="line">    <span class="keyword">if</span> (keyClass == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Output key class not set"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//这个是Text</span></span><br><span class="line">    <span class="keyword">if</span> (valueClass == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Output value class not set"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">SparkHadoopUtil</span>.get.addCredentials(getConf)</span><br><span class="line"></span><br><span class="line">    logDebug(<span class="string">"Saving as hadoop file of type ("</span> + keyClass.getSimpleName + <span class="string">", "</span> +</span><br><span class="line">      valueClass.getSimpleName + <span class="string">")"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="type">SparkHadoopWriterUtils</span>.isOutputSpecValidationEnabled(conf)) &#123;</span><br><span class="line">      <span class="comment">// FileOutputFormat ignores the filesystem parameter</span></span><br><span class="line">      <span class="keyword">val</span> ignoredFs = <span class="type">FileSystem</span>.get(getConf)</span><br><span class="line">        <span class="comment">//检查输出路径是否存在文件,存在就报错</span></span><br><span class="line">      getOutputFormat().checkOutputSpecs(ignoredFs, getConf)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void checkOutputSpecs(<span class="type">FileSystem</span> ignored, <span class="type">JobConf</span> job) <span class="keyword">throws</span> <span class="type">FileAlreadyExistsException</span>, <span class="type">InvalidJobConfException</span>, <span class="type">IOException</span> &#123;</span><br><span class="line">    <span class="type">Path</span> outDir = getOutputPath(job);</span><br><span class="line">    <span class="keyword">if</span> (outDir == <span class="literal">null</span> &amp;&amp; job.getNumReduceTasks() != <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidJobConfException</span>(<span class="string">"Output directory not set in JobConf."</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (outDir != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="type">FileSystem</span> fs = outDir.getFileSystem(job);</span><br><span class="line">            outDir = fs.makeQualified(outDir);</span><br><span class="line">            setOutputPath(job, outDir);</span><br><span class="line">            <span class="type">TokenCache</span>.obtainTokensForNamenodes(job.getCredentials(), <span class="keyword">new</span> <span class="type">Path</span>[]&#123;outDir&#125;, job);</span><br><span class="line">            <span class="keyword">if</span> (fs.exists(outDir)) &#123;</span><br><span class="line">                <span class="comment">//如果目录存在就报错</span></span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">FileAlreadyExistsException</span>(<span class="string">"Output directory "</span> + outDir + <span class="string">" already exists"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>




        <h3   id="saveAsTextFile" >
          <span class="heading-link">saveAsTextFile</span>
        </h3>
      <p>以压缩的形式写出去</p>

        <h3   id="saveAsObjectFile" >
          <span class="heading-link">saveAsObjectFile</span>
        </h3>
      <p>保存对象,这个是需要序列化的,可以使用自带序列化的case class</p>

        <h3   id="objectFile" >
          <span class="heading-link">objectFile</span>
        </h3>
      <p>读取对象文件,反序列化</p>
<p>Application=1Driver+n Executor</p>
<p>Driver =&gt; main中创建sc</p>
<p>Task=&gt;最小执行单位,map,filter,…</p>
<p>WorkNode=&gt;NM</p>
<p>Job=&gt;一个action就是一个job,action in stage</p>
<p>Stage=&gt;遇到shuffle就是一个Stage</p>
</div><footer class="post-footer"><div class="post-end"><p><span>------ </span><span>End of the article, thanks for your reading</span><span> ------</span></p></div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-author-name">Author: </span><span class="post-copyright-author-value"><a href="https://lurongjiang.github.io">LRJ</a></span></div><div class="post-copyright-link"><span class="post-copyright-link-name">Link: </span><span class="post-copyright-link-value"><a href="https://lurongjiang.github.io/2019/04/07/Spark-SparkCore-action/">https://lurongjiang.github.io/2019/04/07/Spark-SparkCore-action/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-notice-name">Copyright: </span><span class="post-copyright-notice-value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><i class="post-tags-item__i fa fa-tags"></i><a class="post-tags-item__a" href="https://lurongjiang.github.io/tags/Spark/">Spark</a></span><span class="post-tags-item"><i class="post-tags-item__i fa fa-tags"></i><a class="post-tags-item__a" href="https://lurongjiang.github.io/tags/SparkCore/">SparkCore</a></span></div><nav class="paginator"><div class="paginator-post"><div class="paginator-post-prev"><a href="/2019/04/08/Spark-SparkCore-2/"><i class="fa fa-chevron-left"></i><span>SparkCore基础-Action-2</span></a></div><div class="paginator-post-next"><a href="/2019/04/06/Flume-%E7%A8%8D%E5%A4%8D%E6%9D%82%E7%9A%84%E4%B8%80%E4%BA%9B%E7%8E%A9%E6%84%8F/"><span>Flume稍复杂的一些玩意</span><i class="fa fa-chevron-right"></i></a></div></div></nav></footer></div></div></div><aside class="sidebar" id="sidebar"><div class="sidebar-inner"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Core-Action"><span class="toc-number">1.</span> <span class="toc-text">
          Spark Core-Action
        </span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Action"><span class="toc-number">2.</span> <span class="toc-text">
          Action
        </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#take"><span class="toc-number">2.1.</span> <span class="toc-text">
          take
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#count"><span class="toc-number">2.2.</span> <span class="toc-text">
          count
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#top和takeOrdered"><span class="toc-number">2.3.</span> <span class="toc-text">
          top和takeOrdered
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduce"><span class="toc-number">2.4.</span> <span class="toc-text">
          reduce
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#countByKey"><span class="toc-number">2.5.</span> <span class="toc-text">
          countByKey
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#foreach"><span class="toc-number">2.6.</span> <span class="toc-text">
          foreach
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#foreachPartition"><span class="toc-number">2.7.</span> <span class="toc-text">
          foreachPartition
        </span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSource"><span class="toc-number">3.</span> <span class="toc-text">
          DataSource
        </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#saveTextFile"><span class="toc-number">3.1.</span> <span class="toc-text">
          saveTextFile
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsTextFile"><span class="toc-number">3.2.</span> <span class="toc-text">
          saveAsTextFile
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsObjectFile"><span class="toc-number">3.3.</span> <span class="toc-text">
          saveAsObjectFile
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#objectFile"><span class="toc-number">3.4.</span> <span class="toc-text">
          objectFile
        </span></a></li></ol></li></ol></div></section><!-- ov = overview --><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/avatar2.png" alt="avatar"></div><p class="sidebar-ov-author__p">LRJ</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state__a posts" href="/archives/"><div class="sidebar-ov-state__a--count">40</div><div class="sidebar-ov-state__a--name">Archives</div></a><a class="sidebar-ov-state__a categories" href="/categories/"><div class="sidebar-ov-state__a--count">17</div><div class="sidebar-ov-state__a--name">Categories</div></a><a class="sidebar-ov-state__a tags" href="/tags/"><div class="sidebar-ov-state__a--count">28</div><div class="sidebar-ov-state__a--name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span>You have read </span><span class="sidebar-reading-info-num">0</span></div><div class="sidebar-reading-line"></div></div></div></aside><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright &copy; 2020</span><span class="fa fa-heart footer-icon"></span><span>LRJ.</span></div><div><span>Powered by <a href="http://hexo.io/" title="hexo" target="_blank" rel="noopener">hexo</a></span><span> v4.2.0.</span><span class="separator">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="stun" target="_blank" rel="noopener">stun</a></span><span> v1.7.0.</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="progress"></div></div><div class="back2top" id="back2top"><i class="back2top-icon fa fa-rocket"></i></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=1.7.0"></script><script src="/js/stun-boot.js?v=1.7.0"></script><script src="/js/scroll.js?v=1.7.0"></script><script src="/js/header.js?v=1.7.0"></script><script src="/js/sidebar.js?v=1.7.0"></script></body></html>