<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="keywords" content="LRJ, Blog"><meta name="description" content="个人博客,记录成长历程"><title>MapReduce教程</title><link rel="icon" href="/images/icons/favicon-16x16.png?v=1.7.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=1.7.0" type="image/png" sizes="32x32"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=1.7.0"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  fontawesome: {"prefix":"fa"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  post_widget: {"end_text":true},
  night_mode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"ocean","word_wrap":false},
  reward: false,
  fancybox: false,
  zoom_image: {"enable":true,"mask_color":"rgba(0,0,0,0.6)"},
  gallery_waterfall: undefined,
  lazyload: undefined,
  pjax: undefined,
  external_link: {"icon":{"enable":true,"name":"external-link"}},
  shortcuts: undefined,
  prompt: {"copy_success":"Copy Success","copy_error":"Copy Error","creative_commons":"Creative Commons","copy_button":"Copy"}
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
</head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-btn fa fa-bars"></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/"><i class="fa fa-home"></i>Home</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/archives/"><i class="fa fa-folder-open"></i>Archives</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/categories/"><i class="fa fa-th"></i>Categories</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/tags/"><i class="fa fa-tags"></i>Tags</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/about/"><i class="fa fa-user"></i>About</a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-info"><div class="header-info-inner"><div class="header-info-title">Blog</div><div class="header-info-subtitle">个人博客,记录成长历程</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content"><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-header-title">MapReduce教程</h1><div class="post-header-meta"><span class="post-header-meta-create"><i class="fa fa-calendar-o"></i><span>Posted </span><span>2019-04-01</span></span><span class="post-header-meta-update"><i class="fa fa-calendar-check-o"></i><span>updated </span><span>2019-04-03</span></span></div></header><div class="post-body">
        <h1   id="MapReduce-Tutorial" >
          <span class="heading-link">MapReduce Tutorial</span>
        </h1>
      
        <h2   id="Overview" >
          <span class="heading-link">Overview</span>
        </h2>
      <ul>
<li>Hadoop MapReduce是一个运行在集群上,并行处理大量数据(TB级别)的框架</li>
<li>MapReduce任务通常讲输入切分成多个独立的块,这些数据块被独立的map任务并行的处理</li>
<li>该框架会对map输出进行排序,作为reduce任务的输入</li>
<li>该框架负责调度任务,监视任务并重新执行失败的任务</li>
<li>通常,计算的节点和数据存储节点是同一个节点,也就是说,MapReduce框架和HDFS都运行在同一些列节点中.这个约束使得框架在数据已经存在的节点上有效地调度任务,从而产生跨集群的非常高的聚合带宽</li>
<li>MapReduce框架由一个ResourceManager,集群每个节点的NodeManager和每个应用程序的MRAppMaster组成</li>
<li>必须指定输入输出路径,实现指定的接口或者抽象类,覆写map和reduce方法</li>
<li>hadoop任务客户端提交任务和相关配置到ResouceManager,ResouceManager负责把任务/配置分发到其他的从节点,并调度和监控任务,给客户端提供任务的状态和诊断信息</li>
<li>hadoop stream允许用户使用任何可执行的程序来作为mapper/reducer任务</li>
<li>hadoop pipes工具可以使用C++ API来实现mapper/reducer</li>
</ul>

        <h2   id="Inputs-and-Outputs" >
          <span class="heading-link">Inputs and Outputs</span>
        </h2>
      <ul>
<li>MapReduce框架只针对&lt;Key,Value&gt;键值对类型操作.也就是说,每个MapReduce任务的输入是&lt;Key,Value&gt;形式,输入也是&lt;Key,Value&gt;形式,输入输出类型可不相同</li>
<li>Key,Value的类型必须是可以被框架序列化的类型,因此他们必须实现Writable接口.</li>
<li>Key的类型除了实现Writable接口之外,还需要实现WritableComparable接口,这样才能被排序</li>
<li>(input) <code>&lt;k1,v1&gt; -&gt;</code> <strong>map</strong> <code>-&gt; &lt;k2,v2&gt; -&gt;</code> <strong>combine</strong> <code>-&gt;  &lt;k2,v2&gt;  -&gt;</code> <strong>reduce</strong> <code>-&gt; &lt;k3,v3&gt;</code>  (output)</li>
</ul>
<p><strong>hadoop jar的一些参数</strong></p>
<ul>
<li>-files 可以使用逗号分隔,指定多个文件</li>
<li>-libjars 可以添加jar包到map和reduce类路径下</li>
<li>-archives 可以使用逗号分隔传入多个压缩包路径</li>
</ul>

        <h2   id="MapReduce-User-Interfaces" >
          <span class="heading-link">MapReduce - User Interfaces</span>
        </h2>
      <ul>
<li>实现Mapper和Reducer接口吗,并提供map/reduce的实现是任务的核心</li>
</ul>

        <h4   id="Mapper" >
          <span class="heading-link">Mapper</span>
        </h4>
      <ul>
<li>Mapper将输入的,Key/Value键值对类型映射成中间结果的Key/Value键值对类型</li>
<li>Maps是独立的任务,负责将输入转成中间结果</li>
<li>中间结果的类型无需和输入的类型一样</li>
<li>一个输入可能对应0,1,或者多个输出</li>
<li>每个InputSplit(由InputFormat产生)都有一个map任务</li>
<li>可以通过<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html"  target="_blank" rel="noopener">Job.setMapperClass(Class)</a><i class="fa fa-external-link"></i></span> 来传入Mapper的实现.框架将对每个键值对形式的InputSplit调用<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Mapper.html"  target="_blank" rel="noopener">map(WritableComparable, Writable, Context)</a><i class="fa fa-external-link"></i></span> 方法.如果需要清理一些必要资源,可以覆写<code>cleanup(Context)</code>方法</li>
<li>map的输出可以通过调用context.write(WritableComparable, Writable)来收集</li>
<li>所有的中间结果会被框架分组,然后传给Reducer.用户使用 <span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html"  target="_blank" rel="noopener">Job.setGroupingComparatorClass(Class)</a><i class="fa fa-external-link"></i></span>指定比较器Comparator来控制分组</li>
<li>Mapper的输出会被排序(sort)和打散(partitioner)分发给每一个Reducer.partitioner数目和reduce任务的数量相同.用户可以实现Partitioner接口来自定义打散规则,控制不同的Key分到对应的reduce任务中</li>
<li>用户可以使用<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html"  target="_blank" rel="noopener">Job.setCombinerClass(Class)</a><i class="fa fa-external-link"></i></span>对中间输出结果进行本地聚合,这可以减少从Mapper传到Reduce的传输量</li>
<li>中间结果都是以简单的 (key-len, key, value-len, value) 形式存储,也可通过Configuration设置对中间结果进行压缩</li>
</ul>

        <h5   id="How-Many-Maps" >
          <span class="heading-link">How Many Maps?</span>
        </h5>
      <ul>
<li>map任务的通常是由输入数据的大小来决定的,也就是输入文件的块数</li>
<li>对于cpu轻量级任务来说,每个节点map的并行度可达300,但是一般情况下并行度在10-100之间.任务的启动需要一定的时间,所以map任务至少需要1min的执行时间</li>
</ul>

        <h4   id="Reducer" >
          <span class="heading-link">Reducer</span>
        </h4>
      <ul>
<li>Reducer将相同key的中间结果集进行处理</li>
<li>reduce任务的个数是通过<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html"  target="_blank" rel="noopener">Job.setNumReduceTasks(int)</a><i class="fa fa-external-link"></i></span>来设置的</li>
<li>通过 <span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html"  target="_blank" rel="noopener">Job.setReducerClass(Class)</a><i class="fa fa-external-link"></i></span>来设置Reducer的实现类.框架对每组&lt;key, (list of values)&gt;的输入进行调用<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Reducer.html"  target="_blank" rel="noopener">reduce(WritableComparable, Iterable, Context)</a><i class="fa fa-external-link"></i></span> 方法进行处理,需要清理资源可以覆写cleanup(Context)</li>
</ul>

        <h5   id="Shuffle" >
          <span class="heading-link">Shuffle</span>
        </h5>
      <ul>
<li>传到Reducer的输入是经过排序后的mapper的输出.shuffle阶段,框架将通过http获取相关partition的mapper输出</li>
</ul>

        <h5   id="Sort" >
          <span class="heading-link">Sort</span>
        </h5>
      <ul>
<li>排序阶段,框架将Reducer的输入进行按Key进行分组</li>
<li>shuffle和sort同时进行.在map输出被拉取时,他们进行合并</li>
</ul>

        <h5   id="Secondary-Sort" >
          <span class="heading-link">Secondary Sort</span>
        </h5>
      <ul>
<li>如果中间结果key的分组规则需要和进入reducer前的keys的分组规则不一样,那么可以通过<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html"  target="_blank" rel="noopener">Job.setSortComparatorClass(Class)</a><i class="fa fa-external-link"></i></span>来设置比较器.因为<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html"  target="_blank" rel="noopener">Job.setSortComparatorClass(Class)</a><i class="fa fa-external-link"></i></span>时用来控制中间结果的keys是怎么分组的,所以可以用这个来对值进行二次排序</li>
</ul>

        <h5   id="Reduce" >
          <span class="heading-link">Reduce</span>
        </h5>
      <ul>
<li>reduce阶段,将对每一组&lt;key, (list of values)&gt;输入调用reduce(WritableComparable, Iterable&lt;Writable&gt;, Context)方法</li>
<li>reduce任务通过 Context.write(WritableComparable, Writable)将输出结果写入文件系统</li>
<li>输出结果并不会进行排序</li>
</ul>

        <h5   id="How-Many-Reduces" >
          <span class="heading-link">How Many Reduces?</span>
        </h5>
      <ul>
<li>比较合理的reduce任务的个数计算公式是:0.95(或1.75)×节点数(注意,不是每个节点的最大container数)</li>
<li>0.95系数可以使得reduce任务在map任务的输出传输结束后同时开始运行</li>
<li>1.75系数可以使得计算快的节点在一批reduce任务计算结束之后开始计算第二批 reduce任务,实现负载均衡</li>
<li>增加reduce的数量虽然会增加负载，但是可以改善负载匀衡，降低任务失败带来的负面影响</li>
<li>放缩系数要比整数略小是因为要给推测性任务和失败任务预留reduce位置</li>
</ul>

        <h5   id="Reducer-NONE" >
          <span class="heading-link">Reducer NONE</span>
        </h5>
      <ul>
<li>如果不需要reduce任务,将reduce任务个数设置为0是合法的</li>
<li>这种情况下,map任务的输出会直接写入文件系统的指定输出路径<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html"  target="_blank" rel="noopener">FileOutputFormat.setOutputPath(Job, Path)</a><i class="fa fa-external-link"></i></span>.在写入文件系统前,map的输出是进行排序的</li>
</ul>

        <h4   id="Partitioner" >
          <span class="heading-link">Partitioner</span>
        </h4>
      <ul>
<li>partitioner控制中间map输出的key的分区</li>
<li>可以按照key(或者key的一部分)来产生分区,默认是使用hash进行分区</li>
<li>分区数和reduce任务的个数相等</li>
<li>控制发送给reduce的任务个数</li>
</ul>

        <h4   id="Counter" >
          <span class="heading-link">Counter</span>
        </h4>
      <ul>
<li>Counter是一个公共基础工具,用来报告MapReduce应用的统计信息</li>
<li>Mapper和Reducer实现类都可以使用Counter来报告统计</li>
</ul>

        <h3   id="Job-Configuration" >
          <span class="heading-link">Job Configuration</span>
        </h3>
      <ul>
<li><p>Job就是MapReduce任务的job配置代表</p>
</li>
<li><p>一般MapReduce框架会严格按照Job的配置执行,但是有几种情况例外</p>
<ul>
<li>某些配置参数被标记为final类型,所以是修改配置是没法达到目的的,例如1.1比例</li>
<li>某些配置虽然可以直接配置,但是还需要配合其他的参数一起配置才能生效</li>
</ul>
</li>
<li><p>Job通常会指定Mapper,combiner(有必要的话),Partitioner,Reducer,InputFormat,OutputFormat的实现类</p>
</li>
<li><p>输入可以使用下列方式指定输入数据文件集</p>
<ul>
<li>(<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html"  target="_blank" rel="noopener">FileInputFormat.setInputPaths(Job, Path…)</a><i class="fa fa-external-link"></i></span>/ <span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html"  target="_blank" rel="noopener">FileInputFormat.addInputPath(Job, Path)</a><i class="fa fa-external-link"></i></span>)</li>
<li>(<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html"  target="_blank" rel="noopener">FileInputFormat.setInputPaths(Job, String…)</a><i class="fa fa-external-link"></i></span>/ <span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html"  target="_blank" rel="noopener">FileInputFormat.addInputPaths(Job, String)</a><i class="fa fa-external-link"></i></span></li>
</ul>
</li>
<li><p>输出可以使用(<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html"  target="_blank" rel="noopener">FileOutputFormat.setOutputPath(Path)</a><i class="fa fa-external-link"></i></span>)来指定输出文件集</p>
</li>
<li><p>其他配置都是可选的,如Caparator的使用,将文件放置到DistributeCache,是否中间结果或者最终输出结果需要压缩,是否允许推测模式,最大任务重试次数等</p>
</li>
<li><p>可以通过<span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/conf/Configuration.html"  target="_blank" rel="noopener">Configuration.set(String, String)</a><i class="fa fa-external-link"></i></span>/ <span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/conf/Configuration.html"  target="_blank" rel="noopener">Configuration.get(String)</a><i class="fa fa-external-link"></i></span>来设置和获取任意需要的参数.但是对于大的只读数据集,还是要用DistributedCache</p>
</li>
</ul>

        <h3   id="Task-Execution-amp-Environment" >
          <span class="heading-link">Task Execution &amp; Environment</span>
        </h3>
      <ul>
<li>MRAppMaster在独立的JVM中执行每个Mapper/Reducer任务(任务进程级别)</li>
<li>子任务继承了MRAppMaster的环境.</li>
<li>用户可以通过 <code>mapreduce.{map|reduce}.java.opts</code> 给子任务添加额外的参数</li>
<li>运行时非标准类库路径可以通过-Djava.library.path=&lt;&gt;指定</li>
<li>如果mapreduce.{map|reduce}.java.opts参数配置包含了<em>@taskid@</em>则在运行时被替换成taskId</li>
<li>显示JVM GC,JVM JMX无密代理(这样可以结合jconsole,查看内存,线程,线程垃圾回收),最大堆内存,添加其他路径到任务java.library.path的例子</li>
</ul>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">  -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc</span><br><span class="line">  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false</span><br><span class="line">  <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">  -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc</span><br><span class="line">  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false</span><br><span class="line">  <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></div></figure>


        <h4   id="Memory-Management" >
          <span class="heading-link">Memory Management</span>
        </h4>
      <ul>
<li><p>用户可以通过<code>mapreduce.{map|reduce}.memory.mb</code>指定子任务的最大虚拟内存.注意这个设置是进程级别的</p>
</li>
<li><p>注意这个参数不要大于-Xmx的参数,否则VM可能会无法启动</p>
</li>
<li><p><code>mapreduce.{map|reduce}.java.opt</code>只能配置MRAppMaster的子任务.配置守护线程的需要参考 <span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-common/ClusterSetup.html#Configuring_Environment_of_Hadoop_Daemons"  target="_blank" rel="noopener">Configuring the Environment of the Hadoop Daemons</a><i class="fa fa-external-link"></i></span></p>
</li>
<li><p>map/reduce任务的性能,可能会被并发数,写入磁盘的频率影响.检查文件系统的统计报告,尤其是从map进入reduce的字节数,这参数是非常宝贵的.</p>
</li>
</ul>

        <h4   id="Map-Parameters" >
          <span class="heading-link">Map Parameters</span>
        </h4>
      <ul>
<li>map输出的记录会被序列化到缓冲区,元数据存储在统计缓冲区</li>
<li>当缓冲区或者元数据超过一定的阈值,缓冲区的内容会被排序然后存储和写入磁盘</li>
<li>如果缓冲区一直是满状态的,map线程将被阻塞</li>
<li>map结束后,没有写入磁盘的map输出记录继续写入.</li>
<li>磁盘上所有的map输出文件段会合并成单个文件</li>
<li>减少写入磁盘的次数,可以减少map的次数,但是加大缓存区会压缩mapper的可用内存</li>
</ul>
<div class="table-container"><table>
<thead>
<tr>
<th align="left">Name</th>
<th align="left">Type</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">mapreduce.task.io.sort.mb</td>
<td align="left">int</td>
<td align="left">序列化和map输出到缓冲区的记录预排序的累计大小,单位为MB</td>
</tr>
<tr>
<td align="left">mapreduce.map.sort.spill.percent</td>
<td align="left">float</td>
<td align="left">序列化缓冲区spill阈值比例,超过会将缓冲区内容写入磁盘</td>
</tr>
</tbody></table></div>
<ul>
<li>spill之后,如果在写入磁盘过程中,map的输出没有超过spill阈值,则会继续收集到spill结束</li>
<li>如果是spill设置为0.33,在spill到磁盘的过程,缓冲区继续会被map的输出填充,下一次spill的时候再将这期间填充的内容写到磁盘</li>
<li>如果spill设置为0.66,则不会触发下一次spill.也就是说,spill可以触发,但是不会阻塞</li>
<li>一条记录大于缓冲区的会先触发spill,而且会被spill到一个单独的文件.无论这条记录有没有定义combiner,它都会被combiner传输</li>
</ul>

        <h4   id="Shuffle-Reduce-Parameters" >
          <span class="heading-link">Shuffle/Reduce Parameters</span>
        </h4>
      <ul>
<li>reduce将partitioner通过http指派给自己的map输出加载到内存,并定期合并输出到磁盘.</li>
<li>如果中间结果是压缩输出,那么输出也是被reduce压缩的读进内存中,减少了内存的压力</li>
</ul>
<div class="table-container"><table>
<thead>
<tr>
<th align="left">Name</th>
<th align="left">Type</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">mapreduce.task.io.soft.factor</td>
<td align="left">int</td>
<td align="left">每次合并磁盘上段的数目.如果超过这个设置会分多次进行合并</td>
</tr>
<tr>
<td align="left">mapreduce.reduce.merge.inmem.thresholds</td>
<td align="left">int</td>
<td align="left">在合并写入磁盘之前,将排序后的map输出加载到内存的map输出数目.这个值通常设置很大(1000)或者直接禁用(0),因为内存合并要比磁盘合并的代价小得多.这个阈值只影响shuffle期间内存中合并的频率</td>
</tr>
<tr>
<td align="left">mapreduce.reduce.shuffle.merge.percent</td>
<td align="left">float</td>
<td align="left">在内存合并之前,读取map输出的内存阈值,代表着用于存储map输出在内存中的百分比.因为map的输出并不适合存储在内存,所以设置很高会知道使得获取和合并的并行度下降.相反,设置为1可以使得内存运行的reduce更快.这个参数只影响shuffle期间的内存内合并频率</td>
</tr>
<tr>
<td align="left">mapreduce.reduce.shuffle.input.buffer.percent</td>
<td align="left">float</td>
<td align="left">在shuffle期间,可以分配来存储map输出的内存百分比,相对于<code>mapreduce.reduce.java.opts</code>指定的最大堆内存.把这个值设的大一点可以存储更多的map输出,但是也应该为框架预留一些内存</td>
</tr>
<tr>
<td align="left">mapreduce.reduce.input.buffer.percent</td>
<td align="left">float</td>
<td align="left">相当于reduce阶段,用于存储map输出的最大堆内存的内存百分比.reduce开始的时候,map的输出被合并到磁盘,知道map输出在一定的阈值之内.默认情况下,在reduce开始之前,map的输出都会被合并到磁盘,这样才能使得reduce充分的利用到内存.对于只要内存密集型的reduce任务,应该增加这个值,减少磁盘的的往返时间</td>
</tr>
</tbody></table></div>

        <h4   id="Configured-Parameters" >
          <span class="heading-link">Configured Parameters</span>
        </h4>
      <p>这些参数都是局部的,每个任务的</p>
<div class="table-container"><table>
<thead>
<tr>
<th align="left">Name</th>
<th align="left">Type</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="left">mapreduce.job.id</td>
<td align="left">String</td>
<td align="left">The job id</td>
</tr>
<tr>
<td align="left">mapreduce.job.jar</td>
<td align="left">String</td>
<td align="left">job.jar location in job directory</td>
</tr>
<tr>
<td align="left">mapreduce.job.local.dir</td>
<td align="left">String</td>
<td align="left">The job specific shared scratch space</td>
</tr>
<tr>
<td align="left">mapreduce.task.id</td>
<td align="left">String</td>
<td align="left">The task id</td>
</tr>
<tr>
<td align="left">mapreduce.task.attempt.id</td>
<td align="left">String</td>
<td align="left">The task attempt id</td>
</tr>
<tr>
<td align="left">mapreduce.task.is.map</td>
<td align="left">boolean</td>
<td align="left">Is this a map task</td>
</tr>
<tr>
<td align="left">mapreduce.task.partition</td>
<td align="left">int</td>
<td align="left">The id of the task within the job</td>
</tr>
<tr>
<td align="left">mapreduce.map.input.file</td>
<td align="left">String</td>
<td align="left">The filename that the map is reading from</td>
</tr>
<tr>
<td align="left">mapreduce.map.input.start</td>
<td align="left">long</td>
<td align="left">The offset of the start of the map input split</td>
</tr>
<tr>
<td align="left">mapreduce.map.input.length</td>
<td align="left">long</td>
<td align="left">The number of bytes in the map input split</td>
</tr>
<tr>
<td align="left">mapreduce.task.output.dir</td>
<td align="left">String</td>
<td align="left">The task’s temporary output directory</td>
</tr>
</tbody></table></div>
<p>在流任务执行过程中,这些参数会被转化.点(.)会被转成下划线(_),所以要想在流任务的mapper/reducer中获得这些值,需要使用下划线形式.</p>

        <h4   id="Distributing-Libraries" >
          <span class="heading-link">Distributing Libraries</span>
        </h4>
      <ul>
<li><span class="external-link"><a   href="https://hadoop.apache.org/docs/r2.8.5/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#DistributedCache"  target="_blank" rel="noopener">DistributedCache</a><i class="fa fa-external-link"></i></span>分布式缓存可以分发jars和本地类库给map/reduce任务使用.</li>
<li>child-jvm总将自己的工作目录添加到java.library.path和LD_LIBRARY_PATH</li>
<li>缓存中的类库可以通过System.loadLibrary或者System.load</li>
</ul>

        <h3   id="Job-Submission-and-Monitoring" >
          <span class="heading-link">Job Submission and Monitoring</span>
        </h3>
      <ul>
<li>Job是用户任务和ResourceManager交互的主要接口</li>
<li>Job的提交流程包括<ul>
<li>检查输入输出路径</li>
<li>计算任务的InputSplit</li>
<li>有必要的话,设置必要的分布式缓存</li>
<li>拷贝任务的jar和配置到MapReduce系统目录</li>
<li>提交任务到ResourceManager.监控任务状态是可选的</li>
<li>任务的执行记录历史存放在 <code>mapreduce.jobhistory.intermediate-done-dir</code> 和<code>mapreduce.jobhistory.done-dir</code></li>
</ul>
</li>
</ul>

        <h4   id="Job-Control" >
          <span class="heading-link">Job Control</span>
        </h4>
      <ul>
<li>对于单个MapReduce任务无法完成的任务,用户可能需要执行MapReduce任务链,才能完成.这还是非常容易的,因为任务的输出一般是存储在分布式文件系统中,所以一个任务的输出可以作为另一个任务的输入.这也就使得判断任务是否完成,不管成功或者失败,都需要用户来控制.主要有两种控制手段<ul>
<li>Job.submit() 提交任务到集群中,立即返回</li>
<li>Job.waitForCompletion(boolean) 提交任务到集群中,等待其完成</li>
</ul>
</li>
</ul>

        <h3   id="Job-Input" >
          <span class="heading-link">Job Input</span>
        </h3>
      <ul>
<li><p>InputFormat描述了MapReduce任务的输入规范</p>
</li>
<li><p>InputFormat的职责是:</p>
<ul>
<li>校验输入是否合法</li>
<li>将输入逻辑切分成InputSplit实例,之后将它们发送到独立的Mapper</li>
<li>RecordReader 实现了从符合框架逻辑的InputSplit实例收集输入的记录,提供给Mapper进行处理</li>
</ul>
</li>
<li><p>默认的InputFormat是基于输入文件的总字节大小,将输入文件切分成逻辑的InputSplit实例,例如FileInputFormat的子类.然而,文件系统的blocksize只是split的上限,下限需要通过<code>mapreduce.input.fileinputformat.split.minsize</code>来设置</p>
</li>
<li><p>压缩文件并不一定可以被切分,如.gz文件会把完整的文件交给一个mapper来处理</p>
</li>
</ul>

        <h4   id="InputSplit" >
          <span class="heading-link">InputSplit</span>
        </h4>
      <ul>
<li><p>InputSplit代表了一个独立Mapper处理的输入数据</p>
</li>
<li><p>通常InputSplit是面向字节的,把面向字节转为面向记录是RecordReader的职责</p>
</li>
<li><p>FileSplit是默认的InputSplit实现,它把输入设置成mapreduce.map.input.file 属性,用于进行逻辑分割</p>
</li>
</ul>

        <h4   id="RecordReader" >
          <span class="heading-link">RecordReader</span>
        </h4>
      <ul>
<li>RecordReader负责将InputSplit的面向字节的输入转换成面向记录,提供给Mapper实现去处理每一条记录.因此RecordReader承担了从记录中提取出键值对的任务</li>
</ul>

        <h3   id="Job-Output" >
          <span class="heading-link">Job Output</span>
        </h3>
      <ul>
<li>OutputFormat描述了MapReduce输出的规范</li>
<li>OutputFormat的职责:<ul>
<li>校验任务的输出,例如输出目录是否存在</li>
<li>RecordWriter实现可以将任务的输出写入到文件,存储在文件系统中</li>
</ul>
</li>
<li>TextOutputFormat是默认的OutputFormat实现</li>
</ul>
</div><footer class="post-footer"><div class="post-end"><p><span>------ </span><span>End of the article, thanks for your reading</span><span> ------</span></p></div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-author-name">Author: </span><span class="post-copyright-author-value"><a href="https://lurongjiang.github.io">LRJ</a></span></div><div class="post-copyright-link"><span class="post-copyright-link-name">Link: </span><span class="post-copyright-link-value"><a href="https://lurongjiang.github.io/2019/04/01/MapReduce%20Tutorial/">https://lurongjiang.github.io/2019/04/01/MapReduce%20Tutorial/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-notice-name">Copyright: </span><span class="post-copyright-notice-value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><i class="post-tags-item__i fa fa-tags"></i><a class="post-tags-item__a" href="https://lurongjiang.github.io/tags/Hadoop-MapReduce/">Hadoop,MapReduce</a></span></div><nav class="paginator"><div class="paginator-post"><div class="paginator-post-prev"><a href="/2019/04/03/Azkaband%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/"><i class="fa fa-chevron-left"></i><span>Azkaband的安装和使用</span></a></div><div class="paginator-post-next"><a href="/2019/04/01/hive%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/"><span>Hive的一些坑</span><i class="fa fa-chevron-right"></i></a></div></div></nav></footer></div></div></div><aside class="sidebar" id="sidebar"><div class="sidebar-inner"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MapReduce-Tutorial"><span class="toc-number">1.</span> <span class="toc-text">
          MapReduce Tutorial
        </span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Overview"><span class="toc-number">1.1.</span> <span class="toc-text">
          Overview
        </span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Inputs-and-Outputs"><span class="toc-number">1.2.</span> <span class="toc-text">
          Inputs and Outputs
        </span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce-User-Interfaces"><span class="toc-number">1.3.</span> <span class="toc-text">
          MapReduce - User Interfaces
        </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Mapper"><span class="toc-number">1.3.0.1.</span> <span class="toc-text">
          Mapper
        </span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#How-Many-Maps"><span class="toc-number">1.3.0.1.1.</span> <span class="toc-text">
          How Many Maps?
        </span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reducer"><span class="toc-number">1.3.0.2.</span> <span class="toc-text">
          Reducer
        </span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Shuffle"><span class="toc-number">1.3.0.2.1.</span> <span class="toc-text">
          Shuffle
        </span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Sort"><span class="toc-number">1.3.0.2.2.</span> <span class="toc-text">
          Sort
        </span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Secondary-Sort"><span class="toc-number">1.3.0.2.3.</span> <span class="toc-text">
          Secondary Sort
        </span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Reduce"><span class="toc-number">1.3.0.2.4.</span> <span class="toc-text">
          Reduce
        </span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#How-Many-Reduces"><span class="toc-number">1.3.0.2.5.</span> <span class="toc-text">
          How Many Reduces?
        </span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Reducer-NONE"><span class="toc-number">1.3.0.2.6.</span> <span class="toc-text">
          Reducer NONE
        </span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Partitioner"><span class="toc-number">1.3.0.3.</span> <span class="toc-text">
          Partitioner
        </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Counter"><span class="toc-number">1.3.0.4.</span> <span class="toc-text">
          Counter
        </span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Job-Configuration"><span class="toc-number">1.3.1.</span> <span class="toc-text">
          Job Configuration
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Task-Execution-amp-Environment"><span class="toc-number">1.3.2.</span> <span class="toc-text">
          Task Execution &amp; Environment
        </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Memory-Management"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">
          Memory Management
        </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Map-Parameters"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">
          Map Parameters
        </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Shuffle-Reduce-Parameters"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">
          Shuffle&#x2F;Reduce Parameters
        </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Configured-Parameters"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">
          Configured Parameters
        </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Distributing-Libraries"><span class="toc-number">1.3.2.5.</span> <span class="toc-text">
          Distributing Libraries
        </span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Job-Submission-and-Monitoring"><span class="toc-number">1.3.3.</span> <span class="toc-text">
          Job Submission and Monitoring
        </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Job-Control"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">
          Job Control
        </span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Job-Input"><span class="toc-number">1.3.4.</span> <span class="toc-text">
          Job Input
        </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#InputSplit"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">
          InputSplit
        </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RecordReader"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">
          RecordReader
        </span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Job-Output"><span class="toc-number">1.3.5.</span> <span class="toc-text">
          Job Output
        </span></a></li></ol></li></ol></li></ol></div></section><!-- ov = overview --><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/avatar2.png" alt="avatar"></div><p class="sidebar-ov-author__p">LRJ</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state__a posts" href="/archives/"><div class="sidebar-ov-state__a--count">12</div><div class="sidebar-ov-state__a--name">Archives</div></a><a class="sidebar-ov-state__a categories" href="/categories/"><div class="sidebar-ov-state__a--count">12</div><div class="sidebar-ov-state__a--name">Categories</div></a><a class="sidebar-ov-state__a tags" href="/tags/"><div class="sidebar-ov-state__a--count">14</div><div class="sidebar-ov-state__a--name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span>You have read </span><span class="sidebar-reading-info-num">0</span></div><div class="sidebar-reading-line"></div></div></div></aside><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright &copy; 2020</span><span class="fa fa-heart footer-icon"></span><span>LRJ.</span></div><div><span>Powered by <a href="http://hexo.io/" title="hexo" target="_blank" rel="noopener">hexo</a></span><span> v4.2.0.</span><span class="separator">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="stun" target="_blank" rel="noopener">stun</a></span><span> v1.7.0.</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="progress"></div></div><div class="back2top" id="back2top"><i class="back2top-icon fa fa-rocket"></i></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=1.7.0"></script><script src="/js/stun-boot.js?v=1.7.0"></script><script src="/js/scroll.js?v=1.7.0"></script><script src="/js/header.js?v=1.7.0"></script><script src="/js/sidebar.js?v=1.7.0"></script></body></html>