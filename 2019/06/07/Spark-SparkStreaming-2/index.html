<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="keywords" content="LRJ, Blog"><meta name="description" content="个人博客,记录成长历程"><title>SparkStreaming基础-SparkStreaming的tranform和与Kakfa集成</title><link rel="icon" href="/images/icons/favicon-16x16.png?v=1.7.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=1.7.0" type="image/png" sizes="32x32"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=1.7.0"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  fontawesome: {"prefix":"fa"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  post_widget: {"end_text":true},
  night_mode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"ocean","word_wrap":false},
  reward: false,
  fancybox: false,
  zoom_image: {"enable":true,"mask_color":"rgba(0,0,0,0.6)"},
  gallery_waterfall: undefined,
  lazyload: undefined,
  pjax: undefined,
  external_link: {"icon":{"enable":true,"name":"external-link"}},
  shortcuts: undefined,
  prompt: {"copy_success":"Copy Success","copy_error":"Copy Error","creative_commons":"Creative Commons","copy_button":"Copy"}
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
</head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-btn fa fa-bars"></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/"><i class="fa fa-home"></i>Home</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/archives/"><i class="fa fa-folder-open"></i>Archives</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/categories/"><i class="fa fa-th"></i>Categories</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/tags/"><i class="fa fa-tags"></i>Tags</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/about/"><i class="fa fa-user"></i>About</a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-info"><div class="header-info-inner"><div class="header-info-title">Blog</div><div class="header-info-subtitle">个人博客,记录成长历程</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content"><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-header-title">SparkStreaming基础-SparkStreaming的tranform和与Kakfa集成</h1><div class="post-header-meta"><span class="post-header-meta-create"><i class="fa fa-calendar-o"></i><span>Posted </span><span>2019-06-07</span></span><span class="post-header-meta-update"><i class="fa fa-calendar-check-o"></i><span>updated </span><span>2019-06-09</span></span></div></header><div class="post-body">
        <h1   id="SparkStreaming-2" >
          <span class="heading-link">SparkStreaming-2</span>
        </h1>
      
        <h2   id="tranform" >
          <span class="heading-link">tranform</span>
        </h2>
      <p>Spark Streaming接收到数据得到的是一个DStream,如果需要DStream和RDD进行关联,此时并没有DStream和RDD关联的API,所以需要tranform算子来进行DStream和RDD的关联</p>
<p>例如,我们只要统计指定的单词</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createStreamContext</span></span>() = &#123;</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"transformApp"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">      <span class="comment">//只统计hello,word</span></span><br><span class="line">      <span class="keyword">val</span> wordRDD = sc.parallelize(<span class="type">List</span>(<span class="string">"hello"</span>, <span class="string">"word"</span>)).map((_, <span class="number">1</span>))</span><br><span class="line">      <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">      <span class="keyword">val</span> stream = ssc.socketTextStream(<span class="string">"hadoop001"</span>, <span class="number">9000</span>)</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">updateState</span> </span>= (n: <span class="type">Seq</span>[<span class="type">Int</span>], o: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> sum = n.sum + o.getOrElse(<span class="number">0</span>)</span><br><span class="line">        <span class="type">Some</span>(sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      stream.flatMap(_.split(<span class="string">","</span>)).map((_, <span class="number">1</span>)).transform(rdd =&gt; &#123;</span><br><span class="line">        rdd.join(wordRDD)</span><br><span class="line">      &#125;).map(t =&gt; (t._1, t._2._1)).updateStateByKey(updateState).print()</span><br><span class="line">      ssc.checkpoint(<span class="string">"checkpoint"</span>)</span><br><span class="line">      ssc</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(<span class="string">"checkpoint"</span>, createStreamContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h2   id="Window" >
          <span class="heading-link">Window</span>
        </h2>
      <p>滑窗有两个重要的概念</p>
<ul>
<li><em>window length</em> - 窗口的间隔</li>
<li><em>sliding interval</em> - 每次滑移的间隔</li>
</ul>
<p>这两个值必须是DStream的间隔时间的倍数,例如DStream可能为每5秒钟一次,window length=30s,则相当于每次的窗口数据包含了6个DStream间隔,silding interval=10s,那么就是10秒滑动一次</p>
<p>例如我们想,每隔10s统计30s中以内的wc</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createStreamContext</span></span>() = &#123;</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WindowApp"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">      <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">      <span class="keyword">val</span> stream = ssc.socketTextStream(<span class="string">"hadoop001"</span>, <span class="number">9000</span>)</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">updateState</span> </span>= (seq: <span class="type">Seq</span>[<span class="type">Int</span>], oldValue: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">        <span class="type">Some</span>(seq.sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      stream.flatMap(_.split(<span class="string">","</span>)).map((_, <span class="number">1</span>))</span><br><span class="line">        .reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;a+b, <span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">        .updateStateByKey(updateState).print()</span><br><span class="line">      ssc.checkpoint(<span class="string">"checkpoint"</span>)</span><br><span class="line">      ssc</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(<span class="string">"checkpoint"</span>, createStreamContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>这样运行后就可以看到,不在时间范围内的就不会被统计</p>
<p><img src="/images/image/image-20200426014314067.png" alt="image-20200426014314067"></p>

        <h2   id="Output-Operations-on-DStreams-输出算子" >
          <span class="heading-link">Output Operations on DStreams 输出算子</span>
        </h2>
      <p>SparkStreaming作为流处理,一般不会输出到文件系统如hdfs,或者保存为文件,因为这样可能会导致大量的小文件问题,一般常用的算子为</p>
<ul>
<li><p>print</p>
</li>
<li><p>foreachRDD</p>
<p>使用最多的算子,在Streaming中,一般全都要使用foreachRDD来操作</p>
</li>
</ul>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ForeachRDDApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createStreamContext</span></span>() = &#123;</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"ForeachRDDApp"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">      <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">      <span class="keyword">val</span> stream = ssc.socketTextStream(<span class="string">"hadoop001"</span>, <span class="number">9000</span>)</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">updateState</span> </span>= (seq: <span class="type">Seq</span>[<span class="type">Int</span>], oldValue: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">        <span class="type">Some</span>(seq.sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      stream.flatMap(_.split(<span class="string">","</span>)).map((_, <span class="number">1</span>))</span><br><span class="line">        .reduceByKeyAndWindow((a: <span class="type">Int</span>, b: <span class="type">Int</span>) =&gt; a + b, <span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">        .updateStateByKey(updateState).foreachRDD((rdd, time) =&gt; &#123;</span><br><span class="line">            <span class="comment">//也可以写到数据库,这里打印方便点</span></span><br><span class="line">        <span class="keyword">val</span> array = rdd.collect()</span><br><span class="line">        <span class="keyword">val</span> str = array.mkString(<span class="string">","</span>)</span><br><span class="line">        println(<span class="string">s"time:<span class="subst">$time</span>----&gt;<span class="subst">$str</span>"</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      ssc.checkpoint(<span class="string">"checkpoint"</span>)</span><br><span class="line">      ssc</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(<span class="string">"checkpoint"</span>, createStreamContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>运行效果</p>
<p><img src="/images/image/image-20200426020128960.png" alt="image-20200426020128960"></p>

        <h3   id="Design-Patterns-for-using-foreachRDD" >
          <span class="heading-link">Design Patterns for using foreachRDD</span>
        </h3>
      <p>foreachRDD可以将数据写到外部系统中,但是使用不当会导致一些错误.常见错误如,创建连接,如何写到外部,但是由于连接之类的很少实现了序列化,所以会导致不能序列化的错误</p>
<p><strong>错误</strong>例子</p>
<p>连接创建在Driver端,但是使用是在worker端,这时需要序列化,所以这种用法是错误的</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> connection = createNewConnection()  <span class="comment">// executed at the driver</span></span><br><span class="line">  rdd.foreach &#123; record =&gt;</span><br><span class="line">    connection.send(record) <span class="comment">// executed at the worker</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>另一种常见的<strong>错误</strong>用法是,每个rdd都创建连接,这样增加系统的负荷</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreach &#123; record =&gt;</span><br><span class="line">    <span class="keyword">val</span> connection = createNewConnection()</span><br><span class="line">    connection.send(record)</span><br><span class="line">    connection.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p><strong>正确的方式</strong>是使用连接池,用完归还</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">    <span class="comment">// ConnectionPool is a static, lazily initialized pool of connections</span></span><br><span class="line">    <span class="keyword">val</span> connection = <span class="type">ConnectionPool</span>.getConnection()</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</span><br><span class="line">    <span class="type">ConnectionPool</span>.returnConnection(connection)  <span class="comment">// return to the pool for future reuse</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>连接池的连接最好做成懒加载的模式,不使用一段时间后进行销毁,避免占用资源</p>

        <h5   id="Other-points-to-remember" >
          <span class="heading-link">Other points to remember</span>
        </h5>
      <ul>
<li>DStream只有遇到输出算子才会执行(和RDD的懒加载一样,只有遇到action才执行),DStream内部RDD的action会强制DStream处理接收到的数据,如果你的应用没有设置输出算子,或者只有foreachRDD算子而内部没有RDD的action算子,那么不会有任何输出</li>
</ul>

        <h2   id="Input-DStreams-and-Receivers" >
          <span class="heading-link">Input DStreams and Receivers</span>
        </h2>
      
        <h3   id="Basic-Sources" >
          <span class="heading-link">Basic Sources</span>
        </h3>
      
        <h3   id="Advanced-Sources" >
          <span class="heading-link">Advanced Sources</span>
        </h3>
      
        <h4   id="Kafka-Source" >
          <span class="heading-link">Kafka Source</span>
        </h4>
      <p>Spark Streaming和Kafka集成的 0.10版本,Kafka的partition和SparkStreaming的partition是一致的,保持1:1</p>

        <h5   id="添加依赖" >
          <span class="heading-link">添加依赖</span>
        </h5>
      <figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_$&#123;scala.tool.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></div></figure>


        <h5   id="Creating-a-Direct-Stream" >
          <span class="heading-link">Creating a Direct Stream</span>
        </h5>
      <figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaStreamApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createStreamContext</span></span>() = &#123;</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaStreamApp"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">      <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">      <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">        <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop001:9091,hadoop001:9092,hadoop001:9093"</span>,</span><br><span class="line">        <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"group.id"</span> -&gt; <span class="string">"myGroup"</span>,</span><br><span class="line">        <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>,</span><br><span class="line">        <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"testA"</span>)</span><br><span class="line">      <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        ssc,</span><br><span class="line">        <span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(newValues: <span class="type">Seq</span>[<span class="type">Int</span>], runningCount: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">        <span class="type">Some</span>(newValues.sum)</span><br><span class="line">      &#125;</span><br><span class="line">      stream.map(record =&gt; (record.value,<span class="number">1</span>))</span><br><span class="line">        .reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;a+b,<span class="type">Seconds</span>(<span class="number">10</span>),<span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        .updateStateByKey(updateFunction).print()</span><br><span class="line">      ssc.checkpoint(<span class="string">"checkpoint"</span>)</span><br><span class="line">      ssc</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(<span class="string">"checkpoint"</span>, createStreamContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>创建消费者</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyKafkaProducer</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop001:9091,hadoop001:9092,hadoop001:9093"</span>)</span><br><span class="line">    props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>)</span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>)</span><br><span class="line">    props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>)</span><br><span class="line">    <span class="keyword">val</span> producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](props)</span><br><span class="line">    <span class="keyword">for</span> (x &lt;- <span class="number">0</span> to <span class="number">1000</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> a = '<span class="type">A</span>'</span><br><span class="line">      <span class="keyword">val</span> word = (<span class="type">Random</span>.nextInt(<span class="number">23</span>) + a).toChar+<span class="string">""</span></span><br><span class="line">      producer.send(<span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">"testA"</span>, x%<span class="number">3</span>, x+<span class="string">""</span>, word))</span><br><span class="line">      <span class="type">TimeUnit</span>.<span class="type">MICROSECONDS</span>.sleep(<span class="number">200</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<p>输出</p>
<p><img src="/images/image/image-20200426033659694.png" alt="image-20200426033659694"></p>

        <h5   id="Obtaining-Offsets获取偏移量" >
          <span class="heading-link">Obtaining Offsets获取偏移量</span>
        </h5>
      <p>获取消费的偏移量</p>
<figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">  rdd.foreachPartition &#123; iter =&gt;</span><br><span class="line">    <span class="keyword">val</span> o: <span class="type">OffsetRange</span> = offsetRanges(<span class="type">TaskContext</span>.get.partitionId)</span><br><span class="line">    println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span> <span class="subst">$&#123;o.partition&#125;</span> <span class="subst">$&#123;o.fromOffset&#125;</span> <span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h5   id="Storing-Offsets存储偏移量" >
          <span class="heading-link">Storing Offsets存储偏移量</span>
        </h5>
      <figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"></span><br><span class="line">  <span class="comment">// some time later, after outputs have completed</span></span><br><span class="line">  stream.asInstanceOf[<span class="type">CanCommitOffsets</span>].commitAsync(offsetRanges)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>


        <h5   id="设置kafka数据的偏移量" >
          <span class="heading-link">设置kafka数据的偏移量</span>
        </h5>
      <figure class="highlight scala"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fromOffsets = selectOffsetsFromYourDatabase.map &#123; resultSet =&gt;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">TopicPartition</span>(resultSet.string(<span class="string">"topic"</span>), resultSet.int(<span class="string">"partition"</span>)) -&gt; resultSet.long(<span class="string">"offset"</span>)</span><br><span class="line">&#125;.toMap</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">  streamingContext,</span><br><span class="line">  <span class="type">PreferConsistent</span>,</span><br><span class="line">  <span class="type">Assign</span>[<span class="type">String</span>, <span class="type">String</span>](fromOffsets.keys.toList, kafkaParams, fromOffsets)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> results = yourCalculation(rdd)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// begin your transaction</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// update results</span></span><br><span class="line">  <span class="comment">// update offsets where the end of existing offsets matches the beginning of this batch of offsets</span></span><br><span class="line">  <span class="comment">// assert that offsets were updated correctly</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// end your transaction</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure></div><footer class="post-footer"><div class="post-end"><p><span>------ </span><span>End of the article, thanks for your reading</span><span> ------</span></p></div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-author-name">Author: </span><span class="post-copyright-author-value"><a href="https://lurongjiang.github.io">LRJ</a></span></div><div class="post-copyright-link"><span class="post-copyright-link-name">Link: </span><span class="post-copyright-link-value"><a href="https://lurongjiang.github.io/2019/06/07/Spark-SparkStreaming-2/">https://lurongjiang.github.io/2019/06/07/Spark-SparkStreaming-2/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-notice-name">Copyright: </span><span class="post-copyright-notice-value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><i class="post-tags-item__i fa fa-tags"></i><a class="post-tags-item__a" href="https://lurongjiang.github.io/tags/Spark/">Spark</a></span><span class="post-tags-item"><i class="post-tags-item__i fa fa-tags"></i><a class="post-tags-item__a" href="https://lurongjiang.github.io/tags/SparkStreaming/">SparkStreaming</a></span><span class="post-tags-item"><i class="post-tags-item__i fa fa-tags"></i><a class="post-tags-item__a" href="https://lurongjiang.github.io/tags/SparkStreaming-Kakfa/">SparkStreaming-Kakfa</a></span></div><nav class="paginator"><div class="paginator-post"><div class="paginator-post-prev"><a href="/2019/08/28/CDH-CHD%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"><i class="fa fa-chevron-left"></i><span>CDH-集群部署</span></a></div><div class="paginator-post-next"><a href="/2019/05/20/Spark-SparkStream/"><span>SparkStreaming基础-SparkStreaming的基本使用</span><i class="fa fa-chevron-right"></i></a></div></div></nav></footer></div></div></div><aside class="sidebar" id="sidebar"><div class="sidebar-inner"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkStreaming-2"><span class="toc-number">1.</span> <span class="toc-text">
          SparkStreaming-2
        </span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tranform"><span class="toc-number">1.1.</span> <span class="toc-text">
          tranform
        </span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Window"><span class="toc-number">1.2.</span> <span class="toc-text">
          Window
        </span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Output-Operations-on-DStreams-输出算子"><span class="toc-number">1.3.</span> <span class="toc-text">
          Output Operations on DStreams 输出算子
        </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Design-Patterns-for-using-foreachRDD"><span class="toc-number">1.3.1.</span> <span class="toc-text">
          Design Patterns for using foreachRDD
        </span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Other-points-to-remember"><span class="toc-number">1.3.1.0.1.</span> <span class="toc-text">
          Other points to remember
        </span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Input-DStreams-and-Receivers"><span class="toc-number">1.4.</span> <span class="toc-text">
          Input DStreams and Receivers
        </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Basic-Sources"><span class="toc-number">1.4.1.</span> <span class="toc-text">
          Basic Sources
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Advanced-Sources"><span class="toc-number">1.4.2.</span> <span class="toc-text">
          Advanced Sources
        </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka-Source"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">
          Kafka Source
        </span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#添加依赖"><span class="toc-number">1.4.2.1.1.</span> <span class="toc-text">
          添加依赖
        </span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Creating-a-Direct-Stream"><span class="toc-number">1.4.2.1.2.</span> <span class="toc-text">
          Creating a Direct Stream
        </span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Obtaining-Offsets获取偏移量"><span class="toc-number">1.4.2.1.3.</span> <span class="toc-text">
          Obtaining Offsets获取偏移量
        </span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Storing-Offsets存储偏移量"><span class="toc-number">1.4.2.1.4.</span> <span class="toc-text">
          Storing Offsets存储偏移量
        </span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#设置kafka数据的偏移量"><span class="toc-number">1.4.2.1.5.</span> <span class="toc-text">
          设置kafka数据的偏移量
        </span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div></section><!-- ov = overview --><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/avatar2.png" alt="avatar"></div><p class="sidebar-ov-author__p">LRJ</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state__a posts" href="/archives/"><div class="sidebar-ov-state__a--count">40</div><div class="sidebar-ov-state__a--name">Archives</div></a><a class="sidebar-ov-state__a categories" href="/categories/"><div class="sidebar-ov-state__a--count">24</div><div class="sidebar-ov-state__a--name">Categories</div></a><a class="sidebar-ov-state__a tags" href="/tags/"><div class="sidebar-ov-state__a--count">32</div><div class="sidebar-ov-state__a--name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span>You have read </span><span class="sidebar-reading-info-num">0</span></div><div class="sidebar-reading-line"></div></div></div></aside><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright &copy; 2020</span><span class="fa fa-heart footer-icon"></span><span>LRJ.</span></div><div><span>Powered by <a href="http://hexo.io/" title="hexo" target="_blank" rel="noopener">hexo</a></span><span> v4.2.0.</span><span class="separator">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="stun" target="_blank" rel="noopener">stun</a></span><span> v1.7.0.</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="progress"></div></div><div class="back2top" id="back2top"><i class="back2top-icon fa fa-rocket"></i></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=1.7.0"></script><script src="/js/stun-boot.js?v=1.7.0"></script><script src="/js/scroll.js?v=1.7.0"></script><script src="/js/header.js?v=1.7.0"></script><script src="/js/sidebar.js?v=1.7.0"></script></body></html>