[{"title":"如何选择大数据平台落地方案","url":"/2020/03/14/如何选择大数据平台落地方案/","content":"\n这个是跟若泽大数据J哥学习的时候做的笔记,虽然J哥也发了,不过这个是自己的笔记\n\n# 如何选择大数据平台落地方案\n## 机器:\n- 云上\n- IDC机房\n- 公司内部机器\n\n### 云上\n优点:\n- 节省运维成本\n- 对运维要求很低\n- 快速扩容,缩减\n\t\n缺点:\n- 云上机器都是虚拟化,性能-20%\n- 云上服务器不算公司资产,不方便上市\n\n总结:\n- 支付缓慢型\n- 后期持续支付\n- \n- 周期长时,费用最高\n\n> 高速盘: 主要是做系统盘\n> 数据盘: 主要存hdfs数据的\n\n### IDC机房\n优点:\n- 不虚拟化,性能高\n- 算公司资产\n\n缺点:\n- 运维能力要求搞\n\n总结:\n- 开始支付多\n- 后期只需要支付IDC机房托管\n\n\n### 内部机器\n\n优点:\n- 没有IDC托管费用\n\n缺点:\n- 不可靠,园区断电\n- 电费\n- 空调\n- 人工巡检\n- UPS\n\n总结:\n- 创业公司\n- 小团队\n\n## 如何采购服务器\n- 找三家供应商\n- 说需求\n- 推荐报价\n- 说预算\n\n> 服务器供应商:\n> - dell\n> - 浪潮\n> - 华为\n> - 惠普\n> - IBM\n\n## 对比供应商\n- 表格\n- 最终报价单\n- 优缺点\n- 实地考察\n- 综合价格+服务+口碑\n- 汇报领导\n- meet讨论\n- 采购部门询价\n\n> 从开始到确定,最好控制再1-2周,因为内存条的价格有浮动\n\n## \t确定供应商\n- 合同word->法务->boss审核->修改\n- 时间一个月\n- 供应商打印合同->盖章->我们\n- 第一笔钱30%,3-6个月结清\n- 等机器时间约1-2周\n\n## IDC机房选择\n选择三家 报价\n- 电信机房\n- xx\n\n选择\n- 2u服务器,散热性好\n- 2个机架\n- 一个防火墙\n- 两个交换机\n\n坑:\n问IDC供应商,我有xx台服务器,采购的配置,电源功率发给人家,让他算\n\n\n- 重复之前的流程进行确认付款交付\n\n\n## 云专线供应商\n- 定期将云的数据同步到IDC,公网不可靠\n- 云专线光纤直达,20ms\n- 500M的带宽光纤约7000/月,需要买2根,一次操作费1.6w\n\n重复之前的流程进行确认付款交付\n\n## 上架\n\n都准备好之后,运维约时间上架服务器,搭建防火墙,打通网络,熟练工2天左右\n都调试完了之后,重启一次机器,网络重启,格式化\n\n\n## 网络拓扑图\n\n\n","tags":["others"],"categories":["others"]},{"title":"SpringSecurity获取Token源码","url":"/2020/02/20/Spring-SpringSecurity创建Token的源码跟踪/","content":"\n# SpringSecurity-创建AccessToken\n\n\n\n![image-20200320174700329](/images/imageimage-20200320174700329.png)\n\n\n\n- TokenEndPoint 就是判断grant_type到底是什么模式\n- ClientDetailService 判断client_id\n- ClientDetails封装请求的client的信息\n- TokenRequest将ClientDetails封装到请求里,因为Client信息也是token的一部分\n- TokenRequest请求TokenGranter(CompositeTokenGranter,默认),TokenGranter封装了四种授权模式,挑一个模式实现来生成token逻辑\n- Oauth2Request包含了client信息的ToekenRequests\n- Authentication包含了当前认证的用户信息,通过UserDetailsService出来的UserDetails\n- Oauth2Authentication包含了当前是哪个第三方client_id请求哪个用户授权,授权模式是什么,授权的参数是什么\n- AuthorizationServerTokenServices(默认实现DefaultTokenService)\n  - TokenStore 令牌的存取关联\n  - TokenEnhancer 令牌的增强体,令牌生成后,可以加一些东西\n\n\n\n## 创建Token的源码跟踪\n\n### TokenEndPoint\n\n申请token入口\n\n```java\n@RequestMapping(value = \"/oauth/token\", method=RequestMethod.POST)\npublic ResponseEntity<OAuth2AccessToken> postAccessToken(Principal principal, @RequestParam\nMap<String, String> parameters) throws HttpRequestMethodNotSupportedException {\n\n\tif (!(principal instanceof Authentication)) {\n\t\tthrow new InsufficientAuthenticationException(\n\t\t\t\t\"There is no client authentication. Try adding an appropriate authentication filter.\");\n\t}\n\n    //从请求令牌中获取client_id\n\tString clientId = getClientId(principal);\n    //调用ClientDetailsService查询第三方应用的详细信息\n\tClientDetails authenticatedClient = getClientDetailsService().loadClientByClientId(clientId);\n\n    //使用第三方应用的信息创建TokenRequest\n\tTokenRequest tokenRequest = getOAuth2RequestFactory().createTokenRequest(parameters, authenticatedClient);\n\n\tif (clientId != null && !clientId.equals(\"\")) {\n\t\t// Only validate the client details if a client authenticated during this\n\t\t// request.\n\t\tif (!clientId.equals(tokenRequest.getClientId())) {\n\t\t\t// double check to make sure that the client ID in the token request is the same as that in the\n\t\t\t// authenticated client\n\t\t\tthrow new InvalidClientException(\"Given client ID does not match authenticated client\");\n\t\t}\n\t}\n\tif (authenticatedClient != null) {\n\t\toAuth2RequestValidator.validateScope(tokenRequest, authenticatedClient);\n\t}\n    //必须带grant_type\n\tif (!StringUtils.hasText(tokenRequest.getGrantType())) {\n\t\tthrow new InvalidRequestException(\"Missing grant type\");\n\t}\n    //如果是简化模式,不会有grant_type,第一次就发放令牌\n\tif (tokenRequest.getGrantType().equals(\"implicit\")) {\n\t\tthrow new InvalidGrantException(\"Implicit grant type not supported from token endpoint\");\n\t}\n\n    //如果是授权码模式请求,先将token请求的scope置空\n    //因为此时只是请求授权码,即使你设置了all,但你也不一定有all的权限\n    //用户的实际scope是在用户授权之后才能决定\n\tif (isAuthCodeRequest(parameters)) {\n\t\t// The scope was requested or determined during the authorization step\n\t\tif (!tokenRequest.getScope().isEmpty()) {\n\t\t\tlogger.debug(\"Clearing scope of incoming token request\");\n\t\t\ttokenRequest.setScope(Collections.<String> emptySet());\n\t\t}\n\t}\n\n    //如果是刷新令牌,令牌有自己的scope,所以重新设置scope\n\tif (isRefreshTokenRequest(parameters)) {\n\t\t// A refresh token has its own default scopes, so we should ignore any added by the factory here.\n\t\ttokenRequest.setScope(OAuth2Utils.parseParameterList(parameters.get(OAuth2Utils.SCOPE)));\n\t}\n\t//生成token\n\tOAuth2AccessToken token = getTokenGranter().grant(tokenRequest.getGrantType(), tokenRequest);\n\tif (token == null) {\n\t\tthrow new UnsupportedGrantTypeException(\"Unsupported grant type: \" + tokenRequest.getGrantType());\n\t}\n\n    //返回token\n\treturn getResponse(token);\n\n}\t\n```\n\n### TokenRequest\n\n封装Client信息成TokenRequest的逻辑\n\n```java\npublic TokenRequest createTokenRequest(Map<String, String> requestParameters, ClientDetails authenticatedClient) {\n\n    //从请求中获取client_id参数\n\tString clientId = requestParameters.get(OAuth2Utils.CLIENT_ID);\n\tif (clientId == null) {\n\t\t// if the clientId wasn't passed in in the map, we add pull it from the authenticated client object\n\t\tclientId = authenticatedClient.getClientId();\n\t}\n\telse {\n        //如果两个client_id不匹配,抛异常\n\t\t// otherwise, make sure that they match\n\t\tif (!clientId.equals(authenticatedClient.getClientId())) {\n\t\t\tthrow new InvalidClientException(\"Given client ID does not match authenticated client\");\n\t\t}\n\t}\n    //获取grant_type\n\tString grantType = requestParameters.get(OAuth2Utils.GRANT_TYPE);\n    //获取scope\n\tSet<String> scopes = extractScopes(requestParameters, clientId);\n\tTokenRequest tokenRequest = new TokenRequest(requestParameters, clientId, scopes, grantType);\n\n\treturn tokenRequest;\n}\n\n//查寻client的scope\nprivate Set<String> extractScopes(Map<String, String> requestParameters, String clientId) {\n    //从参数中获取scope\n\tSet<String> scopes = OAuth2Utils.parseParameterList(requestParameters.get(OAuth2Utils.SCOPE));\n    //调用clientDetailsService获取client信息\n\tClientDetails clientDetails = clientDetailsService.loadClientByClientId(clientId);\n\n    //如果参数没带,直接把数据库的scope设置一下\n\tif ((scopes == null || scopes.isEmpty())) {\n\t\t// If no scopes are specified in the incoming data, use the default values registered with the client\n\t\t// (the spec allows us to choose between this option and rejecting the request completely, so we'll take the\n\t\t// least obnoxious choice as a default).\n\t\tscopes = clientDetails.getScope();\n\t}\n    //检查scope\n\tif (checkUserScopes) {\n\t\tscopes = checkUserScopes(scopes, clientDetails);\n\t}\n\treturn scopes;\n}\n\n\nprivate Set<String> checkUserScopes(Set<String> scopes, ClientDetails clientDetails) {\n\tif (!securityContextAccessor.isUser()) {\n\t\treturn scopes;\n\t}\n\tSet<String> result = new LinkedHashSet<String>();\n\tSet<String> authorities = AuthorityUtils.authorityListToSet(securityContextAccessor.getAuthorities());\n    //如果用户具有这些scope的权限或者角色,添加到scope\n\tfor (String scope : scopes) {\n\t\tif (authorities.contains(scope) || authorities.contains(scope.toUpperCase())\n\t\t\t\t|| authorities.contains(\"ROLE_\" + scope.toUpperCase())) {\n\t\t\tresult.add(scope);\n\t\t}\n\t}\n\treturn result;\n}\n\n```\n\n### TokenGranter\n\n根据4种模式选择token的生成方式\n\n```java\nprivate final List<TokenGranter> tokenGranters;\n....\npublic OAuth2AccessToken grant(String grantType, TokenRequest tokenRequest) {\n    //遍历TokenGranter集合(4种模式+1个Refresh),找到符合的就可以了\n\tfor (TokenGranter granter : tokenGranters) {\n\t\tOAuth2AccessToken grant = granter.grant(grantType, tokenRequest);\n\t\tif (grant!=null) {\n\t\t\treturn grant;\n\t\t}\n\t}\n\treturn null;\n}\npublic OAuth2AccessToken grant(String grantType, TokenRequest tokenRequest) {\n\n\tif (!this.grantType.equals(grantType)) {\n\t\treturn null;\n\t}\n\t\n\tString clientId = tokenRequest.getClientId();\n\tClientDetails client = clientDetailsService.loadClientByClientId(clientId);\n\t//校验请求授权码的模式\n\tvalidateGrantType(grantType, client);\n\t\n\tlogger.debug(\"Getting access token for: \" + clientId);\n\n\treturn getAccessToken(client, tokenRequest);\n\n}\nprotected void validateGrantType(String grantType, ClientDetails clientDetails) {\n\tCollection<String> authorizedGrantTypes = clientDetails.getAuthorizedGrantTypes();\n\t//如果请求授权码的模式和client_id配置的请求模式不一样,抛异常\n\tif (authorizedGrantTypes != null && !authorizedGrantTypes.isEmpty()\n\t\t\t&& !authorizedGrantTypes.contains(grantType)) {\n\t\tthrow new InvalidClientException(\"Unauthorized grant type: \" + grantType);\n\t}\n}\n```\n\n![image-20200320182932444](/images/imageimage-20200320182932444.png)\n\n\n\n### 封装TokenRequest\n\n```java\nprotected OAuth2AccessToken getAccessToken(ClientDetails client, TokenRequest tokenRequest) {\n\t//getOAuth2Authentication根据不同的授权码请求方式,读取相对应的信息\n\t//例如,授权码模式是先把scope返回去,当带scope来请求token的时候,把之前的用户信息读出来\n\t//而密码模式就直接读取用户名密码,生成token\n\treturn tokenServices.createAccessToken(getOAuth2Authentication(client, tokenRequest));\n}\n\nprotected OAuth2Authentication getOAuth2Authentication(ClientDetails client, TokenRequest tokenRequest) {\n\t//OAuth2Authentication中封装了TokenRequest\n\t//先创建TokenRequest:主要是去掉敏感的password和client_secret\n\t//添加grant_type参数,以便在后续的OAuth2Request中可以获取到\n\tOAuth2Request storedOAuth2Request = requestFactory.createOAuth2Request(client, tokenRequest);\n\n\t//创建OAuth2Authentication\n\treturn new OAuth2Authentication(storedOAuth2Request, null);\n}\n\npublic OAuth2Request createOAuth2Request(ClientDetails client) {\n\tMap<String, String> requestParameters = getRequestParameters();\n\tHashMap<String, String> modifiable = new HashMap<String, String>(requestParameters);\n\t// Remove password if present to prevent leaks\n\tmodifiable.remove(\"password\");\n\tmodifiable.remove(\"client_secret\");\n\t// Add grant type so it can be retrieved from OAuth2Request\n\tmodifiable.put(\"grant_type\", grantType);\n\treturn new OAuth2Request(modifiable, client.getClientId(), client.getAuthorities(), true, this.getScope(),\n\t\t\tclient.getResourceIds(), null, null, null);\n}\n```\n\n### 创建AccessToken\n\n```java\npublic OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException {\n\n\t//根据token的存储方式,取出之前的token\n\tOAuth2AccessToken existingAccessToken = tokenStore.getAccessToken(authentication);\n\tOAuth2RefreshToken refreshToken = null;\n\tif (existingAccessToken != null) {\n\t\t//如果token过期了,直接移除refreshToken和accessToken\n\t\tif (existingAccessToken.isExpired()) {\n\t\t\tif (existingAccessToken.getRefreshToken() != null) {\n\t\t\t\trefreshToken = existingAccessToken.getRefreshToken();\n\t\t\t\t// The token store could remove the refresh token when the\n\t\t\t\t// access token is removed, but we want to\n\t\t\t\t// be sure...\n\t\t\t\ttokenStore.removeRefreshToken(refreshToken);\n\t\t\t}\n\t\t\ttokenStore.removeAccessToken(existingAccessToken);\n\t\t}\n\t\telse {\n\t\t\t//没过期,可能你又换了中方式来请求token,直接把之前的token重新存储一下\n\t\t\t// Re-store the access token in case the authentication has changed\n\t\t\ttokenStore.storeAccessToken(existingAccessToken, authentication);\n\t\t\treturn existingAccessToken;\n\t\t}\n\t}\n\n\t// Only create a new refresh token if there wasn't an existing one\n\t// associated with an expired access token.\n\t// Clients might be holding existing refresh tokens, so we re-use it in\n\t// the case that the old access token\n\t// expired.\n\tif (refreshToken == null) {\n\t\t//刷新token没有,创建一个\n\t\trefreshToken = createRefreshToken(authentication);\n\t}\n\t// But the refresh token itself might need to be re-issued if it has\n\t// expired.\n\telse if (refreshToken instanceof ExpiringOAuth2RefreshToken) {\n\t\t//如果刷新token过期了,重新颁发一个\n\t\tExpiringOAuth2RefreshToken expiring = (ExpiringOAuth2RefreshToken) refreshToken;\n\t\tif (System.currentTimeMillis() > expiring.getExpiration().getTime()) {\n\t\t\trefreshToken = createRefreshToken(authentication);\n\t\t}\n\t}\n\t//创建accessToken\n\tOAuth2AccessToken accessToken = createAccessToken(authentication, refreshToken);\n\t//存储token\n\ttokenStore.storeAccessToken(accessToken, authentication);\n\t// In case it was modified\n\t//存储刷新token防止修改\n\trefreshToken = accessToken.getRefreshToken();\n\tif (refreshToken != null) {\n\t\ttokenStore.storeRefreshToken(refreshToken, authentication);\n\t}\n\treturn accessToken;\n\n}\n\n//创建accessToken\nprivate OAuth2AccessToken createAccessToken(OAuth2Authentication authentication, OAuth2RefreshToken refreshToken) {\n\t//accessToken的创建逻辑就是使用UUID,设置过期时间和刷新token\n\tDefaultOAuth2AccessToken token = new DefaultOAuth2AccessToken(UUID.randomUUID().toString());\n\tint validitySeconds = getAccessTokenValiditySeconds(authentication.getOAuth2Request());\n\tif (validitySeconds > 0) {\n\t\ttoken.setExpiration(new Date(System.currentTimeMillis() + (validitySeconds * 1000L)));\n\t}\n\ttoken.setRefreshToken(refreshToken);\n\ttoken.setScope(authentication.getOAuth2Request().getScope());\n\t//如果配置了token增强,则对token调用增强\n\treturn accessTokenEnhancer != null ? accessTokenEnhancer.enhance(token, authentication) : token;\n}\n\n//创建refreshToken\nprivate OAuth2RefreshToken createRefreshToken(OAuth2Authentication authentication) {\n\tif (!isSupportRefreshToken(authentication.getOAuth2Request())) {\n\t\treturn null;\n\t}\n\t//刷新令牌的创建,直接是UUID,设置过期时间偏移一下\n\tint validitySeconds = getRefreshTokenValiditySeconds(authentication.getOAuth2Request());\n\tString value = UUID.randomUUID().toString();\n\tif (validitySeconds > 0) {\n\t\treturn new DefaultExpiringOAuth2RefreshToken(value, new Date(System.currentTimeMillis()\n\t\t\t\t+ (validitySeconds * 1000L)));\n\t}\n\treturn new DefaultOAuth2RefreshToken(value);\n}\n\n```\n\n","tags":["Spring","SpringSecurity"],"categories":["Spring"]},{"title":"SpringCloud概述","url":"/2020/02/11/Spring-SpringCloud基本使用/","content":"# SpringCloud\n\n## SpringCloud概述\n\n### 官网\n\n- [官网](https://spring.io/projects/spring-cloud)\n\n### 主要功能\n![img](/images/image/image-20200207192330288.png)\n\n\n\n### 常用子项目\n\n![img](/images/image/image-20200207192553245.png)\n\n### 版本与兼容\n\n- SpringCloud的版本命名\n  - **版本命名**\n\n    SpringCloud的版本,前半部分(如Hoxton,Greenwich),意思是发布列车(ReleaseTrain),以伦敦地铁的站名命名,因为SpringCloud有很多的子项目,每个项目都有自己的版本管理,按照发布顺序以A,B,C等为首字母依次命名,已经发布的版本顺序为:\n\n  `Angel -> Brixton -> Camden -> Dalston -> Edgware -> Finchley -> Greenwich -> Hoxton`\n  ![img](/images/image/londontuberail-1.png)\n\n\n\n  后半部分(如SR,SR1,SR2),意思是服务发布(ServiceRelease),即重大Bug修复\n  - **版本发布流程**\n\n  `SNAPSHOT -> Mx -> RELEASE -> SRx`,其中x就是一些数字序号,例如M1,M2,SR1,SR2.SNAPSHOT为快照版本(开发版本),Mx为里程碑版本,此时并不是正式版本,但是已经接近正式版,经过多个版本迭代之后,发布第一个RELEASE版本,正式版本;在RELEASE版本之后如果有重大bug修复就会发布SR版本\n\n| Hoxton SR1 **CURRENT** **GA** | [ Reference Doc.](https://cloud.spring.io/spring-cloud-static/Hoxton.SR1/reference/html/spring-cloud.html) |\n| ----------------------------- | ------------------------------------------------------------ |\n| Hoxton **SNAPSHOT**           | [ Reference Doc.](https://spring.io/projects/spring-cloud)   |\n| Greenwich SR5 **GA**          | [ Reference Doc.](https://cloud.spring.io/spring-cloud-static/Greenwich.SR5/) |\n| Greenwich **SNAPSHOT**        | [ Reference Doc.](https://spring.io/projects/spring-cloud)   |\n\n----\n\n- SpringCloud的版本生命周期\n  - **版本发布规划**\n\n    https://github.com/spring-cloud/spring-cloud-release/milestones\n\n  - **版本发布记录**\n\n    https://github.com/spring-cloud/spring-cloud-release/releases\n\n  - **版本终止声明**\n\n    https://spring.io/projects/spring-cloud#overview\n\n- SpringBoot与SpringCloud的兼容性\n  - 版本兼容性非常重要\n    https://spring.io/projects/spring-cloud#overview\n\n| Release Train | Boot Version |\n| ------------- | ------------ |\n| Hoxton        | 2.2.x        |\n| Greenwich     | 2.1.x        |\n| Finchley      | 2.0.x        |\n| Edgware       | 1.5.x        |\n| Dalston       | 1.5.x        |\n\n- 生产环境如何选择选择\n  - 坚决不适用非稳定版本\n  - 坚决不适用end-of-life版本\n  - 尽量使用最新版本\n    - RELEASE版本可以观望/调研,因为是第一个正式版,并没有在生产上得以广泛应用\n    - SR2之后可以大规模使用\n\n### 版本选择\n\n- SpringCloud Hoxton SR1\n- SpringBoot 2.2.4.RELEASE\n\n```xml\n<parent>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>2.2.4.RELEASE</version>\n    <relativePath/> <!-- lookup parent from repository -->\n</parent> \n\n<dependencyManagement>\n    <dependencies>\n        <!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-dependencies -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-dependencies</artifactId>\n            <version>Hoxton.SR1</version>\n            <type>pom</type>\n            <scope>import</scope>\n        </dependency>\n    </dependencies>\n</dependencyManagement>\n```\n\n检查项目是否能运行\n\n```bash\nmvn clean install -U\n```\n\n## SpringCloud服务注册与发现\n\n- 使得服务消费者总能找到服务提供者\n\n### Consul单机版安装\n\n#### Consul下载\n\n- 下载Consoule https://releases.hashicorp.com/consul/1.6.3/consul_1.6.3_linux_amd64.zip\n\n#### 需要的端口\n\n| Use                                                          | Default Ports    |\n| :----------------------------------------------------------- | :--------------- |\n| DNS: The DNS server (TCP and UDP)                            | 8600             |\n| HTTP: The HTTP API (TCP Only)                                | 8500             |\n| HTTPS: The HTTPs API                                         | disabled (8501)* |\n| gRPC: The gRPC API                                           | disabled (8502)* |\n| LAN Serf: The Serf LAN port (TCP and UDP)                    | 8301             |\n| Wan Serf: The Serf WAN port TCP and UDP)                     | 8302             |\n| server: Server RPC address (TCP Only)                        | 8300             |\n| Sidecar Proxy Min: Inclusive min port number to use for automatically assigned sidecar service registrations. | 21000            |\n| Sidecar Proxy Max: Inclusive max port number to use for automatically assigned sidecar service registrations. | 21255            |\n\n检查端口是否被占用的方法\n\n```shell\nWindows:\n# 如果没有结果说明没有被占用\nnetstat -ano| findstr \"8500\"\n\nLinux:\n# 如果没有结果说明没有被占用\nnetstat -antp |grep 8500\n\nmacOS:\n# 如果没有结果说明没有被占用\nnetstat -ant | grep 8500\n或\nlsof -i:8500\n```\n\n#### 安装和启动\n\n- 解压\n\n```shell\n./consul agent -dev -client 0.0.0.0\n```\n\n- 严重是否成功\n\n```shell\n./consul -v\n```\n\n- 访问Consul首页`localhost:8500`\n\n**启动参数**\n\n- -ui 开启ui\n- -client 让consul拥有client功能,接受服务注册;0.0.0.0允许任意ip注册,不写只能使用localhost连接\n- -dev 以开发模式运行consul\n\n### 整合Consul\n\n- 添加依赖\n\n  ```xml\n  <dependency>\n      <groupId>org.springframework.cloud</groupId>\n      <artifactId>spring-cloud-starter-consul-discovery</artifactId>\n  </dependency>\n  ```\n\n  \n\n- 配置\n\n  ```yml\n  spring:\n   application:\n      # 指定注册到consul的服务名称,分隔符不能是下划线\n      # 如果服务发现组件是Consul,会强制转换成中划线,导致找不到服务\n      # 如果服务发现组件是Ribbon,则因为Ribbon的问题(把默认名称当初虚拟主机名,而虚拟主机名不能用下划线),会造成微服务之间无法调用\n      name: micro-service-user\n  \n    cloud:\n      consul:\n        host: 192.168.238.128\n        port: 8500\n  ```\n\n- 启动,检查consul ui的服务上线情况\n\n### Consul健康检查\n![img](/images/image/image-20200207231837317.png)\n\n\n- 添加健康检查依赖\n\n```xml\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n\n- 配置\n\n  ```yml\n  management:\n    endpoints:\n      web:\n        exposure:\n          include: '*'\n  ```\n\n  \n\n- 端点\n\n  http://localhost:8080/actuator 查看端点\n\n  http://localhost:8080/actuator/health 健康检查\n\n  添加详情配置,可以检查详细的健康情况\n\n  ```yml\n  management:\n      endpoint:\n        health:\n          show-details: always\n  ```\n  \n  ![img](/images/image/image-20200207233909996.png)\n\n\n\n\n- 简单研究一下健康检查的源码\n\n  以磁盘检查的为例\n\n  健康检查的类都继承了`AbstractHealthIndicator`抽象类,而`AbstractHealthIndicator`实现了`HealthIndicator`接口,所有健康检查实现类都必须实现`doHealthCheck(Health.Builder builder)`方法\n\n![img](/images/image/image-20200207235333557.png)\n\n\n```java\npublic class DiskSpaceHealthIndicator extends AbstractHealthIndicator {\n\n\tprivate static final Log logger = LogFactory.getLog(DiskSpaceHealthIndicator.class);\n\n\tprivate final File path;\n\n\tprivate final DataSize threshold;\n\n\t/**\n\t * Create a new {@code DiskSpaceHealthIndicator} instance.\n\t * @param path the Path used to compute the available disk space\n\t * @param threshold the minimum disk space that should be available\n\t */\n\tpublic DiskSpaceHealthIndicator(File path, DataSize threshold) {\n\t\tsuper(\"DiskSpace health check failed\");\n\t\tthis.path = path;\n\t\tthis.threshold = threshold;\n\t}\n\n\t@Override\n\tprotected void doHealthCheck(Health.Builder builder) throws Exception {\n        //获取可用的空间字节数\n\t\tlong diskFreeInBytes = this.path.getUsableSpace();\n        //如果可用的字节数大于预留字节数阈值则认为是健康的,设置status为UP\n\t\tif (diskFreeInBytes >= this.threshold.toBytes()) {\n\t\t\tbuilder.up();\n\t\t}\n\t\telse {\n            //否则任务是不健康的,设置status为DOWN\n\t\t\tlogger.warn(LogMessage.format(\"Free disk space below threshold. Available: %d bytes (threshold: %s)\",\n\t\t\t\t\tdiskFreeInBytes, this.threshold));\n\t\t\tbuilder.down();\n\t\t}\n        //输出总空间,可用空间和预留阈值\n\t\tbuilder.withDetail(\"total\", this.path.getTotalSpace()).withDetail(\"free\", diskFreeInBytes)\n\t\t\t\t.withDetail(\"threshold\", this.threshold.toBytes());\n\t}\n\n}\n\n//这个获取可用字节数还是挺好的,直接利用了File提供的方法\npublic long getUsableSpace() {\n    SecurityManager sm = System.getSecurityManager();\n    if (sm != null) {\n        sm.checkPermission(new RuntimePermission(\"getFileSystemAttributes\"));\n        sm.checkRead(path);\n    }\n    if (isInvalid()) {\n        return 0L;\n    }\n    //fs是默认的文件系统FileSystem fs = DefaultFileSystem.getFileSystem();\n    return fs.getSpace(this, FileSystem.SPACE_USABLE);\n}\n\n\n```\n\n健康检查使用了建造者模式,对于不同的健康指标非常方便,值得学习\n![img](/images/image/image-20200208000623044.png)\n\n\n- 整合Consul和SpringCloud的actuator\n\n  修改配置\n\n  ```yml\n  spring:\n      cloud:\n        consul:\n          host: 192.168.238.128\n          port: 8500\n          discovery:\n            health-check-path: /actuator/health\n  ```\n\n这样启动之后,再检查consul ui就可以发现没有红色的叉了\n![img](/images/image/image-20200208001532048.png)\n\n\n其他的健康检查配置\n![img](/images/image/image-20200208001745953.png)\n\n\n### 注册课程微服务到Consul\n\n- 添加依赖\n\n```xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-consul-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n\n- 配置\n\n```yml\nspring:\n  datasource:\n    url: jdbc:mysql://192.168.238.128:3306/ms?serverTimezone=GMT%2B8&characterEncoding=utf8&useSSL=false\n    hikari:\n      username: lrj\n      password: lu11221015\n      driver-class-name: com.mysql.cj.jdbc.Driver\n# JPA配置\n  jpa:\n    hibernate:\n      ddl-auto: update\n    show-sql: true\n  application:\n    name: micro-service-class\n  cloud:\n    consul:\n      host: 192.168.238.128\n      port: 8500\n      discovery:\n        health-check-path: /actuator/health\n# 暴露所有的actuator端点\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: '*'\n  # 开启健康检查详细信息\n  endpoint:\n    health:\n      show-details: always\nserver:\n  port: 8081\n```\n\n- 重构用户微服务\n\n```java\n@RestController\n@RequestMapping(\"user\")\npublic class UserController {\n    @Resource\n    private UserService userService;\n\n    @Resource\n    private DiscoveryClient discoveryClient;\n\n    @GetMapping(\"{id}\")\n    public Object findUserById(@PathVariable(\"id\") Integer id) {\n        return userService.findUserById(id);\n    }\n\n    @GetMapping(\"discoveryTest\")\n    public Object discoveryTest() {\n        return discoveryClient.getInstances(\"micro-service-class\");\n    }\n}\n\n```\n\n- 访问端点,可以发现不需要指定课程微服务的主机和端口就可以拿到相关信息,实现了服务发现\n![img](/images/image/image-20200208003543107.png)\n\n\n### 重构课程微服务\n\n- 原来\n\n  ```java\n  @Service\n  public class LessonServiceImpl implements LessonService {\n      @Resource\n      private LessonRepository lessonRepository;\n      @Resource\n      private LessonUserRepository lessonUserRepository;\n      @Resource\n      private RestTemplate restTemplate;\n  \n  \n      @Override\n      public Lesson buyById(Integer id) {\n          // 1. 根据课程id查询课程\n          Lesson lesson = lessonRepository.findById(id).orElseThrow(() -> new IllegalArgumentException(\"该课程不存在\"));\n          //根据课程查询是否已经购买过\n          LessonUser lessonUser = lessonUserRepository.findByLessonId(id);\n          if (lessonUser != null) {\n              return lesson;\n          }\n          //todo 2.登录之后获取userId\n          String userId = \"1\";\n          // 3. 如果没有购买过,查询用户余额\n          UserDTO userDTO = restTemplate.getForObject(\"http://localhost:8080/user/{userId}\", UserDTO.class, userId);\n          if (userDTO != null && userDTO.getMoney() != null &&\n                  userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() < 0) {\n              throw new IllegalArgumentException(\"余额不足\");\n          }\n          //4. 购买逻辑\n          //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录\n          return lesson;\n      }\n  \n  }\n  ```\n\n  这个写死了主机地址,无法动态获取微服务路径\n\n- 重构\n\n  ```java\n  public class LessonServiceImpl implements LessonService {\n      @Resource\n      private LessonRepository lessonRepository;\n      @Resource\n      private LessonUserRepository lessonUserRepository;\n      @Resource\n      private DiscoveryClient discoveryClient;\n      @Resource\n      private RestTemplate restTemplate;\n  \n      @Override\n      public Lesson buyById(Integer id) {\n          // 1. 根据课程id查询课程\n          Lesson lesson = lessonRepository.findById(id).orElseThrow(() -> new IllegalArgumentException(\"该课程不存在\"));\n          //根据课程查询是否已经购买过\n          LessonUser lessonUser = lessonUserRepository.findByLessonId(id);\n          if (lessonUser != null) {\n              return lesson;\n          }\n          //todo 2.登录之后获取userId\n          String userId = \"1\";\n          List<ServiceInstance> instances = discoveryClient.getInstances(\"micro-service-user\");\n          if (instances != null && !instances.isEmpty()) {\n              //todo 需要改进,如果存在多个实例,需要考虑负载均衡\n              ServiceInstance instance = instances.get(0);\n              URI uri = instance.getUri();\n              UserDTO userDTO = restTemplate.getForObject(uri + \"/user/{userId}\", UserDTO.class, userId);\n              if (userDTO != null && userDTO.getMoney() != null &&\n                      userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() < 0) {\n                  throw new IllegalArgumentException(\"余额不足\");\n              }\n              //4. 购买逻辑\n              //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录\n              return lesson;\n          }\n          throw new IllegalArgumentException(\"用户微服务异常,无法购买课程\");\n      }\n  \n  }\n  \n  ```\n\n  可以动态的获取到用户微服务的地址,请求正常\n  ![img](/images/image/image-20200208004838563.png)\n\n\n### 元数据\n\nConsul是没有元数据的概念的,所以SpringCloud做了个适配,在consul下设置tags作为元数据.\n\n元数据可以对微服务添加描述,标识,例如机房在哪里,这样可以进行就近判断,或者当就近机房不可用时才检查远程机房,当两者都不可用时才认为服务不可用等实现容灾或者跨机房\n\n- 配置\n\n  ```yml\n  spring:\n    cloud:\n      consul:\n        host: 192.168.238.128\n        port: 8500\n        discovery:\n          health-check-path: /actuator/health\n          tags: JiFang=Beijing,JiFang=Shanghai\n  ```\n\n- 实现机房选择\n\n  ```java\n  @GetMapping(\"discoveryTest\")\n  public Object discoveryTest() {\n      List<ServiceInstance> instances = discoveryClient.getInstances(\"micro-service-class\");\n      if (instances != null) {\n          List<ServiceInstance> shanghaiInstances = instances.stream()\n                  .filter(s -> s.getMetadata().containsKey(\"Shanghai\")).collect(Collectors.toList());\n          if (!shanghaiInstances.isEmpty()) {\n              return shanghaiInstances;\n          }\n      }\n      return instances;\n  }\n  ```\n\n\n\n","tags":["Spring","SpringCloud"],"categories":["Spring"]},{"title":"Spring解析xml成BeanDefinition的过程","url":"/2020/01/26/Spring-Bean的解析过程/","content":"# Spring Bean的解析过程\n\n## xml文件的读取\n\n从我们的入口开始\n\n```java\n@Test\npublic void testXml() {\n    ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\");\n    Student student = context.getBean(Student.class);\n    System.out.println(student.getUsername());\n}\n```\n\n先进入ClassPathXmlApplicationContext的构造器\n\n```java\npublic ClassPathXmlApplicationContext(String configLocation) throws BeansException {\n   this(new String[] {configLocation}, true, null);\n}\n```\n\n继续调用另一个构造器\n\n```java\npublic ClassPathXmlApplicationContext(\n      String[] configLocations, boolean refresh, @Nullable ApplicationContext parent)\n      throws BeansException {\n   super(parent);\n   //创建解析器，解析configLocations\n   setConfigLocations(configLocations);\n   if (refresh) {\n      refresh();\n   }\n}\n```\n\n这个refresh()方法是核心方法,点进去\n\n```java\npublic void refresh() throws BeansException, IllegalStateException {\n   synchronized (this.startupShutdownMonitor) {\n      //为容器初始化做准备，重要程度：0\n      // Prepare this context for refreshing.\n      prepareRefresh();\n\n      /*\n         重要程度：5\n        1、创建BeanFactory对象\n      * 2、xml解析\n      *  传统标签解析：bean、import等\n      *  自定义标签解析 如：<context:component-scan base-package=\"com.xiangxue.jack\"/>\n      *  自定义标签解析流程：\n      *     a、根据当前解析标签的头信息找到对应的namespaceUri\n      *     b、加载spring所以jar中的spring.handlers文件。并建立映射关系\n      *     c、根据namespaceUri从映射关系中找到对应的实现了NamespaceHandler接口的类\n      *     d、调用类的init方法，init方法是注册了各种自定义标签的解析类\n      *     e、根据namespaceUri找到对应的解析类，然后调用paser方法完成标签解析\n      *\n      * 3、把解析出来的xml标签封装成BeanDefinition对象\n      * */\n      // Tell the subclass to refresh the internal bean factory.\n      ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();\n\t\t\n       ....\n   }\n}\n```\n\n继续看obtainFreshBeanFactory()方法\n\n```java\nprotected ConfigurableListableBeanFactory obtainFreshBeanFactory() {\n   /**\n    * 核心方法，必须读，重要程度：5\n    * 这里使用了模板设计模式,Spring使用最多的设计模式,父类定义了模板,子类具体实现\n    * 其实反过来看,AbstractApplicationContext的refresh()同样也是定义了模板,onFresh()方法交给子类去实现\n    * 例如SpringBoot中嵌入式Tomcat启动就是覆写了onFresh()方法\n    * @see AbstractApplicationContext#onRefresh()\n    * */\n   refreshBeanFactory();\n   return getBeanFactory();\n}\n\n\t/**\n\t * 因为ClassPathXmlApplicationContext是AbstractRefreshableApplicationContext的子类\n\t * 所以跳转到AbstractRefreshableApplicationContext\n\t * @see AbstractRefreshableApplicationContext#refreshBeanFactory()\n\t * @throws BeansException        if initialization of the bean factory failed\n\t * @throws IllegalStateException if already initialized and multiple refresh\n\t *                               attempts are not supported\n\t *                               <p>\n\t * \n\t */\nprotected abstract void refreshBeanFactory() throws BeansException, IllegalStateException;\n\n```\n\n跳转到AbstractRefreshableApplicationContext的refreshBeanFactory()\n\n```java\nprotected final void refreshBeanFactory() throws BeansException {\n\n   //如果BeanFactory不为空，则清除BeanFactory和里面的实例\n   if (hasBeanFactory()) {\n      destroyBeans();\n      closeBeanFactory();\n   }\n   try {\n      //创建DefaultListableBeanFactory\n      //BeanFactory实例工厂,不管什么实例都可以从BeanFactory获取到\n      DefaultListableBeanFactory beanFactory = createBeanFactory();\n      beanFactory.setSerializationId(getId());\n\n      //设置是否可以循环依赖 allowCircularReferences\n      //是否允许使用相同名称重新注册不同的bean实现.\n      customizeBeanFactory(beanFactory);\n\n      /**\n       * 解析xml，并把xml中的标签封装成BeanDefinition对象\n       * 因为ClassPathXmlApplication是继承自 AbstractXmlApplicationContext\n       * 所以进入AbstractXmlApplicationContext,又是一个模板\n       * @see AbstractXmlApplicationContext#loadBeanDefinitions(org.springframework.beans.factory.support.DefaultListableBeanFactory)\n       */\n      loadBeanDefinitions(beanFactory);\n      synchronized (this.beanFactoryMonitor) {\n         this.beanFactory = beanFactory;\n      }\n   } catch (IOException ex) {\n      throw new ApplicationContextException(\"I/O error parsing bean definition source for \" + getDisplayName(), ex);\n   }\n}\n\nprotected abstract void loadBeanDefinitions(DefaultListableBeanFactory beanFactory)\n\t\t\tthrows BeansException, IOException;\n```\n\n继续看AbstractXmlApplicationContext的loadBeanDefinitions(...)\n\nxml的解析交给了XmlBeanDefinitionReader来解析\n\n```java\nprotected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException {\n   // Create a new XmlBeanDefinitionReader for the given BeanFactory.\n   //创建xml的解析器，这里是一个委托模式\n   //xml的解析工作,委托给XmlBeanDefinitionReader来解析\n   XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory);\n\n....\n    \n   //主要看这个方法  重要程度 5\n   loadBeanDefinitions(beanDefinitionReader);\n}\n```\n\n继续看loadBeanDefinitions(...)\n\n```java\nprotected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException {\n   Resource[] configResources = getConfigResources();\n   if (configResources != null) {\n      reader.loadBeanDefinitions(configResources);\n   }\n   //获取需要加载的xml配置文件\n   String[] configLocations = getConfigLocations();\n   if (configLocations != null) {\n       //委托给reader来解析xml\n      reader.loadBeanDefinitions(configLocations);\n   }\n}\n```\n\n点进去\n\n```java\n@Override\npublic int loadBeanDefinitions(String location) throws BeanDefinitionStoreException {\n   return loadBeanDefinitions(location, null);\n}\n\npublic int loadBeanDefinitions(String location, @Nullable Set<Resource> actualResources) throws BeanDefinitionStoreException {\n    ResourceLoader resourceLoader = getResourceLoader();\n    if (resourceLoader == null) {\n        throw new BeanDefinitionStoreException(\n            \"Cannot load bean definitions from location [\" + location + \"]: no ResourceLoader available\");\n    }\n\n    if (resourceLoader instanceof ResourcePatternResolver) {\n        // Resource pattern matching available.\n        try {\n            //把字符串类型的xml文件路径，形如：classpath*:user/**/*-context.xml,转换成Resource对象类型，其实就是用流\n            //的方式加载配置文件，然后封装成Resource对象，不重要，可以不看\n            Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location);\n\n            //主要看这个方法 ** 重要程度 5\n            int count = loadBeanDefinitions(resources);\n           ....\n            return count;\n        }\n        catch (IOException ex) {\n            throw new BeanDefinitionStoreException(\n                \"Could not resolve bean definition resource pattern [\" + location + \"]\", ex);\n        }\n    }\n    else {\n        // Can only load single resources by absolute URL.\n        Resource resource = resourceLoader.getResource(location);\n        int count = loadBeanDefinitions(resource);\n....\n        return count;\n    }\n}\n```\n\n把xml读出来之后封装成了Resource对象,开始解析Resource\n\n```java\n@Override\npublic int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException {\n   Assert.notNull(resources, \"Resource array must not be null\");\n   int count = 0;\n   for (Resource resource : resources) {\n      //模板设计模式，调用到子类中的方法\n      //又是一个模板,因为委托给了XmlBeanDefinitionReader\n      /**@see org.springframework.beans.factory.xml.XmlBeanDefinitionReader#loadBeanDefinitions(org.springframework.core.io.Resource)*/\n      count += loadBeanDefinitions(resource);\n   }\n   return count;\n}\n```\n\nxml读出来之后,又把Resource封装成带编码的对象,委托给XmlBeanDefinitionReader进行解析\n\n```java\n@Override\npublic int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException {\n   //EncodedResource带编码的对Resource对象的封装\n   //把资源流对象又做了编码的封装\n   return loadBeanDefinitions(new EncodedResource(resource));\n}\n```\n\n再看如何解析Resource的\n\n```java\npublic int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException {\n  ...\n      \n   Set<EncodedResource> currentResources = this.resourcesCurrentlyBeingLoaded.get();\n   if (currentResources == null) {\n      currentResources = new HashSet<>(4);\n      this.resourcesCurrentlyBeingLoaded.set(currentResources);\n   }\n   if (!currentResources.add(encodedResource)) {\n      throw new BeanDefinitionStoreException(\n            \"Detected cyclic loading of \" + encodedResource + \" - check your import definitions!\");\n   }\n   try {\n      //获取Resource对象中的xml文件流对象\n      InputStream inputStream = encodedResource.getResource().getInputStream();\n      try {\n         //InputSource是jdk中的sax xml文件解析对象\n         InputSource inputSource = new InputSource(inputStream);\n         if (encodedResource.getEncoding() != null) {\n            inputSource.setEncoding(encodedResource.getEncoding());\n         }\n         //主要看这个方法 **  重要程度 5\n         return doLoadBeanDefinitions(inputSource, encodedResource.getResource());\n      }\n      finally {\n         inputStream.close();\n      }\n   }\n   catch (IOException ex) {\n    ...\n   }\n   finally {\n      currentResources.remove(encodedResource);\n      if (currentResources.isEmpty()) {\n         this.resourcesCurrentlyBeingLoaded.remove();\n      }\n   }\n}\n```\n\n把InputStream流对象从Resouce中读出来,封装成InputSource对象\n\n```java\nprotected int doLoadBeanDefinitions(InputSource inputSource, Resource resource)\n      throws BeanDefinitionStoreException {\n\n   try {\n      //把inputSource 封装成Document文件对象，这是jdk的API\n      Document doc = doLoadDocument(inputSource, resource);\n\n      //主要看这个方法，根据解析出来的document对象，拿到里面的标签元素封装成BeanDefinition\n      int count = registerBeanDefinitions(doc, resource);\n      if (logger.isDebugEnabled()) {\n         logger.debug(\"Loaded \" + count + \" bean definitions from \" + resource);\n      }\n      return count;\n   }\n   catch (BeanDefinitionStoreException ex) {\n ....\n}\n```\n\n把流对象InputSource使用SAX进行解析成Document对象,对Document对象进行解析\n\n```java\npublic int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException {\n    //xml解析成Document之后,又将Document委托给BeanDefinitionDocumentReader来解析Document成BeanDefinition\n    //又来一记委托模式，BeanDefinitionDocumentReader委托这个类进行document的解析\n    BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader();\n    int countBefore = getRegistry().getBeanDefinitionCount();\n    //主要看这个方法，createReaderContext(resource) XmlReaderContext上下文，封装了XmlBeanDefinitionReader对象\n    documentReader.registerBeanDefinitions(doc, createReaderContext(resource));\n    return getRegistry().getBeanDefinitionCount() - countBefore;\n}\n```\n\nDocument委托给BeanDefinitionDocumentReader来解析Document成BeanDefinition\n\n```java\nvoid registerBeanDefinitions(Document doc, XmlReaderContext readerContext)\n      throws BeanDefinitionStoreException;\n\n\n@Override\npublic void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) {\n    this.readerContext = readerContext;\n    //主要看这个方法，把root节点传进去\n    doRegisterBeanDefinitions(doc.getDocumentElement());\n}\n\nprotected void doRegisterBeanDefinitions(Element root) {\n    BeanDefinitionParserDelegate parent = this.delegate;\n    this.delegate = createDelegate(getReaderContext(), root, parent);\n...\n    //又是模板,冗余设计,空实现\n    preProcessXml(root);\n\n    //主要看这个方法，标签具体解析过程\n    parseBeanDefinitions(root, this.delegate);\n    postProcessXml(root);\n\n    this.delegate = parent;\n}\n```\n\n把Document的根传进去,开始解析Document\n\n```java\nprotected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) {\n   if (delegate.isDefaultNamespace(root)) {\n      //获取根节点下的所有子节点\n      //遍历所有子节点,依次解析\n      NodeList nl = root.getChildNodes();\n      for (int i = 0; i < nl.getLength(); i++) {\n         Node node = nl.item(i);\n         if (node instanceof Element) {\n            Element ele = (Element) node;\n            if (delegate.isDefaultNamespace(ele)) {\n               //默认标签解析,import,alias,bean,beans\n               parseDefaultElement(ele, delegate);\n            }\n            else {\n               //自定义标签解析,委托给BeanDefinitionParserDelegate来解析\n               //context:component-scan等,使用了namespaceUri\n               delegate.parseCustomElement(ele);\n            }\n         }\n      }\n   }\n   else {\n      delegate.parseCustomElement(root);\n   }\n}\n```\n\n标签的解析分为默认标签(包括import,alias,bean,beans)和自定义标签(如context:componet-scan,mvc:annotation-drive等,这类带前缀的标签需要namespaceUri来指定实现,使用了SPI思想)\n\n## 默认标签的解析\n\n先看默认标签的解析\n\n```java\nprivate void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) {\n   //import标签解析  重要程度 1 ，可看可不看\n   if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) {\n      importBeanDefinitionResource(ele);\n   }\n   //alias标签解析 别名标签  重要程度 1 ，可看可不看\n   else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) {\n      processAliasRegistration(ele);\n   }\n   //bean标签，重要程度  5，必须看\n   else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) {\n      processBeanDefinition(ele, delegate);\n   }\n   else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) {\n      // recurse\n      doRegisterBeanDefinitions(ele);\n   }\n}\n```\n\n核心方法processBeanDefinition(...)\n\n```java\nprotected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) {\n   //重点看这个方法，重要程度 5 ，解析document，封装成BeanDefinition\n   BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele);\n   if (bdHolder != null) {\n\n      //该方法功能不重要，设计模式重点看一下，装饰者设计模式，加上SPI设计思想\n      bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder);\n      try {\n\n         //完成document到BeanDefinition对象转换后，对BeanDefinition对象进行缓存注册\n         // Register the final decorated instance.\n         BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry());\n      }\n      catch (BeanDefinitionStoreException ex) {\n         getReaderContext().error(\"Failed to register bean definition with name '\" +\n               bdHolder.getBeanName() + \"'\", ele, ex);\n      }\n      // Send registration event.\n      getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder));\n   }\n}\n```\n\n继续看BeanDefinitionParserDelegate是如何解析的parseBeanDefinitionElement(ele)\n\n```java\npublic BeanDefinitionHolder parseBeanDefinitionElement(Element ele) {\n   return parseBeanDefinitionElement(ele, null);\n}\n```\n\n```java\npublic BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) {\n   String id = ele.getAttribute(ID_ATTRIBUTE);\n   String nameAttr = ele.getAttribute(NAME_ATTRIBUTE);\n\n   List<String> aliases = new ArrayList<>();\n   if (StringUtils.hasLength(nameAttr)) {\n      String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS);\n      aliases.addAll(Arrays.asList(nameArr));\n   }\n\n   String beanName = id;\n   if (!StringUtils.hasText(beanName) && !aliases.isEmpty()) {\n      beanName = aliases.remove(0);\n     ...\n   }\n\n   //检查beanName是否重复\n   if (containingBean == null) {\n      checkNameUniqueness(beanName, aliases, ele);\n   }\n\n    //继续点进去看\n   AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean);\n   if (beanDefinition != null) {\n      if (!StringUtils.hasText(beanName)) {\n         try {\n            if (containingBean != null) {\n               beanName = BeanDefinitionReaderUtils.generateBeanName(\n                     beanDefinition, this.readerContext.getRegistry(), true);\n            }\n            else {\n               beanName = this.readerContext.generateBeanName(beanDefinition);\n               // Register an alias for the plain bean class name, if still possible,\n               // if the generator returned the class name plus a suffix.\n               // This is expected for Spring 1.2/2.0 backwards compatibility.\n               String beanClassName = beanDefinition.getBeanClassName();\n               if (beanClassName != null &&\n                     beanName.startsWith(beanClassName) && beanName.length() > beanClassName.length() &&\n                     !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) {\n                  aliases.add(beanClassName);\n               }\n            }\n            if (logger.isTraceEnabled()) {\n               logger.trace(\"Neither XML 'id' nor 'name' specified - \" +\n                     \"using generated bean name [\" + beanName + \"]\");\n            }\n         }\n         catch (Exception ex) {\n            error(ex.getMessage(), ele);\n            return null;\n         }\n      }\n      String[] aliasesArray = StringUtils.toStringArray(aliases);\n      return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray);\n   }\n\n   return null;\n}\n```\n\n继续点\n\n```java\npublic AbstractBeanDefinition parseBeanDefinitionElement(\n      Element ele, String beanName, @Nullable BeanDefinition containingBean) {\n\n   this.parseState.push(new BeanEntry(beanName));\n\n   String className = null;\n   if (ele.hasAttribute(CLASS_ATTRIBUTE)) {\n      className = ele.getAttribute(CLASS_ATTRIBUTE).trim();\n   }\n   String parent = null;\n   if (ele.hasAttribute(PARENT_ATTRIBUTE)) {\n      parent = ele.getAttribute(PARENT_ATTRIBUTE);\n   }\n\n   try {\n      //创建GenericBeanDefinition对象,设置parent和className\n      AbstractBeanDefinition bd = createBeanDefinition(className, parent);\n\n      //解析bean标签的属性，并把解析出来的属性设置到BeanDefinition对象中\n      parseBeanDefinitionAttributes(ele, beanName, containingBean, bd);\n      bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT));\n\n      //解析bean中的meta标签\n      parseMetaElements(ele, bd);\n\n      //解析bean中的lookup-method标签  重要程度：2，可看可不看\n      parseLookupOverrideSubElements(ele, bd.getMethodOverrides());\n\n      //解析bean中的replaced-method标签  重要程度：2，可看可不看\n      parseReplacedMethodSubElements(ele, bd.getMethodOverrides());\n\n      //解析bean中的constructor-arg标签  重要程度：2，可看可不看\n      parseConstructorArgElements(ele, bd);\n\n      //解析bean中的property标签  重要程度：2，可看可不看\n      parsePropertyElements(ele, bd);\n\n      //可以不看，用不到\n      parseQualifierElements(ele, bd);\n\n      bd.setResource(this.readerContext.getResource());\n      bd.setSource(extractSource(ele));\n\n      return bd;\n   }\n   catch (ClassNotFoundException ex) {\n   ...\n   }\n   finally {\n      this.parseState.pop();\n   }\n\n   return null;\n}\n```\n\n先创建BeanDefinition的封装GenericBeanDefinition\n\n### 属性解析\n\n解析每个节点的属性\n\n```java\npublic AbstractBeanDefinition parseBeanDefinitionAttributes(Element ele, String beanName,\n                                                            @Nullable BeanDefinition containingBean, AbstractBeanDefinition bd) {\n    //如果有singleton属性,先提示一下,建议使用scope属性\n    if (ele.hasAttribute(SINGLETON_ATTRIBUTE)) {\n        error(\"Old 1.x 'singleton' attribute in use - upgrade to 'scope' declaration\", ele);\n    }\n    else if (ele.hasAttribute(SCOPE_ATTRIBUTE)) {\n        //如果有scope属性,设置scope\n        bd.setScope(ele.getAttribute(SCOPE_ATTRIBUTE));\n    }\n    else if (containingBean != null) {\n        // Take default from containing bean in case of an inner bean definition.\n        bd.setScope(containingBean.getScope());\n    }\n\n    //设置abstract属性,不实例化,子类需要parent标签引用,父类提供了公共的属性,子类不需要写那么多了\n    if (ele.hasAttribute(ABSTRACT_ATTRIBUTE)) {\n        bd.setAbstract(TRUE_VALUE.equals(ele.getAttribute(ABSTRACT_ATTRIBUTE)));\n    }\n\n    //设置lazy-init属性\n    String lazyInit = ele.getAttribute(LAZY_INIT_ATTRIBUTE);\n    if (DEFAULT_VALUE.equals(lazyInit)) {\n        lazyInit = this.defaults.getLazyInit();\n    }\n    bd.setLazyInit(TRUE_VALUE.equals(lazyInit));\n\n    //设置autowired属性\n    String autowire = ele.getAttribute(AUTOWIRE_ATTRIBUTE);\n    bd.setAutowireMode(getAutowireMode(autowire));\n\n    //设置depends-on属性\n    if (ele.hasAttribute(DEPENDS_ON_ATTRIBUTE)) {\n        String dependsOn = ele.getAttribute(DEPENDS_ON_ATTRIBUTE);\n        bd.setDependsOn(StringUtils.tokenizeToStringArray(dependsOn, MULTI_VALUE_ATTRIBUTE_DELIMITERS));\n    }\n\n    //设置autowired-candidate\n    String autowireCandidate = ele.getAttribute(AUTOWIRE_CANDIDATE_ATTRIBUTE);\n    if (\"\".equals(autowireCandidate) || DEFAULT_VALUE.equals(autowireCandidate)) {\n        String candidatePattern = this.defaults.getAutowireCandidates();\n        if (candidatePattern != null) {\n            String[] patterns = StringUtils.commaDelimitedListToStringArray(candidatePattern);\n            bd.setAutowireCandidate(PatternMatchUtils.simpleMatch(patterns, beanName));\n        }\n    }\n    else {\n        bd.setAutowireCandidate(TRUE_VALUE.equals(autowireCandidate));\n    }\n\n    //设置primary\n    if (ele.hasAttribute(PRIMARY_ATTRIBUTE)) {\n        bd.setPrimary(TRUE_VALUE.equals(ele.getAttribute(PRIMARY_ATTRIBUTE)));\n    }\n\n    //设置init-method\n    if (ele.hasAttribute(INIT_METHOD_ATTRIBUTE)) {\n        String initMethodName = ele.getAttribute(INIT_METHOD_ATTRIBUTE);\n        bd.setInitMethodName(initMethodName);\n    }\n    else if (this.defaults.getInitMethod() != null) {\n        bd.setInitMethodName(this.defaults.getInitMethod());\n        bd.setEnforceInitMethod(false);\n    }\n\n    //设置destroy-method\n    if (ele.hasAttribute(DESTROY_METHOD_ATTRIBUTE)) {\n        String destroyMethodName = ele.getAttribute(DESTROY_METHOD_ATTRIBUTE);\n        bd.setDestroyMethodName(destroyMethodName);\n    }\n    else if (this.defaults.getDestroyMethod() != null) {\n        bd.setDestroyMethodName(this.defaults.getDestroyMethod());\n        bd.setEnforceDestroyMethod(false);\n    }\n\n    //设置factory-method,指定生成实例的工厂方法\n    if (ele.hasAttribute(FACTORY_METHOD_ATTRIBUTE)) {\n        bd.setFactoryMethodName(ele.getAttribute(FACTORY_METHOD_ATTRIBUTE));\n    }\n    //设置factory-bean属性,指定生成实例的工厂,这个需要配合factory-method使用\n    if (ele.hasAttribute(FACTORY_BEAN_ATTRIBUTE)) {\n        bd.setFactoryBeanName(ele.getAttribute(FACTORY_BEAN_ATTRIBUTE));\n    }\n\n    return bd;\n}\n```\n\nparseBeanDefinitionAttributes(..)方法主要是解析Node的属性并设置了BeanDefinition的一些属性\n\n### meta标签解析\n\n再看meta标签的解析,其实就是把bean标签的meta属性的key,value读取出来,设置到BeanDefinition,没啥用,一个标识而已\n\n```java\npublic void parseMetaElements(Element ele, BeanMetadataAttributeAccessor attributeAccessor) {\n   NodeList nl = ele.getChildNodes();\n   for (int i = 0; i < nl.getLength(); i++) {\n      Node node = nl.item(i);\n      if (isCandidateElement(node) && nodeNameEquals(node, META_ELEMENT)) {\n         Element metaElement = (Element) node;\n         String key = metaElement.getAttribute(KEY_ATTRIBUTE);\n         String value = metaElement.getAttribute(VALUE_ATTRIBUTE);\n         BeanMetadataAttribute attribute = new BeanMetadataAttribute(key, value);\n         attribute.setSource(extractSource(metaElement));\n         attributeAccessor.addMetadataAttribute(attribute);\n      }\n   }\n}\n```\n\n### lookup-method标签解析\n\n同样的lookup-method标签的解析也是类似的\n\n```java\npublic void parseLookupOverrideSubElements(Element beanEle, MethodOverrides overrides) {\n    NodeList nl = beanEle.getChildNodes();\n    for (int i = 0; i < nl.getLength(); i++) {\n        Node node = nl.item(i);\n        if (isCandidateElement(node) && nodeNameEquals(node, LOOKUP_METHOD_ELEMENT)) {\n            Element ele = (Element) node;\n            //获取name属性\n            String methodName = ele.getAttribute(NAME_ATTRIBUTE);\n            //获取bean属性\n            String beanRef = ele.getAttribute(BEAN_ELEMENT);\n            //封装成 LookupOverride\n            LookupOverride override = new LookupOverride(methodName, beanRef);\n            override.setSource(extractSource(ele));\n            //可能有多个lookup-method标签,所以用list装起来\n            overrides.addOverride(override);\n        }\n    }\n}\n```\n\n----\n\n不过值得一提的是,这个lookup-method的设计精髓主要是代理思想,很方便的实现了多态\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd\">\n\n    <bean class=\"com.lrj.test.bean.Student\" id=\"student\"/>\n\n    <bean class=\"com.lrj.test.bean.Women\" id=\"women\"/>\n    <!--实现多态,传入什么就是什么-->\n    <bean class=\"com.lrj.test.bean.AbstractClass\">\n        <lookup-method name=\"getPeople\" bean=\"women\"/>\n    </bean>\n</beans>\n```\n\n```java\npublic abstract class People {\n    public void show(){}\n}\npublic class Women extends People {\n    @Override\n    public void show() {\n        System.out.println(\"I am women\");\n    }\n}\npublic abstract class AbstractClass {\n    public void show() {\n        getPeople().show();\n    }\n\n    public abstract People getPeople();\n}\n//这个方法最终打印的是I am women\n@Test\npublic void testXml() {\n    ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\");\n    AbstractClass abstractClass=context.getBean(AbstractClass.class);\n    abstractClass.show();\n}\n```\n\n----\n\n### replace-method标签解析\n\n再看看parseReplacedMethodSubElements(...)解析replace-method属性\n\n```java\npublic void parseReplacedMethodSubElements(Element beanEle, MethodOverrides overrides) {\n   NodeList nl = beanEle.getChildNodes();\n   for (int i = 0; i < nl.getLength(); i++) {\n      Node node = nl.item(i);\n      if (isCandidateElement(node) && nodeNameEquals(node, REPLACED_METHOD_ELEMENT)) {\n         Element replacedMethodEle = (Element) node;\n         String name = replacedMethodEle.getAttribute(NAME_ATTRIBUTE);\n         String callback = replacedMethodEle.getAttribute(REPLACER_ATTRIBUTE);\n\n         //一个replaced-method标签封装成一个ReplaceOverride对象，最后加入到BeanDefinition对象中\n         ReplaceOverride replaceOverride = new ReplaceOverride(name, callback);\n         // Look for arg-type match elements.\n         List<Element> argTypeEles = DomUtils.getChildElementsByTagName(replacedMethodEle, ARG_TYPE_ELEMENT);\n         for (Element argTypeEle : argTypeEles) {\n\n            //根据方法参数类型来区分同名的不同的方法\n            String match = argTypeEle.getAttribute(ARG_TYPE_MATCH_ATTRIBUTE);\n            match = (StringUtils.hasText(match) ? match : DomUtils.getTextValue(argTypeEle));\n            if (StringUtils.hasText(match)) {\n               replaceOverride.addTypeIdentifier(match);\n            }\n         }\n         replaceOverride.setSource(extractSource(replacedMethodEle));\n         overrides.addOverride(replaceOverride);\n      }\n   }\n}\n```\n\n可以发现lookup-method和replace-method都放入了BeanDefinition的**MethodOverrides**类型的overrides属性中,也就是说,MethodOverrides包含了LookupOverride和ReplaceOverride两种类型的对象\n\n----\n\n```xml\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd\">\n    <bean class=\"com.lrj.test.bean.Replacement\" id=\"replacement\"/>\n    <bean class=\"com.lrj.test.bean.Origin\">\n        <replaced-method name=\"show\" replacer=\"replacement\">\n            <arg-type match=\"int\"/>\n        </replaced-method>\n    </bean>\n</beans>\n```\n\n```java\npublic class Origin {\n    public void show(String str) {\n        System.out.println(\"show str:\" + str);\n    }\n\n    public void show(int str) {\n        System.out.println(\"show int:\" + str);\n    }\n}\n\npublic class Replacement implements MethodReplacer {\n    @Override\n    public Object reimplement(Object obj, Method method, Object[] args) throws Throwable {\n        System.out.println(\"I am a placement method......\");\n        return null;\n    }\n}\n\n//这个打印的将会是show str:hello和I am a placement method......\n//说明int的被替换了,但是String的没有被替换,\n@Test\npublic void testReplace() {\n    ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\");\n    Origin origin = context.getBean(Origin.class);\n    origin.show(\"hello\");\n    origin.show(555);\n}\n```\n\n这个听说是在项目封版之后,不想改代码了,直接改配置,符合开闭原则,但是这个Replacement必须要实现MethodReplacer感觉有点鸡肋\n\n### constructor-arg标签解析\n\n这个没啥讲的,无非是根据index或者name来设置\n\n不过需要注意ConstructorArgumentValues对象保存了ValueHolder集合\n\n解析construct-arg标签,读取下标或者name封装成**ValueHolder**,构成BeanDefinition的**ConstructorArgumentValues**\n\n```java\npublic void parseConstructorArgElements(Element beanEle, BeanDefinition bd) {\n   NodeList nl = beanEle.getChildNodes();\n   for (int i = 0; i < nl.getLength(); i++) {\n      Node node = nl.item(i);\n      if (isCandidateElement(node) && nodeNameEquals(node, CONSTRUCTOR_ARG_ELEMENT)) {\n         parseConstructorArgElement((Element) node, bd);\n      }\n   }\n}\n\npublic void parseConstructorArgElement(Element ele, BeanDefinition bd) {\n    String indexAttr = ele.getAttribute(INDEX_ATTRIBUTE);\n    String typeAttr = ele.getAttribute(TYPE_ATTRIBUTE);\n    String nameAttr = ele.getAttribute(NAME_ATTRIBUTE);\n    if (StringUtils.hasLength(indexAttr)) {\n        try {\n            int index = Integer.parseInt(indexAttr);\n            if (index < 0) {\n                error(\"'index' cannot be lower than 0\", ele);\n            }\n            else {\n                try {\n                    this.parseState.push(new ConstructorArgumentEntry(index));\n                    Object value = parsePropertyValue(ele, bd, null);\n                    ConstructorArgumentValues.ValueHolder valueHolder = new ConstructorArgumentValues.ValueHolder(value);\n                    if (StringUtils.hasLength(typeAttr)) {\n                        valueHolder.setType(typeAttr);\n                    }\n                    if (StringUtils.hasLength(nameAttr)) {\n                        valueHolder.setName(nameAttr);\n                    }\n                    valueHolder.setSource(extractSource(ele));\n                    if (bd.getConstructorArgumentValues().hasIndexedArgumentValue(index)) {\n                        error(\"Ambiguous constructor-arg entries for index \" + index, ele);\n                    }\n                    else {\n                        //将ValueHolder添加到BeanDefinition\n                        bd.getConstructorArgumentValues().addIndexedArgumentValue(index, valueHolder);\n                    }\n                }\n                finally {\n                    this.parseState.pop();\n                }\n            }\n        }\n        catch (NumberFormatException ex) {\n            error(\"Attribute 'index' of tag 'constructor-arg' must be an integer\", ele);\n        }\n    }\n    else {\n        try {\n            this.parseState.push(new ConstructorArgumentEntry());\n            Object value = parsePropertyValue(ele, bd, null);\n            ConstructorArgumentValues.ValueHolder valueHolder = new ConstructorArgumentValues.ValueHolder(value);\n            if (StringUtils.hasLength(typeAttr)) {\n                valueHolder.setType(typeAttr);\n            }\n            if (StringUtils.hasLength(nameAttr)) {\n                valueHolder.setName(nameAttr);\n            }\n            valueHolder.setSource(extractSource(ele));\n            //将ValueHolder添加到BeanDefinition\n            bd.getConstructorArgumentValues().addGenericArgumentValue(valueHolder);\n        }\n        finally {\n            this.parseState.pop();\n        }\n    }\n}\n```\n\n### property标签解析\n\n```java\npublic void parsePropertyElements(Element beanEle, BeanDefinition bd) {\n   NodeList nl = beanEle.getChildNodes();\n   for (int i = 0; i < nl.getLength(); i++) {\n      Node node = nl.item(i);\n      if (isCandidateElement(node) && nodeNameEquals(node, PROPERTY_ELEMENT)) {\n         parsePropertyElement((Element) node, bd);\n      }\n   }\n}\n\npublic void parsePropertyElement(Element ele, BeanDefinition bd) {\n    //获取name属性\n    String propertyName = ele.getAttribute(NAME_ATTRIBUTE);\n    if (!StringUtils.hasLength(propertyName)) {\n        error(\"Tag 'property' must have a 'name' attribute\", ele);\n        return;\n    }\n    this.parseState.push(new PropertyEntry(propertyName));\n    try {\n        if (bd.getPropertyValues().contains(propertyName)) {\n            error(\"Multiple 'property' definitions for property '\" + propertyName + \"'\", ele);\n            return;\n        }\n        Object val = parsePropertyValue(ele, bd, propertyName);\n        //将属性设置包装成 PropertyValue\n        PropertyValue pv = new PropertyValue(propertyName, val);\n        parseMetaElements(ele, pv);\n        pv.setSource(extractSource(ele));\n        //将 PropertyValue添加到BeanDefinition\n        bd.getPropertyValues().addPropertyValue(pv);\n    }\n    finally {\n        this.parseState.pop();\n    }\n}\n```\n\n和解析构造函数的参数一样,对于property标签的解析,同样是将key,value封装成**PropertyValue**,添加到BeanDefinition中,形成**MutablePropertyValues**类型\n\n### qualifier标签解析\n\n```java\npublic void parseQualifierElements(Element beanEle, AbstractBeanDefinition bd) {\n   NodeList nl = beanEle.getChildNodes();\n   for (int i = 0; i < nl.getLength(); i++) {\n      Node node = nl.item(i);\n      if (isCandidateElement(node) && nodeNameEquals(node, QUALIFIER_ELEMENT)) {\n         parseQualifierElement((Element) node, bd);\n      }\n   }\n}\n\npublic void parseQualifierElement(Element ele, AbstractBeanDefinition bd) {\n    String typeName = ele.getAttribute(TYPE_ATTRIBUTE);\n    if (!StringUtils.hasLength(typeName)) {\n        error(\"Tag 'qualifier' must have a 'type' attribute\", ele);\n        return;\n    }\n    this.parseState.push(new QualifierEntry(typeName));\n    try {\n        AutowireCandidateQualifier qualifier = new AutowireCandidateQualifier(typeName);\n        qualifier.setSource(extractSource(ele));\n        String value = ele.getAttribute(VALUE_ATTRIBUTE);\n        if (StringUtils.hasLength(value)) {\n            qualifier.setAttribute(AutowireCandidateQualifier.VALUE_KEY, value);\n        }\n        NodeList nl = ele.getChildNodes();\n        for (int i = 0; i < nl.getLength(); i++) {\n            Node node = nl.item(i);\n            if (isCandidateElement(node) && nodeNameEquals(node, QUALIFIER_ATTRIBUTE_ELEMENT)) {\n                Element attributeEle = (Element) node;\n                String attributeName = attributeEle.getAttribute(KEY_ATTRIBUTE);\n                String attributeValue = attributeEle.getAttribute(VALUE_ATTRIBUTE);\n                if (StringUtils.hasLength(attributeName) && StringUtils.hasLength(attributeValue)) {\n                    BeanMetadataAttribute attribute = new BeanMetadataAttribute(attributeName, attributeValue);\n                    attribute.setSource(extractSource(attributeEle));\n                    qualifier.addMetadataAttribute(attribute);\n                }\n                else {\n                    error(\"Qualifier 'attribute' tag must have a 'name' and 'value'\", attributeEle);\n                    return;\n                }\n            }\n        }\n        bd.addQualifier(qualifier);\n    }\n    finally {\n        this.parseState.pop();\n    }\n}\n```\n\n至此,BeanDefinition的解析完成,此时再回到BeanDefinitionParserDelegate.parseBeanDefinitionElement(org.w3c.dom.Element, org.springframework.beans.factory.config.BeanDefinition)\n\n```java\npublic BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) {\n   String id = ele.getAttribute(ID_ATTRIBUTE);\n   String nameAttr = ele.getAttribute(NAME_ATTRIBUTE);\n\n   List<String> aliases = new ArrayList<>();\n   if (StringUtils.hasLength(nameAttr)) {\n      String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS);\n      aliases.addAll(Arrays.asList(nameArr));\n   }\n\n   String beanName = id;\n   if (!StringUtils.hasText(beanName) && !aliases.isEmpty()) {\n      beanName = aliases.remove(0);\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"No XML 'id' specified - using '\" + beanName +\n               \"' as bean name and \" + aliases + \" as aliases\");\n      }\n   }\n\n   //检查beanName是否重复\n   if (containingBean == null) {\n      checkNameUniqueness(beanName, aliases, ele);\n   }\n\n   //点进去\n   AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean);\n   if (beanDefinition != null) {\n      if (!StringUtils.hasText(beanName)) {\n         try {\n            if (containingBean != null) {\n               beanName = BeanDefinitionReaderUtils.generateBeanName(\n                     beanDefinition, this.readerContext.getRegistry(), true);\n            }\n            else {\n               beanName = this.readerContext.generateBeanName(beanDefinition);\n               // Register an alias for the plain bean class name, if still possible,\n               // if the generator returned the class name plus a suffix.\n               // This is expected for Spring 1.2/2.0 backwards compatibility.\n               String beanClassName = beanDefinition.getBeanClassName();\n               if (beanClassName != null &&\n                     beanName.startsWith(beanClassName) && beanName.length() > beanClassName.length() &&\n                     !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) {\n                  aliases.add(beanClassName);\n               }\n            }\n            if (logger.isTraceEnabled()) {\n               logger.trace(\"Neither XML 'id' nor 'name' specified - \" +\n                     \"using generated bean name [\" + beanName + \"]\");\n            }\n         }\n         catch (Exception ex) {\n            error(ex.getMessage(), ele);\n            return null;\n         }\n      }\n      String[] aliasesArray = StringUtils.toStringArray(aliases);\n      return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray);\n   }\n\n   return null;\n}\n```\n\n这里返回的是,又对BeanDefinition做了一层包装,成BeanDefinitionHolder,形成name->BeanDefinition的映射","tags":["Spring","BeanDefinition"],"categories":["Spring","SpringCore"]},{"title":"Gradle的安装和使用","url":"/2020/01/15/Gradle-安装和使用/","content":"# Gradle的安装和使用\n\n## 下载\n\n可以去Gradle官网下载最新的稳定版本,目前是6.2,我自己下的4.8.1\n\nhttps://downloads.gradle-dn.com/distributions/gradle-4.8.1-bin.zip\n\n## 安装\n\nGradle的安装和Maven类似,就简单的解压,配置环境变量到Path就ok了\n\n![image-20200221102147192](/images/image/image-20200221102147192.png)\n\n![image-20200221102231848](/images/image/image-20200221102231848.png)\n\n配置完成之后,打开cmd查看一下是否配置好了\n\n![image-20200221102351240](/images/image/image-20200221102351240.png)\n\n看到正确输出了gradle的版本就说明配置好了\n\n## IDEA配置Gradle\n\nGradle和Maven这一点不同,Gradle无需再IDEA中进行配置操作,本地仓库地址的配置可以再IDEA中配置\n\n![image-20200221102917822](/images/image/image-20200221102917822.png)\n\n这个我用的是环境变量来配置的,IDEA会自动识别,只需要在环境变量中新建一个GRADLE_USER_HOME变量指向自己的本地仓库地址就可以了\n\n![image-20200221103016741](/images/image/image-20200221103016741.png)\n\n## Gradle初体验\n\n### 新建Gradle工程\n\n选择Gradle和JDK\n\n![image-20200221103418062](/images/image/image-20200221103418062.png)\n\n### 填写项目的GAV\n\n填写项目的GAV坐标,点Finished\n\n![image-20200221103642260](/images/image/image-20200221103642260.png)\n\n### Gradle的目录\n\nGradle的目录结构和Maven类似\n\n![image-20200221104845138](/images/image/image-20200221104845138.png)\n\n- src就是source目录\n  - src/main放代码目录\n    - src/main/java 放java代码目录\n    - src/main/resouces放资源文件\n\n- src/test是测试目录\n  - src/test/java 放测试的java代码目录\n  - src/test/resouces放测试的资源文件\n\n## Groovy编程\n\n### 打开Groovy Console\n\nTools->Groovy Console\n\n![image-20200221105555961](/images/image/image-20200221105555961.png)\n\n### HelloWorld\n\n凡事先HelloWorld一下\n\n![image-20200221105836306](/images/image/image-20200221105836306.png)\n\n![image-20200221105915909](/images/image/image-20200221105915909.png)\n\n### Groovy语法\n\n- Groovy可以省略最末尾的分号\n\n- Groovy可以省略小括号\n\n  ![image-20200221110227268](/images/image/image-20200221110227268.png)\n\n> 这两个特性可以看出,Groovy的书写更加自由,随意\n\n- 定义变量 def\n\n  ![image-20200221110338615](/images/image/image-20200221110338615.png)\n\n  groovy会根据数据自动推断类型\n\n- 定义集合\n\n  ![image-20200221110630481](/images/image/image-20200221110630481.png)\n\n- 定义Map\n\n  ![image-20200221110825151](/images/image/image-20200221110825151.png)\n\n- 闭包\n\n  闭包就是一段代码块,在Gradle中主要是把闭包当参数使用\n\n  ![image-20200221111242382](/images/image/image-20200221111242382.png)\n\n- 带参数的闭包\n\n  ![image-20200221111511073](/images/image/image-20200221111511073.png)\n\n## Gradle配置文件\n\n- build.gradle 构建项目配置\n\n  ![image-20200221112432889](/images/image/image-20200221112432889.png)\n\n## 优先本地加载\n\n在repositories中指定先从本地加载\n\n![image-20200221113447193](/images/image/image-20200221113447193.png)","tags":["Gradle"],"categories":["Gradle"]},{"title":"Redis缓存问题","url":"/2019/10/11/Redis-Redis-缓存问题/","content":"## Redis缓存问题\n\n### 缓存穿透\n\n恶意用户刻意伪造数据库不存在的数据进行大量访问,此时,由于数据库中没有,Redis也没把该信息缓存,请求过来之后,发现缓存没有,就去数据库中查找,而短时间内这样的大量请求会导致数据库负载压力大,甚至崩溃\n\n#### 解决方案\n\n1. 非法请求赋值一个特殊值,下次请求时,不查数据库\n\n2. 布隆过滤器\n\n   将可能出现的结果映射到一个大的集合,每次都从集合中获取\n\n### 缓存雪崩\n\n如果key的过期周期都差不多,在一段时间内没用访问之后,所有key的缓存都失效了,此时,如果大量请求过来,那么请求也都落到数据库,数据库瞬间崩盘\n\n#### 解决方案\n\n- 过期时间设置可以包含一定的随机数,保证不会同时失效,也就是时间错开\n- 使用setnx设置互斥锁,当有一个线程去load数据到缓存时,其他只能在队列等待,当load回来之后,如果命中就不需要去数据库查找了\n\n\n### 缓存击穿\n\n热点数据的缓存失效,如果短时间内大量请求,也会对数据库造成压力甚至崩溃\n \n#### 解决方案\n\n- 数据预热,提前对热点数据加载到缓存\n- 使用互斥锁加载数据到缓存\n- 热点数据生命周期设置为不过期,热度过去之后再设置为常规声明周期\n\n\n","tags":["Redis"],"categories":["Redis"]},{"title":"CDH-镜像恢复","url":"/2019/09/03/CDH-CDH镜像恢复/","content":"# CDH镜像恢复\n\n阿里云使用oss对cdh集群进行了快照存储,需要使用时,需要使用镜像来创建实例,以恢复cdh集群\n\n- 镜像创建实例\n\n  选择按量付费\n\n- 检查mysql是否正常\n\n- 修改hosts\n\n  镜像恢复之后,内网地址发生了变化,所以需要再次进行hosts的修改\n\n- 检查cm-server\n\n  db.properties\n\n  com.cloudera.cmf.db.hostname=xx\n\n- 检查cm-agent\n\n  config.ini\n\n  server_host=xxx\n\n- 检查mysql\n\n  cmf库的hosts\n\n  update为相应的host\n\n- 启动cm-server\n\n- 启动agent\n\n- web界面启动scm,cdh","tags":["CDH"],"categories":["CDH"]},{"title":"CDH-卸载","url":"/2019/08/30/CDH-CDH的卸载/","content":"# CDH的卸载\n\n## 卸载前规划\n\n- 关闭集群和MySQL\n- 删除部署文件 /opt/cloudera*\n- 删除数据文件夹 /etc/xxx\n\n## 存储目录\n\nweb->cluster->服务->选择对应的Configuration->data directory\n\n 如\n\n/dfs/nn\n\n/dfs/dn\n\n/dfs/snn\n\n/yarn/nm\n\n/var/lib/zookeeper\n\n## 卸载\n\n- web\n\n  - 关闭scm\n\n  - 关闭cdh\n\n- 服务器\n\n  - mysql\n    - drop database cmf\n    - drop user cmf\n    - drop user amon\n\n  - 关闭agent\n\n  - 关闭server\n\n  - 关闭cloudera进程\n\n    kill -9 $(pgrep -f cloudera)\n\n  - 校验df -h\n\n    umount xx\n\n- 部署文件夹删除,/opt/cloudera*\n\n- 删除隐藏文件\n\n  - /tmp/scm*\n  - /tmp/.scm*\n\n- 全局搜索\n\n  find / -name '\\*clouder\\*'\n\n- /etc/alternatives动态管理,软连接删除","tags":["CDH","CM"],"categories":["CDH"]},{"title":"CDH-集群部署","url":"/2019/08/28/CDH-CHD集群部署/","content":"# CHD集群部署\n\n## 软件\n\n```\nCM5.16.1\nhttp://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz\n\nCDH5.16.1\nhttp://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel\nhttp://archive.cloudera.com/cdh5/parcels/5.16.1/CDH-5.16.1-1.cdh5.16.2.p0.3-el7.parcel.sha1\nhttp://archive.cloudera.com/cdh5/parcels/5.16.1/manifest.json\n```\n\n\n\n## 集群规划\n\n| 节点      | 部署                                                    |\n| --------- | ------------------------------------------------------- |\n| hadoop001 | mysql,cm-server,cm-agent,nn,sn,dn,rm,nm,zk,kafka-broker |\n| hadoop002 | cm-agent,dn,nm,zk,kafka-broker                          |\n| hadoop003 | cm-agent,dn,nm,zk,kafka-broker                          |\n\n## 关闭防火墙\n\n```bash\nsystemctl stop firewalld\nsystemctl disable firewalld\niptables -F\n```\n\n## 关闭selinux\n\n```bash\nvi /etc/selinux/config\n\tSELINUX=diabled\n```\n\n## 时间同步ntp(云主机忽略)\n\n```bash\ndate\ntimedatectl\ntimedatectl list-timezones\ntimedatectl set-timezone Asia/Shanghai\n```\n\n选取hadoop001作为ntp主节点\n\nhadoop002,hadoop003为从节点\n\n```bash\nyum install -y ntp\nvi /etc/ntp.cnf\n\t# 设置time\n\tserver 1.aisa.pool.ntp.org\n\tserver 2.aisa.pool.ntp.org\n\tserver 3.aisa.pool.ntp.org\n\t# 当外部time不可用,使用本机\n\tserver 127.0.0.1 inburst local lock\n\t# 允许哪些网段可以来同步时间\n\trestrict hadoop002 mask 255.255.255.0 nomodify notrap\n\trestrict hadoop003 mask 255.255.255.0 nomodify notrap\n# 启动ntp\nsystemctl start ntpd\nsystemctl restart ntpd\n# 查看状态\nsystemctl status ntpd\n# 验证\nntp -q\n```\n\n另外节点不需要ntp,关闭ntp\n\n```bash\nsystemctl stop ntpd\nsystemctl disable ntpd\n```\n\n同步hadoop001时间\n\n```bash\nntpdate hadoop001\n```\n\n配置crontab进行定时同步\n\n```bash\n# 每天0点0分同步ntp\ncrontab -e\n\t0 0 * * * ntpdate hadoop001\n# 查看定时任务\ncrontab -l\n```\n\n## 免密登录\n\n为了方便拷贝东西,做一下免密\n\n```bash\nssh-keygen #之后一路enter\ncat ~/.ssh/id_rsa.pub # 将三台的公钥拷贝到文本中\n# 将公钥写入三台的authorized_keys\nvi ~/.ssh/authorized_keys \n# 修改文件权限\nchmod 0600 ~/.ssh/authorized_keys\n# 测试\nssh hadoop001 date\nssh hadoop002 date\nssh hadoop003 date\n```\n\n\n\n## 拷贝文件到另外两台\n\n```bash\nscp -r hadoop001:~/cdh5.16.1/* ~/cdh5.16.1/\n```\n\n\n\n## 部署JDK\n\n<u>jdk统一部署到/usr/java下,避免乱七八糟的错误</u>\n\n```bash\ntar -zxvf jdk-8u45-linux-x64.gz\nmkdir /usr/java\nmv ./jdk1.8.0_45 /usr/java\n\n# 修改权限\nchown -R root:root /usr/java/jdk1.8.0_45\nln -s /usr/java/jdk1.8.0_45 /usr/java/java8\n\nvi ~/.bash_profile\n\texport JAVA_HOME=/usr/java/java8\n\texport PATH=$JAVA_HOME/bin:$PATH\n\nsource ~/.bash_profile\n# 查看一下jdk版本\njava -version\n```\n\n\n\n## 部署MySQL\n\n### 解压\n\n```bash\ntar -zxvf mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz\nchown -R root:root ./*\nln -s /root/cdh5.16.1/mysql-5.7.11-linux-glibc2.5-x86_64 /usr/local/mysql\nmkdir /usr/local/mysql/arch /usr/local/mysql/data /usr/local/mysql/tmp\n```\n\n### my.cnf\n\n```bash\ncp /etc/my.cnf /etc/my.cnf.bak\nvi /etc/my.cnf #先清空\n```\n\n配置my.cnf\n\n```conf\n[client]\nport            = 3306\nsocket          = /usr/local/mysql/data/mysql.sock\ndefault-character-set=utf8mb4\n\n[mysqld]\nport            = 3306\nsocket          = /usr/local/mysql/data/mysql.sock\n\nskip-slave-start\n\nskip-external-locking\nkey_buffer_size = 256M\nsort_buffer_size = 2M\nread_buffer_size = 2M\nread_rnd_buffer_size = 4M\nquery_cache_size= 32M\nmax_allowed_packet = 16M\nmyisam_sort_buffer_size=128M\ntmp_table_size=32M\n\ntable_open_cache = 512\nthread_cache_size = 8\nwait_timeout = 86400\ninteractive_timeout = 86400\nmax_connections = 600\n\n# Try number of CPU's*2 for thread_concurrency\n#thread_concurrency = 32 \n\n#isolation level and default engine \ndefault-storage-engine = INNODB\ntransaction-isolation = READ-COMMITTED\n\nserver-id  = 1739\nbasedir     = /usr/local/mysql\ndatadir     = /usr/local/mysql/data\npid-file     = /usr/local/mysql/data/hostname.pid\n\n#open performance schema\nlog-warnings\nsysdate-is-now\n\nbinlog_format = ROW\nlog_bin_trust_function_creators=1\nlog-error  = /usr/local/mysql/data/hostname.err\nlog-bin = /usr/local/mysql/arch/mysql-bin\nexpire_logs_days = 7\n\ninnodb_write_io_threads=16\n\nrelay-log  = /usr/local/mysql/relay_log/relay-log\nrelay-log-index = /usr/local/mysql/relay_log/relay-log.index\nrelay_log_info_file= /usr/local/mysql/relay_log/relay-log.info\n\nlog_slave_updates=1\ngtid_mode=OFF\nenforce_gtid_consistency=OFF\n\n# slave\nslave-parallel-type=LOGICAL_CLOCK\nslave-parallel-workers=4\nmaster_info_repository=TABLE\nrelay_log_info_repository=TABLE\nrelay_log_recovery=ON\n\n#other logs\n#general_log =1\n#general_log_file  = /usr/local/mysql/data/general_log.err\n#slow_query_log=1\n#slow_query_log_file=/usr/local/mysql/data/slow_log.err\n\n#for replication slave\nsync_binlog = 500\n\n\n#for innodb options \ninnodb_data_home_dir = /usr/local/mysql/data/\ninnodb_data_file_path = ibdata1:1G;ibdata2:1G:autoextend\n\ninnodb_log_group_home_dir = /usr/local/mysql/arch\ninnodb_log_files_in_group = 4\ninnodb_log_file_size = 1G\ninnodb_log_buffer_size = 200M\n\n#根据生产需要，调整pool size \ninnodb_buffer_pool_size = 2G\n#innodb_additional_mem_pool_size = 50M #deprecated in 5.6\ntmpdir = /usr/local/mysql/tmp\n\ninnodb_lock_wait_timeout = 1000\n#innodb_thread_concurrency = 0\ninnodb_flush_log_at_trx_commit = 2\n\ninnodb_locks_unsafe_for_binlog=1\n\n#innodb io features: add for mysql5.5.8\nperformance_schema\ninnodb_read_io_threads=4\ninnodb-write-io-threads=4\ninnodb-io-capacity=200\n#purge threads change default(0) to 1 for purge\ninnodb_purge_threads=1\ninnodb_use_native_aio=on\n\n#case-sensitive file names and separate tablespace\ninnodb_file_per_table = 1\nlower_case_table_names=1\n\n[mysqldump]\nquick\nmax_allowed_packet = 128M\n\n[mysql]\nno-auto-rehash\ndefault-character-set=utf8mb4\n\n[mysqlhotcopy]\ninteractive-timeout\n\n[myisamchk]\nkey_buffer_size = 256M\nsort_buffer_size = 256M\nread_buffer = 2M\nwrite_buffer = 2M\n```\n\n### 配置用户组\n\n```bash\ngroupadd -g 101 dba\nuseradd -u 514 -g dba -G root -d /usr/local/mysql mysqladmin\n# 查看一下 uid=514(mysqladmin) gid=101(dba) groups=101(dba),0(root)\nid mysqladmin\n\n# 拷贝环境变量文件到mysqladmin home目录\ncp /etc/skel/.* /usr/local/mysql5.7\n```\n\n### 配置mysqladmin环境变量\n\n```bash\nsu - mysqladmin\nvi .bash_profile\n```\n\n追加\n\n```conf\nexport MYSQL_BASE=/usr/local/mysql\nexport PATH=${MYSQL_BASE}/bin:$PATH\nunset USERNAME\nset umask to 022\numask 022\nPS1=`uname -n`\":\"'$USER'\":\"'$PWD'\":>\"; export PS1\n```\n\n### 修改权限\n\n```bash\nchown  mysqladmin:dba /etc/my.cnf\nchmod  640 /etc/my.cnf\nchown -R mysqladmin:dba /usr/local/mysql\nchmod -R 755 /usr/local/mysql\n```\n\n### 自启动配置\n\n```bash\ncp /usr/local/mysql/support-files/mysql.server /etc/rc.d/init.d/mysql\nchmod +x /etc/rc.d/init.d/mysql\nchkconfig --add mysql\nchkconfig --level 345 mysql on\n```\n\n### 按照mysql依赖\n\n```bash\nyum -y install libaio\nsu - mysqladmin\n```\n\n### 初始化mysql\n\n```bash\nsudo su - mysqladmin\nbin/mysqld \\\n--defaults-file=/etc/my.cnf \\\n--user=mysqladmin \\\n--basedir=/usr/local/mysql/ \\\n--datadir=/usr/local/mysql/data/ \\\n--initialize\n\n# 等待初始化完成后 eC%gbl>g3KUJ\ncat data/hostname.err|grep password\n```\n\n### 启动mysql并修改密码\n\n```bash\n/usr/local/mysql/bin/mysqld_safe --defaults-file=/etc/my.cnf &\n\nmysql -u root -p\n\n# 修改密码,赋权\nalter user root@localhost identified by 'root@1234';\n\nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root@1234' ;\ncreate  database cmf default character set utf8;\nGRANT ALL PRIVILEGES ON cmf.* TO 'cmf'@'%' IDENTIFIED BY 'cmf@1234' ;\ncreate  database amon default character set utf8;\nGRANT ALL PRIVILEGES ON amon.* TO 'amon'@'%' IDENTIFIED BY 'amon@1234' ;\n\nflush privileges;\n\n```\n\n### 上传mysql jar\n\n```bash\nmkdir /usr/share/java\ncd /usr/share/java\nrz\nscp ./mysql-connector-java-5.1.47.jar hadoop002:/usr/share/java\nscp ./mysql-connector-java-5.1.47.jar hadoop003:/usr/share/java\n```\n\n\n\n## 安装CM\n\n### 解压\n\n```bash\nmkdir /opt/cm\ntar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cm\n```\n\n### 配置agent\n\n```bash\ncd /opt/cm/cm-5.16.1/etc/cloudera-scm-agent\nvi config.ini\n\t# 配置cmd_server\n\tserver_host=hadoop001\n```\n\n### 配置cm-server\n\n```bash\ncd /opt/cm/cm-5.16.1/etc/cloudera-scm-server\nvi db.properties\n\tcom.cloudera.cmf.db.host=hadoop001\n\tcom.cloudera.cmf.db.name=cmf\n\tcom.cloudera.cmf.db.password=cmf@1234\n\tcom.cloudera.cmf.db.setupType=EXTERNAL\n```\n\n### 添加用户\n\n```bash\nuseradd --system \\\n--home=/opt/cm/cm-5.16.1/run/cloudera-scm-server \\\n--no-create-home \\\n--shell=/bin/false \\\n--comment='ClouderManager User' \\\ncloudmanager\n```\n\n### 修改权限\n\n```bash\nchown -R cloudmanager:cloudmanager /opt/cm\n```\n\n## Parcel部署\n\nparcel将hdfs,zk,kafka等组件做了自己的版本兼容修改,打成自己的包\n\n### 校验\n\n记得校验一下是否下载完整了,相同就是下载完成了,否则安装时会继续下载\n\n```bash\nmv CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha\ncat  CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha\nsha1sum CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel\n```\n\n### 部署准备\n\n```bash\nmkdir -p /opt/cloudera/parcel-repo\nmv CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel* /opt/cloudera/parcel-repo\nmv manifest.json /opt/cloudera/parcel-repo\nchown -R cloudmanager:cloudmanager /opt/cloudera\n```\n\n\n\n## 正式集群部署\n\n### 创建目录和用户\n\n所有节点创建软件的安装目录,用户和用户组\n\n```bash\nmkdir -p /opt/cloudera/parcels\nchown -R cloudmanager:cloudmanager /opt/cloudera/\n```\n\n\n\n### 启动\n\n```bash\n# 先启动mysql\nsudo su - mysqladmin\nservice mysql restart\nexit\ncd /opt/cm/cm-5.16.1/etc/init.d\n./cloudera-scm-server start\n```\n\n发现报错了....\n\n```\ninstall: invalid user ‘cloudera-scm’\nStarting cloudera-scm-server:                              [FAILED]\n```\n\n\n\n还是老实用人家的默认用户cloudera-scm\n\n```bash\nuseradd --system \\\n--home=/opt/cm/cm-5.16.1/run/cloudera-scm-server \\\n--no-create-home \\\n--shell=/bin/false \\\n--comment='ClouderManager User' \\\ncloudera-scm\n\nchown -R cloudera-scm:cloudera-scm /opt/cm\nchown -R cloudera-scm:cloudera-scm /opt/cloudera\n```\n\n\n\n继续启动,发现还是报错了,查看一下日志\n\n```bash\ntailf -200 /opt/cm/cm-5.16.1/log/cloudera-scm-server/cloudera-scm-server.log\n```\n\n![image-20200502112327363](/images/image/image-20200502112327363.png)\n\n```bash\nchmod 755 /usr/share/java/mysql-connector-java-5.1.47.jar\n# 这个一定要做,去掉版本号,否则不认\nmv /usr/share/java/mysql-connector-java-5.1.47.jar /usr/share/java/mysql-connector-java.jar\n```\n\n### 安全组配置\n\n开放7180\n\n待程序启动后,访问7180端口,就可以看到cm界面了\n\n![image-20200502115719630](/images/image/image-20200502115719630.png)\n\n默认账号密码就是admin/admin\n\n### 启动agent\n\n所有的agent启动\n\n```bash\n/opt/cm/cm-5.16.1/etc/init.d/cloudera-scm-agent start\ntailf -200 /opt/cm/cm-5.16.1/log/cloudera-scm-agent/cloudera-scm-agent.log\n```\n\n### 配置cm\n\n- 选择免费版\n\n- 选择自己下载好的parcel版本,不然会重新下载,很慢\n\n- 去掉警告\n\n  ```bash\n  echo never > /sys/kernel/mm/transparent_hugepage/defrag\n  echo never > /sys/kernel/mm/transparent_hugepage/enabled\n  ```\n\n  \n\n- 选择自定义服务,按需安装hdfs,zk,yarn\n\n\n\n\n\n## 关闭集群\n\n- web界面关闭cdh和cms\n\n  ![image-20200502145018638](/images/image/image-20200502145018638.png)\n\n- 关闭agent\n\n  ```bash\n  /opt/cm/cm-5.16.1/etc/init.d/cloudera-scm-agent stop\n  ```\n\n  \n\n- 关闭server\n\n  ```bash\n  /opt/cm/cm-5.16.1/etc/init.d/cloudera-scm-server stop\n  ```\n\n  \n\n- 关闭mysql\n\n  ```bash\n  su - mysqladmin\n  service mysql stop\n  ```\n\n- 关闭实例\n\n- 为实例创建镜像(需要购买OSS快照存储),释放实例\n\n","tags":["CDH","CM"],"categories":["CDH"]},{"title":"SparkStreaming基础-SparkStreaming的tranform和与Kakfa集成","url":"/2019/06/07/Spark-SparkStreaming-2/","content":"# SparkStreaming-2\n\n## tranform\n\nSpark Streaming接收到数据得到的是一个DStream,如果需要DStream和RDD进行关联,此时并没有DStream和RDD关联的API,所以需要tranform算子来进行DStream和RDD的关联\n\n例如,我们只要统计指定的单词\n\n```scala\nobject TransformApp {\n  def main(args: Array[String]): Unit = {\n    def createStreamContext() = {\n      val conf = new SparkConf().setAppName(\"transformApp\").setMaster(\"local[2]\")\n      val sc = new SparkContext(conf)\n      //只统计hello,word\n      val wordRDD = sc.parallelize(List(\"hello\", \"word\")).map((_, 1))\n      val ssc = new StreamingContext(sc, Seconds(5))\n      val stream = ssc.socketTextStream(\"hadoop001\", 9000)\n\n      def updateState = (n: Seq[Int], o: Option[Int]) => {\n        val sum = n.sum + o.getOrElse(0)\n        Some(sum)\n      }\n\n      stream.flatMap(_.split(\",\")).map((_, 1)).transform(rdd => {\n        rdd.join(wordRDD)\n      }).map(t => (t._1, t._2._1)).updateStateByKey(updateState).print()\n      ssc.checkpoint(\"checkpoint\")\n      ssc\n    }\n\n    val ssc = StreamingContext.getOrCreate(\"checkpoint\", createStreamContext)\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```\n\n## Window\n\n滑窗有两个重要的概念\n\n- *window length* - 窗口的间隔\n- *sliding interval* - 每次滑移的间隔\n\n这两个值必须是DStream的间隔时间的倍数,例如DStream可能为每5秒钟一次,window length=30s,则相当于每次的窗口数据包含了6个DStream间隔,silding interval=10s,那么就是10秒滑动一次\n\n例如我们想,每隔10s统计30s中以内的wc\n\n```scala\nobject WindowApp {\n  def main(args: Array[String]): Unit = {\n    def createStreamContext() = {\n      val conf = new SparkConf().setAppName(\"WindowApp\").setMaster(\"local[2]\")\n      val sc = new SparkContext(conf)\n      val ssc = new StreamingContext(sc, Seconds(5))\n      val stream = ssc.socketTextStream(\"hadoop001\", 9000)\n\n      def updateState = (seq: Seq[Int], oldValue: Option[Int]) => {\n        Some(seq.sum)\n      }\n\n      stream.flatMap(_.split(\",\")).map((_, 1))\n        .reduceByKeyAndWindow((a:Int,b:Int)=>a+b, Seconds(30), Seconds(10))\n        .updateStateByKey(updateState).print()\n      ssc.checkpoint(\"checkpoint\")\n      ssc\n    }\n\n    val ssc = StreamingContext.getOrCreate(\"checkpoint\", createStreamContext)\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```\n\n这样运行后就可以看到,不在时间范围内的就不会被统计\n\n![image-20200426014314067](/images/image/image-20200426014314067.png)\n\n\n\n## Output Operations on DStreams 输出算子\n\nSparkStreaming作为流处理,一般不会输出到文件系统如hdfs,或者保存为文件,因为这样可能会导致大量的小文件问题,一般常用的算子为\n\n- print\n\n- foreachRDD\n\n  使用最多的算子,在Streaming中,一般全都要使用foreachRDD来操作\n\n```scala\nobject ForeachRDDApp {\n  def main(args: Array[String]): Unit = {\n    def createStreamContext() = {\n      val conf = new SparkConf().setAppName(\"ForeachRDDApp\").setMaster(\"local[2]\")\n      val sc = new SparkContext(conf)\n      val ssc = new StreamingContext(sc, Seconds(5))\n      val stream = ssc.socketTextStream(\"hadoop001\", 9000)\n\n      def updateState = (seq: Seq[Int], oldValue: Option[Int]) => {\n        Some(seq.sum)\n      }\n\n      stream.flatMap(_.split(\",\")).map((_, 1))\n        .reduceByKeyAndWindow((a: Int, b: Int) => a + b, Seconds(30), Seconds(10))\n        .updateStateByKey(updateState).foreachRDD((rdd, time) => {\n            //也可以写到数据库,这里打印方便点\n        val array = rdd.collect()\n        val str = array.mkString(\",\")\n        println(s\"time:$time---->$str\")\n      })\n      ssc.checkpoint(\"checkpoint\")\n      ssc\n    }\n\n    val ssc = StreamingContext.getOrCreate(\"checkpoint\", createStreamContext)\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```\n\n运行效果\n\n![image-20200426020128960](/images/image/image-20200426020128960.png)\n\n### Design Patterns for using foreachRDD\n\nforeachRDD可以将数据写到外部系统中,但是使用不当会导致一些错误.常见错误如,创建连接,如何写到外部,但是由于连接之类的很少实现了序列化,所以会导致不能序列化的错误\n\n**错误**例子\n\n连接创建在Driver端,但是使用是在worker端,这时需要序列化,所以这种用法是错误的\n\n```scala\ndstream.foreachRDD { rdd =>\n  val connection = createNewConnection()  // executed at the driver\n  rdd.foreach { record =>\n    connection.send(record) // executed at the worker\n  }\n}\n```\n\n另一种常见的**错误**用法是,每个rdd都创建连接,这样增加系统的负荷\n\n```scala\ndstream.foreachRDD { rdd =>\n  rdd.foreach { record =>\n    val connection = createNewConnection()\n    connection.send(record)\n    connection.close()\n  }\n}\n```\n\n**正确的方式**是使用连接池,用完归还\n\n```scala\ndstream.foreachRDD { rdd =>\n  rdd.foreachPartition { partitionOfRecords =>\n    // ConnectionPool is a static, lazily initialized pool of connections\n    val connection = ConnectionPool.getConnection()\n    partitionOfRecords.foreach(record => connection.send(record))\n    ConnectionPool.returnConnection(connection)  // return to the pool for future reuse\n  }\n}\n```\n\n连接池的连接最好做成懒加载的模式,不使用一段时间后进行销毁,避免占用资源\n\n##### Other points to remember\n\n- DStream只有遇到输出算子才会执行(和RDD的懒加载一样,只有遇到action才执行),DStream内部RDD的action会强制DStream处理接收到的数据,如果你的应用没有设置输出算子,或者只有foreachRDD算子而内部没有RDD的action算子,那么不会有任何输出\n\n\n\n## Input DStreams and Receivers\n\n### Basic Sources\n\n### Advanced Sources\n\n#### [Kafka Source](http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html)\n\nSpark Streaming和Kafka集成的 0.10版本,Kafka的partition和SparkStreaming的partition是一致的,保持1:1\n\n##### 添加依赖\n\n```xml\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-streaming-kafka-0-10_${scala.tool.version}</artifactId>\n    <version>${spark.version}</version>\n</dependency>\n```\n\n##### Creating a Direct Stream\n\n```scala\nobject KafkaStreamApp {\n  def main(args: Array[String]): Unit = {\n    def createStreamContext() = {\n      val conf = new SparkConf().setAppName(\"KafkaStreamApp\").setMaster(\"local[2]\")\n      val sc = new SparkContext(conf)\n      val ssc = new StreamingContext(sc, Seconds(5))\n      val kafkaParams = Map[String, Object](\n        \"bootstrap.servers\" -> \"hadoop001:9091,hadoop001:9092,hadoop001:9093\",\n        \"key.deserializer\" -> classOf[StringDeserializer],\n        \"value.deserializer\" -> classOf[StringDeserializer],\n        \"group.id\" -> \"myGroup\",\n        \"auto.offset.reset\" -> \"latest\",\n        \"enable.auto.commit\" -> (false: java.lang.Boolean)\n      )\n\n      val topics = Array(\"testA\")\n      val stream = KafkaUtils.createDirectStream[String, String](\n        ssc,\n        PreferConsistent,\n        Subscribe[String, String](topics, kafkaParams)\n      )\n\n      def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {\n        Some(newValues.sum)\n      }\n      stream.map(record => (record.value,1))\n        .reduceByKeyAndWindow((a:Int,b:Int)=>a+b,Seconds(10),Seconds(5))\n        .updateStateByKey(updateFunction).print()\n      ssc.checkpoint(\"checkpoint\")\n      ssc\n    }\n\n    val ssc = StreamingContext.getOrCreate(\"checkpoint\", createStreamContext)\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```\n\n创建消费者\n\n```scala\nobject MyKafkaProducer {\n  def main(args: Array[String]): Unit = {\n    val props = new Properties();\n    props.put(\"bootstrap.servers\", \"hadoop001:9091,hadoop001:9092,hadoop001:9093\")\n    props.put(\"acks\", \"all\")\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n    val producer = new KafkaProducer[String, String](props)\n    for (x <- 0 to 1000) {\n      val a = 'A'\n      val word = (Random.nextInt(23) + a).toChar+\"\"\n      producer.send(new ProducerRecord[String, String](\"testA\", x%3, x+\"\", word))\n      TimeUnit.MICROSECONDS.sleep(200)\n    }\n  }\n}\n```\n\n输出\n\n![image-20200426033659694](/images/image/image-20200426033659694.png)\n\n##### Obtaining Offsets获取偏移量\n\n获取消费的偏移量\n\n```scala\nstream.foreachRDD { rdd =>\n  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n  rdd.foreachPartition { iter =>\n    val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)\n    println(s\"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}\")\n  }\n}\n```\n\n##### Storing Offsets存储偏移量\n\n```scala\nstream.foreachRDD { rdd =>\n  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n\n  // some time later, after outputs have completed\n  stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)\n}\n```\n\n##### 设置kafka数据的偏移量\n\n```scala\nval fromOffsets = selectOffsetsFromYourDatabase.map { resultSet =>\n  new TopicPartition(resultSet.string(\"topic\"), resultSet.int(\"partition\")) -> resultSet.long(\"offset\")\n}.toMap\n\nval stream = KafkaUtils.createDirectStream[String, String](\n  streamingContext,\n  PreferConsistent,\n  Assign[String, String](fromOffsets.keys.toList, kafkaParams, fromOffsets)\n)\n\nstream.foreachRDD { rdd =>\n  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n\n  val results = yourCalculation(rdd)\n\n  // begin your transaction\n\n  // update results\n  // update offsets where the end of existing offsets matches the beginning of this batch of offsets\n  // assert that offsets were updated correctly\n\n  // end your transaction\n}\n```","tags":["Spark","SparkStreaming","SparkStreaming-Kakfa"],"categories":["Spark"]},{"title":"SparkStreaming基础-SparkStreaming的基本使用","url":"/2019/05/20/Spark-SparkStream/","content":"# SparkStream-1\n\n## [SparkStream](http://spark.apache.org/docs/latest/streaming-programming-guide.html)概述\n\n- SparkCore的拓展\n- 高吞吐量,容错在线流数据处理\n- 数据源诸如:Kafka,Flume,TCP sockets\n- 可以使用高阶函数来进行复炸的操作\n- 可以输出到文件系统,数据库等\n- 数据按时间间隔切分成批次交由Spark引擎处理\n\n## DStream概述\n\n- 基本抽象**DStream**,代表着连续的数据流\n- DStream可由Kafka,Flume,TCP socket产生\n\n## WordCount\n\n- 首先创建SparkStream入口StreamContext\n\n  ```scala\n  val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"wc\")\n  val sc = new SparkContext(conf)\n  val ssc = new StreamingContext(sc, Seconds(5))\n  ```\n\n  \n\n- socketTextStream\n\n  创建DStream\n\n  ```scala\n  val stream = ssc.socketTextStream(\"hadoop001\", 9000)\n  ```\n\n- wc逻辑\n\n  ```scala\n  stream.flatMap(x => x.split(\",\")).map((_, 1)).reduceByKey(_ + _).print()\n  \n  //启动stream\n  ssc.start()\n  ssc.awaitTermination()\n  ```\n\n> 注意\n>\n> socketTextStream底层会创建socket连接进行监听,所以core必须大于等于receiver数量,否则没有可用的core来处理数据,导致stream无限挂起\n>\n> ```scala\n> class SocketReceiver[T: ClassTag](\n>     host: String,\n>     port: Int,\n>     bytesToObjects: InputStream => Iterator[T],\n>     storageLevel: StorageLevel\n>   ) extends Receiver[T](storageLevel) with Logging {\n> \n>   private var socket: Socket = _\n> \n>   def onStart() {\n> \n>     logInfo(s\"Connecting to $host:$port\")\n>     try {\n>       socket = new Socket(host, port)\n>     } catch {\n>       case e: ConnectException =>\n>         restart(s\"Error connecting to $host:$port\", e)\n>         return\n>     }\n>     logInfo(s\"Connected to $host:$port\")\n> \n>     // Start the thread that receives data over a connection\n>     new Thread(\"Socket Receiver\") {\n>       setDaemon(true)\n>       override def run() { receive() }\n>     }.start()\n> }\n> ```\n\n\n\n**注意点**:\n\n- streaming.start()之后,任何stream的操作都没用了,所以所有的额streaming操作都要在start之前\n- 一旦stream stop,无法重启\n\n## DStream\n\n- DStream是一系列的RDD,按时间间隔切分\n\n  ![image-20200425212811895](/images/imageimage-20200425212811895.png)\n\n- 对DStream的转换操作就是对DStream代表的所以RDD进行相同的操作\n\n  ![image-20200425212950581](/images/imageimage-20200425212950581.png)\n\n## Input DStreams and Receivers\n\nSparkStream提供了两种内置的DStream数据源\n\n- 基本的数据源,如fileSystem,socket\n- 高级的数据源,如kafka,flume\n\n##### Points to remember\n\n- 本地运行,如果使用基于receiver的DStream时,不能使用local/local[1],因为这样只有一个core,而这个core用于接收数据了,就没用剩余的core来处理数据了\n- 拓宽到集群,core的数量也必须大于receiver的数量\n- 但是不基于receiver的DStream就没有这个限制了,如textFileStream\n- SparkStreaming只能监听到启动之后进来的数据,之前的数据是没法监听到的\n- DStream的count是按行计算的记录数,如果需要知道元素的个数,需要在遍历每一行进行计算\n\n\n\n## Transformations on DStreams\n\nDStream除了可以使用高阶函数(如map,filter,flatMap等),还提供了**updateStateByKey**算子,用于更新DStream的状态,例如每批数据进来之后对wc中相同的key进行累加操作\n\n#### UpdateStateByKey Operation\n\n为了使用这个算子,需要:\n\n- 定义State,可为任意类型\n- 定义更新操作函数,处理旧值和新值\n\n```scala\nobject WcStateApp {\n  def main(args: Array[String]): Unit = {\n    //1.创建StreamingContext\n    val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"wc\")\n    val sc = new SparkContext(conf)\n    val ssc = new StreamingContext(sc, Seconds(5))\n\n    def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {\n      val newCount = newValues.sum + runningCount.getOrElse(0)\n      Some(newCount)\n    }\n\n    //2.创建DStream,这里监听socket\n    val stream = ssc.socketTextStream(\"hadoop001\", 9000)\n    //指定一个checkpoint目录,这样streaming就可以获取到以前的统计了\n    ssc.checkpoint(\"checkpoint\")\n    stream.flatMap(x => x.split(\",\")).map((_, 1)).updateStateByKey(updateFunction).print()\n    ssc.start()\n    ssc.awaitTermination()\n  }\n```\n\n此时,再次启动wc程序,已经发现可以对批次累加了\n\n但是这里还是有问题的,因为只要stream重启,以前的统计值就没法拿到了,我们应该从checkpoint中创建StreamingContext,这个才[checkponit](http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing)中做了相关的论述\n\n需要对程序做一下修改\n\n```scala\nobject WcStateApp {\n  def main(args: Array[String]): Unit = {\n    //1.创建StreamingContext\n    def functionToCreateContext(): StreamingContext = {\n\n      def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {\n        val newCount = newValues.sum + runningCount.getOrElse(0)\n        Some(newCount)\n      }\n      val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"wc\")\n      val sc = new SparkContext(conf)\n      val ssc = new StreamingContext(sc, Seconds(5))\n      //2.创建DStream,这里监听socket\n      val stream = ssc.socketTextStream(\"hadoop001\", 9000)\n      stream.flatMap(x => x.split(\",\")).map((_, 1)).updateStateByKey(updateFunction).print()\n\n      ssc.checkpoint(\"checkpoint\")\n      ssc\n    }\n\n    // Get StreamingContext from checkpoint data or create a new one\n    val ssc = StreamingContext.getOrCreate(\"checkpoint\", functionToCreateContext _)\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```\n\n再启动时,可以发现历史的已经可以从checkpoint中都出来了,但是历史闪的速度很快,并不是我们指定duration\n\n#### mapWithState\n\n新版提供了mapWithState算子来管理state,与updateStateByKey不同的是,每接收到元素就会进行mappingFunction,而不是一批一批的mappingFunction,所以会导致同一批的统计中有很多值,如,一条记录是这样的:\n\n1,5,4,2,1,4,1,41,5,4,2,1,4,1,41,5,4,2,1,4,1,41,5,4,2,1,4,1\n\n此时,控制台的输出将是\n\n![image-20200425231515582](/images/imageimage-20200425231515582.png)\n\n可以看到,同一个key出现了多次,我们需要取最高值,而不是之前的值,感觉还不如updateStateByKey\n\n```scala\nobject WcMapWithStateApp {\n  def main(args: Array[String]): Unit = {\n    //1.创建StreamingContext\n    def functionToCreateContext(): StreamingContext = {\n\n      val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"wc\")\n      val sc = new SparkContext(conf)\n      val ssc = new StreamingContext(sc, Seconds(5))\n      //2.创建DStream,这里监听socket\n      val stream = ssc.socketTextStream(\"hadoop001\", 9000)\n\n      def mappingFunction = (key: String, value: Option[Int], state: State[Int]) => {\n        // Use state.exists(), state.get(), state.update() and state.remove()\n        // to manage state, and return the necessary string\n        val sum = value.getOrElse(0) + state.getOption().getOrElse(0)\n        state.update(sum)\n        (key, sum)\n      }\n\n      //新版提供了StateSpec.function(mappingFunction)进行mapWithState\n      stream.flatMap(x => x.split(\",\")).map((_, 1)).mapWithState(StateSpec.function(mappingFunction)).print()\n\n      ssc.checkpoint(\"checkpoint\")\n      ssc\n    }\n\n    // Get StreamingContext from checkpoint data or create a new one\n    val ssc = StreamingContext.getOrCreate(\"checkpoint\", functionToCreateContext _)\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n\n```\n\n","tags":["Spark","SparkStreaming"],"categories":["Spark"]},{"title":"SparkSQL基础-Spark SQL和RDD的交互","url":"/2019/05/05/Spark-SparkSQL-和RDD的交互/","content":"# SparkSQL-和RDD的交互\n\n## RDD到DataSet\n\n- 使用反射的方式来推导SparkSQL的schema(如:已知case class)\n- programmatic\n\n### 反射方式\n\n这种预先知道并定义了case class\n\n准备数据\n\n```\nJustin,25\nAndy,35\nMichael,14\n```\n\n程序\n\n```scala\nobject RddToDfTest {\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession.builder().appName(\"UdfTest\").master(\"local[2]\").getOrCreate()\n    byCaseClass(spark)\n    spark.close()\n  }\n  case class Person(name:String,age:Int)\n  def byCaseClass(spark: SparkSession): Unit ={\n    val rdd = spark.sparkContext.textFile(\"input/info.txt\")\n    val person = rdd.map(str => {\n      val words = str.split(\",\")\n      Person(words(0), words(1).toInt)\n    })\n    //导入转换,否则没有toDF\n    import spark.implicits._\n    person.toDF().printSchema()\n  }\n}\n```\n\noutput\n\n```\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = false)\n```\n\n### Row+Schema\n\n- 获取RDD[String]\n- RDD[String]转成RDD[Row]\n- 定义schema\n- 使用RDD和schema创建df\n\nprogram\n\n```scala\ndef byRow(spark: SparkSession): Unit ={\n    val rdd = spark.sparkContext.textFile(\"input/info.txt\")\n    // 1.将RDD[String]=>RDD[Row]\n    val rowRDD = rdd.map(str => {\n      val words = str.split(\",\")\n      Row(words(0),words(1).toInt)\n    })\n    // 2.定义schema\n    val struct=StructType(\n      StructField(\"name\",StringType)::\n      StructField(\"age\",IntegerType)::Nil\n    )\n\n    // 3.将row作用schema转成df\n    val df = spark.createDataFrame(rowRDD, struct)\n    df.printSchema()\n    df.show()\n}\n```\n\noutput\n\n```\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n\n+-------+---+\n|   name|age|\n+-------+---+\n| Justin| 25|\n|   Andy| 35|\n|Michael| 14|\n+-------+---+\n```\n\n\n\n","tags":["Spark","SparkSQL","RDD"],"categories":["Spark"]},{"title":"SparkSQL基础-Spark SQL-聚合和UDF","url":"/2019/05/02/Spark-SparkSQL-聚合/","content":"# SparkSQL-聚合和UDF\n\n准备数据\n\n```\nsubject,userId,sex,score\nSubjectA,user1,male,512\nSubjectA,user2,female,521\nSubjectA,user3,female,152\nSubjectA,user4,male,542\nSubjectA,user5,male,542\nSubjectB,user11,male,652\nSubjectB,user21,male,562\nSubjectB,user31,female,562\nSubjectB,user41,female,2344\nSubjectB,user51,male,133\nSubjectC,user111,male,455\nSubjectC,user211,female,4553\nSubjectC,user311,male,4553\nSubjectC,user411,female,455\nSubjectC,user511,male,4552\n```\n\n## 分组求和\n\n```scala\ndef groupBy(spark: SparkSession): Unit ={\n  import spark.implicits._\n  val df = spark.read\n    .option(\"sep\", \",\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .format(\"csv\").load(\"input/groupby.txt\")\n  val dataFrame = df.select($\"subject\", $\"sex\", $\"score\")\n  import org.apache.spark.sql.functions._\n  dataFrame.groupBy(\"subject\",\"sex\").agg(sum($\"score\")).show()\n}\n```\n\nAPI方式需要导入org.apache.spark.sql.functions已经定义好的函数\n\n![image-20200503121212427](/images/imageimage-20200503121212427.png)\n\noutput\n\n```\n+--------+------+----------+\n| subject|   sex|sum(score)|\n+--------+------+----------+\n|SubjectC|female|      5008|\n|SubjectC|  male|      9560|\n|SubjectB|  male|      1347|\n|SubjectB|female|      2906|\n|SubjectA|female|       673|\n|SubjectA|  male|      1596|\n+--------+------+----------+\n```\n\n## 分组topN\n\n```scala\ndef groupTopN(spark: SparkSession): Unit ={\n  val df = spark.read\n    .option(\"sep\", \",\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .format(\"csv\").load(\"input/groupby.txt\")\n  df.createTempView(\"subjects\")\n  spark.sql(\n    \"\"\"\n      |select t.* from (\n      |select subject,sex,score,row_number() over (partition by subject,sex order by score desc) rn from subjects)t\n      |where t.rn<=2\n      |\"\"\".stripMargin).show()\n}\n```\n\noutput\n\n```\n+--------+------+-----+---+\n| subject|   sex|score| rn|\n+--------+------+-----+---+\n|SubjectC|female| 4553|  1|\n|SubjectC|female|  455|  2|\n|SubjectC|  male| 4553|  1|\n|SubjectC|  male| 4552|  2|\n|SubjectB|  male|  652|  1|\n|SubjectB|  male|  562|  2|\n|SubjectB|female| 2344|  1|\n|SubjectB|female|  562|  2|\n|SubjectA|female|  521|  1|\n|SubjectA|female|  152|  2|\n|SubjectA|  male|  542|  1|\n|SubjectA|  male|  542|  2|\n+--------+------+-----+---+\n```\n\n## UDF\n\n准备数据\n\n```\n{\"name\":\"zhangsan\",\"age\":25,\"sex\":0}\n{\"name\":\"lisi\",\"age\":41,\"sex\":1}\n{\"name\":\"wangwu\",\"age\":32,\"sex\":1}\n{\"name\":\"zhaoliu\",\"age\":41,\"sex\":0}\n```\n\n程序\n\n```scala\n\nobject UdfTest {\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession.builder().appName(\"UdfTest\").master(\"local[2]\").getOrCreate()\n    val sexFun = spark.udf.register(\"aa\", (str: String) => {\n      val toInt = str.toInt\n      if (toInt == 0) {\n        \"male\"\n      } else {\n        \"female\"\n      }\n    })\n    val df = spark.read.json(\"input/people.txt\")\n    import spark.implicits._\n    df.select($\"name\", $\"age\", sexFun($\"sex\").as(\"sex\")).show\n    spark.close()\n  }\n}\n\n```\n\nregister进行注册,后面调用一下就可以了\n\noutput\n\n```\n+--------+---+------+\n|    name|age|   sex|\n+--------+---+------+\n|zhangsan| 25|  male|\n|    lisi| 41|female|\n|  wangwu| 32|female|\n| zhaoliu| 41|  male|\n+--------+---+------+\n```","tags":["Spark","SparkSQL","Agg","UDF"],"categories":["Spark"]},{"title":"SparkSQL基础-Spark SQL-Source","url":"/2019/04/28/Spark-SparkSQL-2/","content":"# SparkSQL-Source\n\n## JSON文件\n\n### 读取\n\n- sparkSession.read.format(\"json\").load(path)\n- sparkSession.read.json(path)\n\n准备一份数据\n\n```json\n{\"name\":\"zhangsan\",\"age\":18,\"sex\":\"male\"}\n{\"name\":\"lisi\",\"age\":32,\"sex\":\"female\"}\n{\"name\":\"wangwu\",\"age\":45,\"sex\":\"male\"}\n{\"name\":\"zhaoliu\",\"age\":87,\"sex\":\"male\"}\n```\n\n程序\n\n#### read.format(\"json\").load(path)\n\n```scala\nobject JsonSourceTest {\n  def main(args: Array[String]): Unit = {\n    //创建SparkSession\n    val spark = SparkSession.builder()\n      .appName(\"JsonSource\")\n      .master(\"local\")\n      .getOrCreate()\n    //readJson1(spark)\n    readJson2(spark)\n    spark.close()\n  }\n\n  def readJson1(spark:SparkSession): Unit ={\n    val df = spark.read.format(\"json\").load(\"input/json.txt\")\n    df.printSchema()\n  }\n\n  \n}\n```\n\n#### sparkSession.read.json(path)\n\n```scala\ndef readJson2(spark:SparkSession): Unit ={\n    val df = spark.read.json(\"input/json.txt\")\n    df.printSchema()\n}\n```\n\n\n\n输出\n\n```\nroot\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n |-- sex: string (nullable = true)\n```\n\n#### 部分字段 select\n\n```scala\ndef readJson3(spark:SparkSession): Unit ={\n  val df = spark.read.format(\"json\").load(\"input/json.txt\")\n  df.printSchema()\n  //选择部分列\n  df.select(\"name\",\"sex\").show(false)\n}\n```\n\n输出\n\n```\n+--------+------+\n|name    |sex   |\n+--------+------+\n|zhangsan|male  |\n|lisi    |female|\n|wangwu  |male  |\n|zhaoliu |male  |\n+--------+------+\n```\n\n#### 部分字段Select-DSL\n\n```scala\ndef readJson4(spark:SparkSession): Unit ={\n  val df = spark.read.format(\"json\").load(\"input/json.txt\")\n  //需要添加隐式转换才能使用DSL语法\n  import spark.implicits._\n  //选择部分列\n  df.select($\"name\",$\"sex\").show(false)\n}\n```\n\n#### 过滤-filter\n\n```scala\n  def readJsonFilter(spark:SparkSession): Unit ={\n    val df = spark.read.format(\"json\").load(\"input/json.txt\")\n    import spark.implicits._\n    //选择部分列\n    val dataFrame = df.select($\"name\", $\"sex\")\n    //1.字符串\n    dataFrame.filter(\"sex='female'\").show(false)\n    //2.df方式\n    dataFrame.filter(df(\"sex\")===\"female\").show()\n    //3.DSL\n    dataFrame.filter('sex===\"female\").show()\n}\n```\n\n输出\n\n```\n+----+------+\n|name|sex   |\n+----+------+\n|lisi|female|\n+----+------+\n...\n```\n\n#### 复杂对象\n\n准备数据\n\n```json\n{\"id\":0,\"name\":\"admin\",\"users\":[{\"id\":2,\"name\":\"guest\"},{\"id\":3,\"name\":\"root\"}]}\n```\n\n程序\n\n```scala\ndef readJsonComplex(spark:SparkSession): Unit ={\n    val df = spark.read.format(\"json\").load(\"input/complexJson.txt\")\n    df.printSchema()\n    import spark.implicits._\n    //选择部分列\n    df.select($\"name\", $\"users.id\",$\"users.name\".as(\"users.name\")).show()\n}\n```\n\n输出\n\n```\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- users: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- name: string (nullable = true)\n \n+-----+------+-------------+\n| name|    id|   users.name|\n+-----+------+-------------+\n|admin|[2, 3]|[guest, root]|\n+-----+------+-------------+\n```\n\n\n\n### 输出\n\n- df.write.format(\"json\").mode(SaveMode.Overwrite).save(path)\n- df.write.mode(SaveMode.Overwrite).json(path)\n\n```scala\ndef writeJson(spark:SparkSession): Unit ={\n  val df = spark.read.format(\"json\").load(\"input/json.txt\")\n  import spark.implicits._\n  //选择部分列\n  val dataFrame = df.select($\"name\", $\"sex\")\n  dataFrame.write.format(\"json\").mode(SaveMode.Overwrite).save(\"output/write.json\")\n}\n```\n\n发现这个写出去和hdfs操作一样,output/write.json只是一个目录,真正的数据是partxxx\n\n\n\n## Text文本文件\n\n### 读取\n\n- read.format(\"text\").load(path)\n- read.text(path)\n\n准备数据\n\n```txt\n13429100031 22552  8  2013-03-11 08:55:19.151754088  571    571    282    571\n13429100082    22540  8  2013-03-11 08:58:20.152622488  571    571    270    571\n13429100082    22691  8  2013-03-11 08:56:37.149593624  571    571    103    571\n```\n\n#### read.format(\"text\").load(path)\n\n程序\n\n```scala\ndef readText(spark:SparkSession): Unit ={\n    val df = spark.read.format(\"text\").load(\"input/text.txt\")\n    df.show()\n    df.printSchema()\n}\n```\n\n输出\n\n```\n+--------------------+\n|               value|\n+--------------------+\n|13429100031\t22552...|\n|13429100082\t22540...|\n|13429100082\t22691...|\n+--------------------+\n\nroot\n |-- value: string (nullable = true)\n\n```\n\n#### map\n\n```scala\ndef map(spark:SparkSession): Unit ={\n  val df = spark.read.format(\"text\").load(\"input/text.txt\")\n  import spark.implicits._\n  df.map(row=>{\n    val strings = row.getString(0).split(\"\\t\")\n    (strings(0),strings(1))\n  }).show()\n}\n```\n\noutput\n\n```\n+-----------+-----+\n|         _1|   _2|\n+-----------+-----+\n|13429100031|22552|\n|13429100082|22540|\n|13429100082|22691|\n+-----------+-----+\n```\n\n#### rdd\n\n```scala\ndef rdd(spark:SparkSession): Unit ={\n  val df = spark.read.format(\"text\").load(\"input/text.txt\")\n  df.rdd.map(row=>{\n    val strings = row.getString(0).split(\"\\t\")\n    (strings(0),strings(1))\n  }).foreach(println)\n}\n```\n\noutput\n\n```\n(13429100031,22552)\n(13429100082,22540)\n(13429100082,22691)\n```\n\n#### textFile\n\ntextFile直接返回一个DataSet而不是一个DataFrame\n\n```scala\ndef textFile(spark: SparkSession): Unit = {\n  val ds = spark.read.textFile(\"input/text.txt\")\n  import spark.implicits._\n  ds.map(row => {\n    val strings = row.split(\"\\t\")\n    (strings(0), strings(1))\n  }).show()\n}\n```\n\noutput\n\n```\n+-----------+-----+\n|         _1|   _2|\n+-----------+-----+\n|13429100031|22552|\n|13429100082|22540|\n|13429100082|22691|\n+-----------+-----+\n```\n\n### 输出\n\n- write.text(path)\n- write.format(\"text\").save(path)\n\n```scala\ndef writeTextFile(spark: SparkSession): Unit = {\n  val ds = spark.read.textFile(\"input/text.txt\")\n  import spark.implicits._\n  val value = ds.map(row => {\n    val strings = row.split(\"\\t\")\n    (strings(0), strings(1))\n  })\n  value.write.mode(SaveMode.Overwrite).text(\"output/text\")\n}\n```\n\n此时会报异常\n\n```\norg.apache.spark.sql.AnalysisException: Text data source supports only a single column, and you have 2 columns.;\n```\n\n多列数据想存储为text格式的时候,需要合并成一列\n\n```scala\ndef writeTextFile(spark: SparkSession): Unit = {\n  val df = spark.read.text(\"input/text.txt\")\n  import spark.implicits._\n  val value = df.map(row => {\n    val strings = row.getString(0).split(\"\\t\")\n    (strings(0), strings(1))\n  }).map(x => x._1 +\",\"+ x._2)\n  value.write.mode(SaveMode.Overwrite).text(\"output/text\")\n}\n```\n\noutput\n\n```\n13429100031,22552\n13429100082,22540\n13429100082,22691\n```\n\n#### compress\n\n要压缩保存,可以使用option指定compression\n\n```scala\ndef writeCompressionTextFile(spark: SparkSession): Unit = {\n  val df = spark.read.text(\"input/text.txt\")\n  import spark.implicits._\n  val value = df.map(row => {\n    val strings = row.getString(0).split(\"\\t\")\n    (strings(0), strings(1))\n  }).map(x => x._1 +\",\"+ x._2)\n  value.write.mode(SaveMode.Overwrite).option(\"compression\",\"bzip2\").text(\"output/text\")\n}\n```\n\n## csv文件\n\n### 读取\n\n- read.format(\"csv\").load(path)\n- read.csv(path)\n\n准备数据\n\n```\nname|age|sex\nzhangsan|20|male\nlisi|15|female\nwangwu|25|male\n```\n\n#### read.format(\"csv\").load(path)\n\n```scala\ndef readCsv(spark:SparkSession): Unit ={\n  val df = spark.read.format(\"csv\")\n    .option(\"header\",\"true\")\n    .option(\"sep\", \"|\").load(\"input/csv.txt\")\n  df.printSchema()\n  df.show()\n}\n```\n\noutput\n\n```\nroot\n |-- name: string (nullable = true)\n |-- age: string (nullable = true)\n |-- sex: string (nullable = true)\n \n+--------+---+------+\n|    name|age|   sex|\n+--------+---+------+\n|zhangsan| 20|  male|\n|    lisi| 15|female|\n|  wangwu| 25|  male|\n+--------+---+------+\n```\n\n可以发现,csv好像不进行类型推断了,那是因为这个option没开启\n\n```scala\ndef readCsvInfer(spark:SparkSession): Unit ={\n  val df = spark.read.format(\"csv\")\n    .option(\"header\",\"true\").option(\"inferSchema\",\"true\")\n    .option(\"sep\", \"|\").load(\"input/csv.txt\")\n  df.printSchema()\n}\n```\n\noutput\n\n```\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- sex: string (nullable = true)\n```\n\n\n\n> Source的option\n>\n> 每个Source对应的option可以在 org.apache.spark.sql.execution包下找到相应的xxOption类\n\n\n\n## JDBC\n\n### 读取\n\n```scala\ndef readMysql(spark: SparkSession): Unit ={\n  val df = spark.read.format(\"jdbc\")\n    .option(\"url\", \"jdbc:mysql:///test?serverTimezone=Asia/Shanghai\")\n    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\n    .option(\"dbtable\", \"t_order\")\n    .option(\"user\", \"lrj\")\n    .option(\"password\", \"123456\")\n    .load()\n  df.printSchema()\n  df.show()\n}\n```\n\noutput\n\n```\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- number: integer (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- total: double (nullable = true)\n \n+---+-----+------+----------+------+\n| id| name|number|product_id| total|\n+---+-----+------+----------+------+\n|  1|订单1|     2|         1| 355.0|\n|  2|订单1|     2|         2|  45.0|\n|  3|订单2|     5|         1|1000.0|\n+---+-----+------+----------+------+\n```\n\n### 输出\n\n```scala\ndef writeMysql(spark: SparkSession): Unit ={\n    val df = spark.read.format(\"jdbc\")\n      .option(\"url\", \"jdbc:mysql:///test?serverTimezone=Asia/Shanghai\")\n      .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\n      .option(\"dbtable\", \"t_order\")\n      .option(\"user\", \"lrj\")\n      .option(\"password\", \"123456\")\n      .load()\n    import spark.implicits._\n    df.filter('total<500) .write.option(\"url\", \"jdbc:mysql:///test?serverTimezone=Asia/Shanghai\")\n      .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\n      .option(\"dbtable\", \"t_order_filter\")\n      .option(\"user\", \"lrj\")\n      .option(\"password\", \"123456\").mode(SaveMode.Overwrite).format(\"jdbc\").save()\n}\n```\n\n这个还是挺骚的,不需要提前创建表\n\n> SaveMode\n>\n> write的SaveMode需要注意,如果不指定,当输出路径存在时会报错","tags":["Spark","SparkSQL"],"categories":["Spark"]},{"title":"SparkSQL基础-Spark SQL","url":"/2019/04/21/Spark-SparkSQL-1/","content":"\n# SparkSQL-1\n\n## 为什么要使用SQL\n\n传统SQL手段,对于RDBMS在数据量大之后,很难满足需求,或者性能很低,需要将原来的SQL方式转为大数据方向,也就是云化.但是最好能保留原有的SQL方式,不会影响原来的业务.\n\n## 常见的SQL On Hadoop框架\n\n- Hive\n\n  Hive的出现已经很久了,技术成熟,主要处理**离线业务,时延比较高**\n\n- Impala\n\n  Cloudera公司基于Parquet实现\n\n- Presto\n\n- Shark\n\n  基于Hive,将SQL运行在Spark之上,但是对执行计划优化比较弱,过于依赖Hive,维护升级难\n\n- Hive On Spark\n\n  Hive默认使用MR作业方式运行,可以通过设置set =spark转换Hive为Spark作业,语法支持丰富\n\n  可以通过set hive.execution.engine=spark\n\n- Spark SQL\n  语法解析,执行计划优化全都由Spark来实现,逐步支持各种SQL\n\n- Drill\n\n- Phoenix\n\n  HBase SQL化\n\n## SparkSQL\n\nSpark处理结构化数据的一个模块\n\nSparkSQL\n\n### 特点\n\n- 使用Spark编程无缝对接SQL查询\n\n  使用SQL或者DataFrameAPI查询处理结构化数据\n\n- 统一数据访问\n\n  使用DataFrame和SQL可以访问多种数据源,包括Hive,Avro,Parquet,ORC,JSON,JDBC等\n\n- Hive集成\n\n  支持HiveQL语法,UDF等\n\n- 标准连接\n\n  支持JDBC,ODBC连接\n\n> Spark SQL强调的是**结构化数据**,而不是SQL\n>\n> Spark SQL支持SQL,DataFrame,DataSet\n>\n> 总的来说:\n>\n> Spark SQL is not about SQL\n>\n> Spark SQL is about more than SQL\n\n## DataSet & DataFrame\n\n### DataSet\n\n- 特殊的RDD,也是分布式数据集\n- spark1.6引入的\n- 强类型,可以使用lamda函数\n- 可以从JVM对象中使用转换函数转换而来\n\n### DataFrame\n\n- DataFrame是一个特殊的DataSet\n\n- 等同于关系型数据库的一张表\n\n- DataFrame可以结构化数据文件,Hive,外部数据源或者已存在的RDD中构造出来\n- DF=DataSet[Row]\n\n## Hello World\n\n```scala\n\nobject HelloWorldApp {\n  def main(args: Array[String]): Unit = {\n    //创建SparkSession对象,这个是SparkSQL的入口\n    val spark = SparkSession.builder()\n      .appName(\"helloWorldApp\")\n      .master(\"local[2]\").getOrCreate()\n\n    //读取json,传入json文件路径\n    val df = spark.read.json(\"input/access.json\")\n\n    df.printSchema()\n    //将读到的json创建一个临时表,这样后面就可以使用sql了,传入表名\n    df.createOrReplaceTempView(\"json\")\n\n    spark.sql(\"select ip,ispname,cityname from json\").show(false)\n\n    spark.close()\n  }\n}\n```\n\n\n\n## 注意\n\n- Spark RDD中的cache是lazy的,Spark1.6开始,Spark SQL中的cache/uncache是eager,默认存储级别内存无序列化","tags":["Spark","SparkSQL"],"categories":["Spark"]},{"title":"SparkCore基础-Spark监控","url":"/2019/04/18/Spark-Spark监控/","content":"# Spark监控\n\n## spark自带的Monitor\n\n4040端口生命周期可见\n\n生命周期结束要看,需要配置\n\nspark-default.properties\n\n```properties\nspark.master=local\nspark.eventLog.enable=true\nspark.eventLog.dir=xx\n```\n\nspark-env.sh\n\n```properties\nexport SPARK_HISTORY_OPTS=\"-Dspark.history.fs.logDirectory=xx -Dspark.history.ui.port=xx\"\n```\n\n## 自定义实现\n\nSparkLister\n\n","tags":["Spark","SparkCore"],"categories":["Spark"]},{"title":"SparkCore基础-Spark作业提交","url":"/2019/04/17/Spark-SparkCore-SumbmitApplication/","content":"\n# SparkCore-SumbmitApplication\n\n\n\nhdfs:\nHADOOP_CONF_DIR\n\n```bash\nspark-submit \\\n--class com.lrj.spark.core.offline.WordCountApp \\\n--name WCApp \\\n--master local[2] \\\n--jars /usr/software/spark/lib/spark-utils-1.0-SNAPSHOT.jar \\\n/usr/software/spark/lib/spark-demo-1.0-SNAPSHOT.jar \\\ninput/wc.txt output/wc\n```\n\n\n\nyarn:\nYARN_CONF_DIR\n\n```bash\nspark-submit \\\n--class com.lrj.spark.core.offline.WordCountApp \\\n--name WCApp \\\n--master yarn \\\n--jars /usr/software/spark/lib/spark-utils-1.0-SNAPSHOT.jar \\\n/usr/software/spark/lib/spark-demo-1.0-SNAPSHOT.jar \\\ninput/wc.txt output/wc\n```\n\n\n\n![image-20200323011505418](/images/imageimage-20200323011505418.png)\n\n\n\n作业提交之后,先Accept,等到得到资源之后才会开始运行,如果资源不够可能回导致等待时间很长\n\n![image-20200323011637172](/images/imageimage-20200323011637172.png)\n\n\n\n### 不同模式的Spark任务\n\n#### client模式\n\n- 本地启动Driver (new SparkContext(xx).setMaster(xx) )\n\n- Driver和RM通信,请求启动AM\n- RM找一台NM启动AM\n- AM向RM申请资源\n- RM分配一批资源给AM,AM根据资源分布与NM通信,启动Executor容器\n- Executor反向注册到Driver\n\nExecutor和Driver必须时不时的通信,可能会出现网络激增\n\n\n\n关于spark-submit的说明\n\n```bash\nUsage: spark-submit [options] <app jar | python file | R file> [app arguments]\nUsage: spark-submit --kill [submission ID] --master [spark://...]\nUsage: spark-submit --status [submission ID] --master [spark://...]\nUsage: spark-submit run-example [options] example-class [example args]\n\nOptions:\n  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n                              k8s://https://host:port, or local (Default: local[*]).\n  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n                              on one of the worker machines inside the cluster (\"cluster\")\n                              (Default: client).\n  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n  --name NAME                 A name of your application.\n  --jars JARS                 Comma-separated list of jars to include on the driver\n                              and executor classpaths.\n  --packages                  Comma-separated list of maven coordinates of jars to include\n                              on the driver and executor classpaths. Will search the local\n                              maven repo, then maven central and any additional remote\n                              repositories given by --repositories. The format for the\n                              coordinates should be groupId:artifactId:version.\n  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n                              resolving the dependencies provided in --packages to avoid\n                              dependency conflicts.\n  --repositories              Comma-separated list of additional remote repositories to\n                              search for the maven coordinates given with --packages.\n  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n                              on the PYTHONPATH for Python apps.\n  --files FILES               Comma-separated list of files to be placed in the working\n                              directory of each executor. File paths of these files\n                              in executors can be accessed via SparkFiles.get(fileName).\n\n  --conf PROP=VALUE           Arbitrary Spark configuration property.\n  --properties-file FILE      Path to a file from which to load extra properties. If not\n                              specified, this will look for conf/spark-defaults.conf.\n\n  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n  --driver-java-options       Extra Java options to pass to the driver.\n  --driver-library-path       Extra library path entries to pass to the driver.\n  --driver-class-path         Extra class path entries to pass to the driver. Note that\n                              jars added with --jars are automatically included in the\n                              classpath.\n\n  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n\n  --proxy-user NAME           User to impersonate when submitting the application.\n                              This argument does not work with --principal / --keytab.\n\n  --help, -h                  Show this help message and exit.\n  --verbose, -v               Print additional debug output.\n  --version,                  Print the version of current Spark.\n\n Cluster deploy mode only:\n  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n                              (Default: 1).\n\n Spark standalone or Mesos with cluster deploy mode only:\n  --supervise                 If given, restarts the driver on failure.\n  --kill SUBMISSION_ID        If given, kills the driver specified.\n  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n\n Spark standalone and Mesos only:\n  --total-executor-cores NUM  Total cores for all executors.\n\n Spark standalone and YARN only:\n  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\n                              or all available cores on the worker in standalone mode)\n\n YARN-only:\n  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n  --num-executors NUM         Number of executors to launch (Default: 2).\n                              If dynamic allocation is enabled, the initial number of\n                              executors will be at least NUM.\n  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n                              working directory of each executor.\n  --principal PRINCIPAL       Principal to be used to login to KDC, while running on\n                              secure HDFS.\n  --keytab KEYTAB             The full path to the file that contains the keytab for the\n                              principal specified above. This keytab will be copied to\n                              the node running the Application Master via the Secure\n                              Distributed Cache, for renewing the login tickets and the\n                              delegation tokens periodically.\n```\n\n","tags":["Spark","SparkCore"],"categories":["Spark"]},{"title":"SparkCore基础-Spark On Yarn","url":"/2019/04/15/Spark-Spark On Yarn/","content":"# Spark On Yarn\n\n## [Launching Spark on YARN](http://spark.apache.org/docs/latest/running-on-yarn.html#launching-spark-on-yarn)\n\n- Spark On Yarn务必配置`HADOOP_CONF_DIR`或者`YARN_CONF_DIR`指向Hadoop的配置目录(`$HADOOP_HOME/etc/hadoop`),否则报错\n\n- **Spark On Yarn的两种模式**\n\n  - client\n\n    client模式中,Driver进程运行再客户端的进程中,所以客户端提交任务后必须等待任务执行完成,适合交互性任务\n\n  - cluster\n\n    cluster模式中,Driver是运行在Yarn管理应用程序主进程中,客户端提交完任务后就可以退出了,不适合交互性任务\n\n- Yarn模式和其他cluster模式不一样\n\n  - 其他模式的--master参数必须指定到master的地址\n  - yarn模式--master只需要写一个yarn就行,yarn会从配置文件中获取到ResourceManager的地\n\n- 启动spark集群命令\n\n  ```bash\n  spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]\n  ```\n\n  例如\n\n  ```bash\n  spark-submit --class org.apache.spark.examples.SparkPi \\\n      --master yarn \\\n      --deploy-mode cluster \\\n      --driver-memory 4g \\\n      --executor-memory 2g \\\n      --executor-cores 1 \\\n      --queue thequeue \\\n      examples/jars/spark-examples*.jar \\\n      10\n  ```\n\n  如果需要使用yarn client模式,只需把--deploy-mode设置为client\n\n  ```bash\n  spark-submit --class org.apache.spark.examples.SparkPi \\\n      --master yarn \\\n      --deploy-mode cluster \\\n      --driver-memory 4g \\\n      --executor-memory 2g \\\n      --executor-cores 1 \\\n      --queue thequeue \\\n      examples/jars/spark-examples*.jar \\\n      10\n  ```\n\n  \n\n## [Adding Other JARs](http://spark.apache.org/docs/latest/running-on-yarn.html#adding-other-jars)\n\n- 集群模式中,Driver不是运行在client端,所以使用`SparkContext.addJar`并没有用,如果需要添加额外的依赖包,需要使用--jars传入,多个jar包使用逗号分隔\n\n## [Preparations](http://spark.apache.org/docs/latest/running-on-yarn.html#preparations)\n\n![image-20200328140011807](/images/imageimage-20200328140011807.png)\n\n![image-20200328140028637](/images/imageimage-20200328140028637.png)\n\n为了让Spark能在Yarn上运行,需要指定`spark.yarn.archive`或者`spark.yarn.jars`属性,如果两个都没指定,Spark每次任务运行的时候就把`$SPARK_HOME/jars`下的所有jar包上传到分布式缓存中\n\n没设置时\n\n![image-20200328162119061](/images/imageimage-20200328162119061.png)\n\n设置之后\n\n![image-20200328165016988](/images/imageimage-20200328165016988.png)\n\nspark-default.conf\n\n```conf\nspark.yarn.jars hdfs:///spark/*.jar\n```\n\n需要把jar包上传到/spark目录,这样就不会每次都上传jar包到hdfs了\n\n\n\n## [Debugging your Application](http://spark.apache.org/docs/latest/running-on-yarn.html#debugging-your-application)\n\n- 如果开启了yarn日志收集功能(配置了` yarn.log-aggregation-enable`=true),日志会拷贝到yarn上,然后删除本地日志,如果要看日志,需要使用命令\n\n  ```bash\n  yarn logs -applicationId <app ID>\n  ```\n\n  这个命令会打印出指定appId的所有日志信息\n\n  你也可以通过hdfs shell和相关api获取\n\n  你可以通过`yarn.nodemanager.remote-app-log-dir` and `yarn.nodemanager.remote-app-log-dir-suffix`来定位你的日志到底是哪一个\n\n  这些日在在SparkUI中也可以查看到,在Executor页签中\n\n  你需要保证Spark日志服务和Yarn的日志服务都是启动的,并且  `yarn-site.xml`中的`yarn.log.server.url`属性配置正确\n\n- 如果没有开启日志收集功能,则日志会存在本地,你可以在 `$HADOOP_HOME/logs/user`中查看到\n\n- 为了检查看每个Container的环境,你需要配置`yarn.nodemanager.delete.debug-delay-sec`多久删除本地的日志,这样你就可以进入事先配置的`yarn.nodemanager.local-dirs`目录下查看每个容器的环境情况,下面包含了任务执行的script, JARs和所有的环境变量\n\n- 为了使用自己的log4j来打印日志,可以在spark-submit脚本中,把自己的log4j.properties通过--files传入\n\n  或者在`spark.driver.extraJavaOptions`中设置'-Dlog4j.configuration=< location of configuration file>'\n\n  当然你可以直接修改$SPARK_HOME/conf/log4j.properties\n\n\n\n## [Important notes](http://spark.apache.org/docs/latest/running-on-yarn.html#important-notes)\n\n- Yarn配置什么调度器就使用什么方式调度\n\n- 在cluster模式中,executor和driver使用的本地目录可以通过`yarn.nodemanager.local-dirs`来指定\n\n  如果p配置了`spark.local.dir`,则会忽略掉yarn的配置\n\n  client模式可以指定`spark.local.dir`,因为Driver并不是运行在yarn集群中的\n\n- `--files` 和`--archives`选项都可以使用类似Hadoop中`#`的用法(和别名类似,不需要每次写全路径)\n\n- cluster模式中使用`--jars`来指定本地的jars包路径,但是如果使用的是HDFS, HTTP, HTTPS, or FTP的文件则不需要\n\n","tags":["Spark","SparkCore"],"categories":["Spark"]},{"title":"SparkCore基础-Action-2","url":"/2019/04/08/Spark-SparkCore-2/","content":"# SparkCore-2\n\n### 新建工程\n\n- 新建maven父工程\n\n- 新建Module>maven\n  - core\n  - streaming\n  - ml\n  - hbase\n\n## SparkCore\n\n### Transformation\n\n#### map\n\n迭代每一个元素\n\n#### mapPartition\n\n迭代每个分区\n\n#### mapPartitionWithIndex\n\n#### grom\n\n#### filter\n\n#### sample\n\n取样\n\n#### zip\n\n拉链\n\n>  注意点\n>\n> spark和scala不同,spark中必须保证:\n>\n> - 元素数目相同\n>\n> - 分区数相同\n\n#### zipWithIndex\n\n\n\n#### union\n\n#### intersection\n\n交集,去重\n\n#### substract\n\n差集,不去重\n\n#### cartesian\n\n卡笛尔集\n\n#### distinct\n\n去重\n\n分区数不变\n\n#### sortBy\n\n#### sorkByKey\n\nKV类型\n\n#### groupBy\n\n这个出来的是(key,Interator),需要对iteration的内容进行进一步的处理\n\n#### groupByKey\n\n出来(k,v)\n\n#### mapValues\n\n#### reduceByKey\n\n先本地聚合,再shuffle\n\n#### join\n\ninner join\n\n#### leftOuterJoion\n\n#### rightOuterJoin\n\n#### fullOutJoin\n\n#### coalesce\n\n由多变少\n\n指定第二个参数true之后,可以实现少变多,重新分区1\n\n#### repartition\n\n遇到shuffle就切分statge\n\n\n\n","tags":["Spark","SparkCore"],"categories":["Spark"]},{"title":"SparkCore基础-Action","url":"/2019/04/07/Spark-SparkCore-action/","content":"## Spark Core-Action\n\n## Action\n\n### take\n\ntake使用需要慎重,避免冲爆Driver端内存\n\n```scala\n/**\n * Take the first num elements of the RDD. It works by first scanning one partition, and use the\n * results from that partition to estimate the number of additional partitions needed to satisfy\n * the limit.\n * 先扫描一个分区,再估算还需要扫描多少个其他分区才能满足\n *\n * @note This method should only be used if the resulting array is expected to be small, as\n * all the data is loaded into the driver's memory.\n * @note Due to complications in the internal implementation, this method will raise\n * an exception if called on an RDD of `Nothing` or `Null`.\n */\ndef take(num: Int): Array[T] = withScope {\n  val scaleUpFactor = Math.max(conf.getInt(\"spark.rdd.limit.scaleUpFactor\", 4), 2)\n  if (num == 0) {\n    new Array[T](0)\n  } else {\n    val buf = new ArrayBuffer[T]\n    val totalParts = this.partitions.length\n    var partsScanned = 0\n    while (buf.size < num && partsScanned < totalParts) {\n      // The number of partitions to try in this iteration. It is ok for this number to be\n      // greater than totalParts because we actually cap it at totalParts in runJob.\n      var numPartsToTry = 1L\n      //还剩多少没取\n      val left = num - buf.size\n      //如果不是第一次扫描\n      if (partsScanned > 0) {\n        // If we didn't find any rows after the previous iteration, quadruple and retry.\n        // Otherwise, interpolate the number of partitions we need to try, but overestimate\n        // it by 50%. We also cap the estimation in the end.\n        if (buf.isEmpty) {\n          //之前扫描的结果集是空的,说明之前都没扫到,扫描范围扩充为原来的N倍\n          numPartsToTry = partsScanned * scaleUpFactor\n        } else {\n          // As left > 0, numPartsToTry is always >= 1\n          //如果之前扫描到了,但数量还不够,采用插值法,增加50%的扫描范围\n          numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt\n          numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor)\n        }\n      }\n      //计算要扫描第几个分区\n      val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt)\n      val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p)\n\n      res.foreach(buf ++= _.take(num - buf.size))\n      partsScanned += p.size\n    }\n\n    buf.toArray\n  }\n}\n```\n\n### count\n\n```scala\n//返回RDD中的元素\n//Utils.getIteratorSize就是把集合的迭代器数一遍\ndef count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum\n\ndef getIteratorSize(iterator: Iterator[_]): Long = {\n    var count = 0L\n    while (iterator.hasNext) {\n        count += 1L\n        iterator.next()\n    }\n    count\n}\n```\n\n### top和takeOrdered\n\n取topN个元素,这个是全局的,所以注意只能在数据量小的时候使用,否则可能会冲爆Driver端内存\n\n```scala\n/**\nThis method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver's memory.\n\nsc.parallelize(Seq(10, 4, 2, 12, 3)).top(1)\n// returns Array(12)\nsc.parallelize(Seq(2, 3, 4, 5, 6)).top(2)\n// returns Array(6, 5)\n\n*/\ndef top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope {\n    //使用柯里化的方式,第一个是要取多少个,第二按降序排列\n    takeOrdered(num)(ord.reverse)\n}\n/**\n*   sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1)\n*   // returns Array(2)\n*\n*   sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2)\n*   // returns Array(2, 3)\n* }}}\n*\n* @note This method should only be used if the resulting array is expected to be small\n* as all the data is loaded into the driver's memory.\n*/\ndef takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope {\n    //取0个,直接返回空\n    if (num == 0) {\n        Array.empty\n    } else {\n        //每个partition取topN\n        val mapRDDs = mapPartitions { items => \n            // Priority keeps the largest elements, so let's reverse the ordering.\n            //构建有界优先队列(N个长度)\n            val queue = new BoundedPriorityQueue[T](num)(ord.reverse)\n            //将数据加进去\n            queue ++= collectionUtils.takeOrdered(items, num)(ord)\n            Iterator.single(queue)\n                                    }\n        if (mapRDDs.partitions.length == 0) {\n            Array.empty\n        } else {\n            //归并一下,因为是有界优先队列,后面大的数据会替换掉小的\n            mapRDDs.reduce { (queue1, queue2) =>\n                queue1 ++= queue2\n                queue1\n                           }.toArray.sorted(ord)\n        }\n    }\n}\n```\n\n可以看出top方法的takeOrdered传入的是一个隐式转换:降序,所以如果需要升序,可以直接takeOrdered\n\n```scala\nobject ActionTes {\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf()\n    conf.setMaster(\"local\")\n    conf.setAppName(\"app\")\n    val sc = new SparkContext(conf)\n    val list = List(1, 2, 3)\n    val value = sc.parallelize(list)\n    //升序\n    value.takeOrdered(2)\n    //降序\n    value.takeOrdered(2)(Ordering.by(-_))\n  }\n}\n```\n\n### reduce\n\n两两合并\n\n### countByKey\n\n针对PairRDDFunction\n\n计算key出现的次数\n\n使用场景:数据倾斜key检查\n\n```scala\ndef countByKey(): Map[K, Long] = self.withScope {\n  self.mapValues(_ => 1L).reduceByKey(_ + _).collect().toMap\n}\n```\n\n### foreach\n\n对集合中的每一个元素作用一个function\n\n### foreachPartition\n\n针对每个分区作用一个函数,也就是说一批一批的处理,一般效率会比foreach效率要高\n\n值得注意的是,foreachPartition本身是没有返回值的,但是使用了sc.runJob,这个是有返回值的\n\n如果分区的内容很大,可能会出现OOM\n\n```scala\n/**\n * Applies a function f to each partition of this RDD.\n */\ndef foreachPartition(f: Iterator[T] => Unit): Unit = withScope {\n  val cleanF = sc.clean(f)\n  sc.runJob(this, (iter: Iterator[T]) => cleanF(iter))\n}\n/**\n* Run a job on all partitions in an RDD and return the results in an array.\n*\n* @param rdd target RDD to run tasks on\n* @param func a function to run on each partition of the RDD\n* @return in-memory collection with a result of the job \n* (each collection element will contain a result from one partition)\n* 返回一个任务结果的集合到内存中\n*/\ndef runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = {\nrunJob(rdd, func, 0 until rdd.partitions.length)\n}\n```\n\n## DataSource\n\n### saveTextFile\n\n将rdd作为文本按分区写出去\n\n先检查输出路径,存在报错\n\n使用hadoop的output检查\n\n```scala\ndef saveAsTextFile(path: String): Unit = withScope {\n\t// https://issues.apache.org/jira/browse/SPARK-2075\n\t//\n\t// NullWritable is a `Comparable` in Hadoop 1.+, so the compiler cannot find an implicit\n\t// Ordering for it and will use the default `null`. However, it's a `Comparable[NullWritable]`\n\t// in Hadoop 2.+, so the compiler will call the implicit `Ordering.ordered` method to create an\n\t// Ordering for `NullWritable`. That's why the compiler will generate different anonymous\n\t// classes for `saveAsTextFile` in Hadoop 1.+ and Hadoop 2.+.\n\t//\n\t// Therefore, here we provide an explicit Ordering `null` to make sure the compiler generate\n\t// same bytecodes for `saveAsTextFile`.\n\t// 将输出以<NullWritable,Text>的形式写出去\n\tval nullWritableClassTag = implicitly[ClassTag[NullWritable]]\n\tval textClassTag = implicitly[ClassTag[Text]]\n\tval r = this.mapPartitions { iter =>\n\t  val text = new Text()\n\t  iter.map { x =>\n\t    text.set(x.toString)\n\t    (NullWritable.get(), text)\n\t  }\n\t}\n\t//RDD.rddToPairRDDFunctions生成PairRDDFunctions,\n\t//采用hdfs默认的FileOutputFormat实现TextOutputFormat\n\tRDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null).saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)\n}\n\n//底层抵用hadoop的读写进行保存文件\n/**\n* Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class\n* supporting the key and value types K and V in this RDD.\n*/\ndef saveAsHadoopFile[F <: OutputFormat[K, V]](\n                                             path: String)(implicit fm: ClassTag[F]): Unit = self.withScope {\n\tsaveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[Class[F]])\n}\n\n\ndef saveAsHadoopFile(\n      path: String,\n      keyClass: Class[_],\n      valueClass: Class[_],\n      outputFormatClass: Class[_ <: OutputFormat[_, _]],\n      conf: JobConf = new JobConf(self.context.hadoopConfiguration),\n      codec: Option[Class[_ <: CompressionCodec]] = None): Unit = self.withScope {\n    // Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038).\n    val hadoopConf = conf\n    //设置hdfs的输出K,V类型\n    hadoopConf.setOutputKeyClass(keyClass)\n    hadoopConf.setOutputValueClass(valueClass)\n    //其实就是TextOutputFormat\n    conf.setOutputFormat(outputFormatClass)\n    for (c <- codec) {\n      //如果设置了压缩\n      hadoopConf.setCompressMapOutput(true)\n      hadoopConf.set(\"mapreduce.output.fileoutputformat.compress\", \"true\")\n      hadoopConf.setMapOutputCompressorClass(c)\n      hadoopConf.set(\"mapreduce.output.fileoutputformat.compress.codec\", c.getCanonicalName)\n      hadoopConf.set(\"mapreduce.output.fileoutputformat.compress.type\",\n        CompressionType.BLOCK.toString)\n    }\n\n    // Use configured output committer if already set\n    if (conf.getOutputCommitter == null) {\n      hadoopConf.setOutputCommitter(classOf[FileOutputCommitter])\n    }\n\n    // When speculation is on and output committer class name contains \"Direct\", we should warn\n    // users that they may loss data if they are using a direct output committer.\n    val speculationEnabled = self.conf.getBoolean(\"spark.speculation\", false)\n    val outputCommitterClass = hadoopConf.get(\"mapred.output.committer.class\", \"\")\n    if (speculationEnabled && outputCommitterClass.contains(\"Direct\")) {\n      val warningMessage =\n        s\"$outputCommitterClass may be an output committer that writes data directly to \" +\n          \"the final location. Because speculation is enabled, this output committer may \" +\n          \"cause data loss (see the case in SPARK-10063). If possible, please use an output \" +\n          \"committer that does not have this behavior (e.g. FileOutputCommitter).\"\n      logWarning(warningMessage)\n    }\n    //设置输出目录\n    FileOutputFormat.setOutputPath(hadoopConf,\n      SparkHadoopWriterUtils.createPathFromString(path, hadoopConf))\n    saveAsHadoopDataset(hadoopConf)\n}\n/**\n* Output the RDD to any Hadoop-supported storage system, using a Hadoop JobConf object for\n* that storage system. The JobConf should set an OutputFormat and any output paths required\n* (e.g. a table name to write to) in the same way as it would be configured for a Hadoop\n* MapReduce job.\n*/\ndef saveAsHadoopDataset(conf: JobConf): Unit = self.withScope {\nval config = new HadoopMapRedWriteConfigUtil[K, V](new SerializableJobConf(conf))\n\tSparkHadoopWriter.write(\n\t  rdd = self,\n\t  config = config)\n}\n\n/** Basic work flow of this command is:\n* 1. Driver side setup, prepare the data source and hadoop configuration for the write job to\n*    be issued.\n* 2. Issues a write job consists of one or more executor side tasks, each of which writes all\n*    rows within an RDD partition.\n* 3. If no exception is thrown in a task, commits that task, otherwise aborts that task;  If any\n*    exception is thrown during task commitment, also aborts that task.\n* 4. If all tasks are committed, commit the job, otherwise aborts the job;  If any exception is\n*    thrown during job commitment, also aborts the job.\n*/\ndef write[K, V: ClassTag](\n      rdd: RDD[(K, V)],\n      config: HadoopWriteConfigUtil[K, V]): Unit = {\n    // Extract context and configuration from RDD.\n    val sparkContext = rdd.context\n    val commitJobId = rdd.id\n\n    //获取jobId\n    // Set up a job.\n    val jobTrackerId = createJobTrackerID(new Date())\n    //创建上下文\n    val jobContext = config.createJobContext(jobTrackerId, commitJobId)\n    //设置Output Class\n    config.initOutputFormat(jobContext)\n\n    //判断K,V类型,输出目录是否存在\n    // Assert the output format/key/value class is set in JobConf.\n    config.assertConf(jobContext, rdd.conf)\n\n    val committer = config.createCommitter(commitJobId)\n    committer.setupJob(jobContext)\n\n    // Try to write all RDD partitions as a Hadoop OutputFormat.\n    try {\n      val ret = sparkContext.runJob(rdd, (context: TaskContext, iter: Iterator[(K, V)]) => {\n        // SPARK-24552: Generate a unique \"attempt ID\" based on the stage and task attempt numbers.\n        // Assumes that there won't be more than Short.MaxValue attempts, at least not concurrently.\n        val attemptId = (context.stageAttemptNumber << 16) | context.attemptNumber\n\n        executeTask(\n          context = context,\n          config = config,\n          jobTrackerId = jobTrackerId,\n          commitJobId = commitJobId,\n          sparkPartitionId = context.partitionId,\n          sparkAttemptNumber = attemptId,\n          committer = committer,\n          iterator = iter)\n      })\n\n      committer.commitJob(jobContext, ret)\n      logInfo(s\"Job ${jobContext.getJobID} committed.\")\n    } catch {\n      case cause: Throwable =>\n        logError(s\"Aborting job ${jobContext.getJobID}.\", cause)\n        committer.abortJob(jobContext)\n        throw new SparkException(\"Job aborted.\", cause)\n    }\n}\n\n//检查输出\noverride def assertConf(jobContext: NewJobContext, conf: SparkConf): Unit = {\n    val outputFormatInstance = getOutputFormat()\n    val keyClass = getConf.getOutputKeyClass\n    val valueClass = getConf.getOutputValueClass\n    //这个基本不可能,他自己带过来的TextOutputFormat\n    if (outputFormatInstance == null) {\n      throw new SparkException(\"Output format class not set\")\n    }\n    //这个就是NullWritable\n    if (keyClass == null) {\n      throw new SparkException(\"Output key class not set\")\n    }\n    //这个是Text\n    if (valueClass == null) {\n      throw new SparkException(\"Output value class not set\")\n    }\n    SparkHadoopUtil.get.addCredentials(getConf)\n\n    logDebug(\"Saving as hadoop file of type (\" + keyClass.getSimpleName + \", \" +\n      valueClass.getSimpleName + \")\")\n\n    if (SparkHadoopWriterUtils.isOutputSpecValidationEnabled(conf)) {\n      // FileOutputFormat ignores the filesystem parameter\n      val ignoredFs = FileSystem.get(getConf)\n        //检查输出路径是否存在文件,存在就报错\n      getOutputFormat().checkOutputSpecs(ignoredFs, getConf)\n    }\n}\n\npublic void checkOutputSpecs(FileSystem ignored, JobConf job) throws FileAlreadyExistsException, InvalidJobConfException, IOException {\n    Path outDir = getOutputPath(job);\n    if (outDir == null && job.getNumReduceTasks() != 0) {\n        throw new InvalidJobConfException(\"Output directory not set in JobConf.\");\n    } else {\n        if (outDir != null) {\n            FileSystem fs = outDir.getFileSystem(job);\n            outDir = fs.makeQualified(outDir);\n            setOutputPath(job, outDir);\n            TokenCache.obtainTokensForNamenodes(job.getCredentials(), new Path[]{outDir}, job);\n            if (fs.exists(outDir)) {\n                //如果目录存在就报错\n                throw new FileAlreadyExistsException(\"Output directory \" + outDir + \" already exists\");\n            }\n        }\n\n    }\n}\n```\n\n\n\n### saveAsTextFile\n\n以压缩的形式写出去\n\n### saveAsObjectFile\n\n保存对象,这个是需要序列化的,可以使用自带序列化的case class\n\n### objectFile\n\n读取对象文件,反序列化\n\n\n\nApplication=1Driver+n Executor\n\nDriver => main中创建sc\n\nTask=>最小执行单位,map,filter,...\n\nWorkNode=>NM\n\nJob=>一个action就是一个job,action in stage\n\nStage=>遇到shuffle就是一个Stage","tags":["Spark","SparkCore"],"categories":["Spark"]},{"title":"Flume稍复杂的一些玩意","url":"/2019/04/06/Flume-稍复杂的一些玩意/","content":"# Flume稍复杂的一些玩意\n\n## Flume的稍复杂配置\n\n### 两个agent的传递\n\nagent A ->发送数据到agent B\n\n![image-20200229143256301](/images/image/image-20200229143256301.png)\n\na.conf\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F ~/usr/software/test.txt\n\n\n# Describe the sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = 0.0.0.0\na1.sinks.k1.port = 44441\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\nb.conf\n\n```properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = avro\na1.sources.r1.bind = 0.0.0.0\na1.sources.r1.port = 44441\n\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\n**启动**\n\n注意启动顺序\n\n```bash\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/b.conf\n```\n\n```bash\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/a.conf\n```\n\n### 两个source\n\n采集多个渠道的日志\n\n![image-20200229162321779](/images/image/image-20200229162321779.png)\n\ntwo_source.conf\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1 r2\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /usr/software/test.txt\n\na1.sources.r2.type = netcat\na1.sources.r2.bind = 0.0.0.0\na1.sources.r2.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sources.r2.channels = c1\na1.sinks.k1.channel = c1\n```\n\n**启动**\n\n```bash\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/two_source.conf\n```\n\n之后可以telnet 44444端口和写文件进行测试\n\n\n\n### 两个Sink\n\n上游下来一份数据,写到两个位置\n\n![image-20200229145931379](/images/image/image-20200229145931379.png)\n\ntwo_sink.conf\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1 k2\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /usr/software/test.txt\n\n# Describe the sink\na1.sinks.k1.type = logger\n\na1.sinks.k2.type = hdfs\na1.sinks.k2.hdfs.path = hdfs://hadoop001:9000/flume/test\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c1\n```\n\n**启动**\n\n```bash\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/two_sink.conf\n```\n\n但是启动之后会发现,有一些数据在hdfs上,有的在logger中,由此可见,flume默认同一份你数据只会发一次\n\n### 两个channel和两个Sink\n\n前面的两个Sink似乎并不是我们想要的结果,我们希望写到两个位置,上面虽然写道了两个位置,但是都不是完整的,所以还需要调整一下\n\n![image-20200229162902113](/images/image/image-20200229162902113.png)\n\n\n\nc2s2.conf\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1 k2\na1.channels = c1 c2\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /usr/software/test.txt\n\n# Describe the sink\na1.sinks.k1.type = logger\n\na1.sinks.k2.type = hdfs\na1.sinks.k2.hdfs.path = hdfs://hadoop001:9000/flume/test\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c2.type = memory\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1 c2\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c2\n```\n\n**启动**\n\n```bash\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/c2s2.conf\n```\n\n> 多个个Channel其实涉及到一个ChannelSelector的概念,但是ChannelSelector的策略默认是replicating复制策略,正是我们需要的,所以就不用指定了\n>\n> a1.sources.r1.selector.type=....\n>\n> a1.sources.r1.selector.optional=....\n\n### Multiplexing Channel Selector\n\n有时候日志是从不同的地方传过来的,携带了不同的标识信息header,需要存放不同的地方或者归类\n\n![image-20200229170325938](/images/image/image-20200229170325938.png)\n\n因为需要添加header,所以需要增加Interceptor\n\nsink1.conf\n\n```properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = 0.0.0.0\na1.sources.r1.port = 44441\n\n# Describe the sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = 0.0.0.0\na1.sinks.k1.port = 44444\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\n# 添加CN Header\na1.sources.r1.interceptors = i1\na1.sources.r1.interceptors.i1.type = static\na1.sources.r1.interceptors.i1.key = header\na1.sources.r1.interceptors.i1.value = CN\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\nsink2.conf\n\n```properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /usr/software/test.txt\n\n\n# Describe the sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = 0.0.0.0\na1.sinks.k1.port = 44444\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\n# 添加EN Header\na1.sources.r1.interceptors = i1\na1.sources.r1.interceptors.i1.type = static\na1.sources.r1.interceptors.i1.key = header\na1.sources.r1.interceptors.i1.value = EN\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\nsink3.conf\n\n```properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = TAILDIR\na1.sources.r1.filegroups = f1\na1.sources.r1.filegroups.f1 = /usr/software/test/.*.log\n\n# Describe the sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = 0.0.0.0\na1.sinks.k1.port = 44444\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\n#不加Header\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\n接收端的sink-all.conf\n\n```properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1 k2 k3\na1.channels = c1 c2 c3\n\n# Describe/configure the source\na1.sources.r1.type = avro\na1.sources.r1.bind = 0.0.0.0\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path = hdfs://hadoop001:9000/flume/cn\na1.sinks.k1.hdfs.writeFormat = Text\na1.sinks.k1.hdfs.fileType = DataStream\n\na1.sinks.k2.type = hdfs\na1.sinks.k2.hdfs.path = hdfs://hadoop001:9000/flume/en\na1.sinks.k2.hdfs.writeFormat = Text\na1.sinks.k2.hdfs.fileType = DataStream\n\na1.sinks.k3.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c2.type = memory\na1.channels.c3.type = memory\n\na1.sources.r1.selector.type = multiplexing\na1.sources.r1.selector.header = header\na1.sources.r1.selector.mapping.CN = c1\na1.sources.r1.selector.mapping.EN = c2\na1.sources.r1.selector.default = c3\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1 c2 c3\na1.sinks.k1.channel = c1\na1.sinks.k1.channel = c2\na1.sinks.k1.channel = c3\n```\n\n**启动**\n\n注意启动顺序\n\n先启动sink-all\n\n```bash\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/sink-all.conf\n```\n\n启动sink 1,2,3\n\n```bash\n# 使用telnet 44441端口测试\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/sink1.conf\n\n# 监听的是 /usr/software/test.txt\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/sink2.conf\n\n# 监听的是/usr/software/test/.*.log\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/sink-3.conf\n```\n\n使用对应的方式进行测试,即可\n\n## Sink Processor\n\n### FailOver配置\n\n容错配置和两个Sink类似,但是正常情况下,只会用优先级高的Sink,当另一个Sink不可用的时候才会使用备用的Sink\n\nfirst.conf\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = avro\na1.sources.r1.bind = 0.0.0.0\na1.sources.r1.port = 44441\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\nsecond.conf\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = avro\na1.sources.r1.bind = 0.0.0.0\na1.sources.r1.port = 44442\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\nfailover.conf\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1 k2\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /usr/software/test.txt\n\n# Describe the sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = 0.0.0.0\na1.sinks.k1.port = 44441\n\na1.sinks.k2.type = avro\na1.sinks.k2.hostname = 0.0.0.0\na1.sinks.k2.port = 44442\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c1\n\n# 设置sink的优先级,sink process为容错机制\na1.sinkgroups = g1\na1.sinkgroups.g1.sinks = k1 k2\na1.sinkgroups.g1.processor.type = failover\na1.sinkgroups.g1.processor.priority.k1 = 5\na1.sinkgroups.g1.processor.priority.k2 = 10\na1.sinkgroups.g1.processor.maxpenalty = 10000\n```\n\n**启动**\n\n启动k1,k2\n\n```bash\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/first.conf\n\n\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/second.conf\n```\n\n启动failover.conf\n\n```bash\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/failover.conf\n```\n\n然后追加内容到/usr/software/test.txt就可以看到变化了,当两个都可用的时候,44442端口优先使用,当关闭44442对应的agent,则会自动切换到44441的agent\n\n>可以发现,**优先的值越高,优先级越高**\n\n### LoadBalance配置\n\nload1.conf\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = avro\na1.sources.r1.bind = 0.0.0.0\na1.sources.r1.port = 44441\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\nload2.conf\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = avro\na1.sources.r1.bind = 0.0.0.0\na1.sources.r1.port = 44442\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\nload.conf\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1 k2\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /usr/software/test.txt\n\n# Describe the sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = 0.0.0.0\na1.sinks.k1.port = 44441\n\na1.sinks.k2.type = avro\na1.sinks.k2.hostname = 0.0.0.0\na1.sinks.k2.port = 44442\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c1\n\n# 配置负载均衡为随机数机制\na1.sinkgroups = g1\na1.sinkgroups.g1.sinks = k1 k2\na1.sinkgroups.g1.processor.type = load_balance\na1.sinkgroups.g1.processor.backoff = true\na1.sinkgroups.g1.processor.selector = random\n```\n\n**启动**\n\n启动load1,load2\n\n```bash\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/load1.conf\n\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/load2.conf\n```\n\n启动load.conf\n\n```bash\n\nflume-ng agent \\\n--name 'a1' \\\n--conf $FLUEM_HOME/conf \\\n--conf-file $FLUME_HOME/myconf/load.conf\n```\n\n之后往/usr/software/test.txt添加内容可以看到两个都可以看到内容的变化,实现负载均衡.\n\n负载均衡还有一个机制:轮询","tags":["Flume"],"categories":["Flume"]},{"title":"Azkaban配置代理用户执行任务","url":"/2019/04/06/Azkaban-配置代理用户执行任务/","content":"\n# Azkaban配置代理用户执行任务\n\n有时候我们希望启动azkaban的用户和启动任务脚本使用不同的用户\n\n这时候我们可以使用azkaban的execute-as-user插件完成,参考文档[Plugin Configurations](https://azkaban.readthedocs.io/en/latest/configuration.html#plugin-configurations)\n\n```bash\n# 这里修正一下,execute-as-user.c的位置官网写成azkaban-common的目录\n# 但是3.81.0我在github在azkaban-util下面找到的\nscp ./az-exec-util/src/main/c/execute-as-user.c\n\n# 编译C文件,需要提前安装gcc\ngcc execute-as-user.c -o execute-as-user\n\n# 设置权限为root\nchown root execute-as-user\n# 这个权限比较有意思,---Sr-s---,不懂意思,很特殊的权限\nchmod 6050 execute-as-user\n```\n\n配置一下plugin\n\n$AZKABAN_HOME/plugins/jobtypes/commonprivate.properties\n\n```properties\nexecute.as.user=true\nazkaban.native.lib=/home/lurongjiang/azkaban-3.81.0\nproxy.user=\nazkaban.should.proxy=true\n```\n\n这样在project的.flow中就可以设置user.to.proxy参数来实现用户代理执行了\n\n> **注意**\n>\n> - 要代理的用户记得要在linux中存在\n> - 默认使用的是azkaban用户组,所以需要groupadd,或者参考文档中的另外一个参数\n> - 记得赋予proxy用户执行脚本的权限,否则百搭\n\ntest.flow\n\n```yaml\nconfig:\n  time: \"\"\n  user.to.proxy: lurongjiang\nnodes:\n  - name: jobA\n    type: command\n    config:\n      command: /home/lurongjiang/test/a.sh ${time}\n\n  - name: jobB\n    type: command\n    config:\n      command: /home/lurongjiang/test/b.sh ${time}\n  - name: jobC\n    type: command\n    config:\n      command: /home/lurongjiang/test/c.sh ${time}\n```\n\n","tags":["Azkaban"],"categories":["Azkaban"]},{"title":"SparkCore基础-1","url":"/2019/04/06/Spark-SparkCore-RDD/","content":"\n# SparkCore-RDD\n\n弹性分布式数据集\n\n- 不可变的\n- 这个集合可分区(拆开的)\n- 数据集的元素可以并行操作\n\nspark中,RDD是一个抽象类\n\nRDD实现了序列化接口和Logging\n\n## RDD五大特点\n\n- 有多个分区\n- 对RDD的计算操作就是对每一个分区的计算操作\n- RDD之间有依赖关系\n- KV类型的RDD有Partitioner分区器的概念\n- 数据位置有优先\n\n## SparkContext\n\nSpark上下文对象\n\n- 一个JVM只能有一个上下文对象.启动一个,必须关掉另一个\n\n## SparkConf\n\n- 传递自定义参数,必须是spark.开头\n- SparkContext必须设置master和appName,也就是conf必须包含(spark.master和spark.app.name)\n- 对于master和appName不要硬编码,而是通过脚本传进去\n\n\n\n## RDD\n\n### 1. RDD的创建\n\n#### parallelize\n\n将集合转成RDD对象\n\n- parallelize第二个参数就是split数\n\n- parallelize默认使用core数作为第二个默认参数,例如local[2],每次collect就是2\n\n### 2. 外部数据源\n\n### textFile\n\n读取文件作为RDD对象(MapPartitionRDD)\n\n- 跨节点(如standalone),这种方式必须保证所有节点的相同路径下都要有这个文件\n- 支持目录和通配符路径\n- 默认块大小128M\n\n### wholeTextFiles\n\n读取整个文件,返回一个(Int,String)的元组RDD\n\n### 3. 通过RDD转换获取RDD\n\n这个就是RDD五大特点的特点3\n\n## RDD操作\n\nRDD的操作包括两种:\n\n- transformation: 从RDD转换成另一个RDD\n- action: 把结果返回到Driver端\n\n### transformation\n\n如map,filter\n\n- 所有的transformation都是lazy模式的,不会立即执行的\n- 遇到action才会执行\n\n操作\n\n- map 作用在集合的每一个元素\n- filter\n- mapPartition 作用在集合的每一个partition\n- mapPartitionWithIndex 带分区id对每一个分区进行操作\n- mapValues 对K,V类型的集合中的每一个V进行操作\n- flatMap 对两层的结构压平成一层\n\n### action\n\n如reduce\n\n### persist/cache\n\n持久化或者缓存RDD到磁盘","tags":["Spark","SparkCore"],"categories":["Spark"]},{"title":"Flume使用教程","url":"/2019/04/04/Flume-基本使用/","content":"\n# Flume\n\nFlume是高可靠,高可用的,分布式的海量日志收集,聚合和传输的系统,\n\n## 数据流模型\n\n![image-20200225205404604](/images/image/image-20200225205404604.png)\n\nA Flume event is defined as a unit of data flow having a byte payload and an optional set of string attributes. A Flume agent is a (JVM) process that hosts the components through which events flow from an external source to the next destination (hop).\n\nA Flume source consumes events delivered to it by an external source like a web server. The external source sends events to Flume in a format that is recognized by the target Flume source. For example, an Avro Flume source can be used to receive Avro events from Avro clients or other Flume agents in the flow that send events from an Avro sink. A similar flow can be defined using a Thrift Flume Source to receive events from a Thrift Sink or a Flume Thrift Rpc Client or Thrift clients written in any language generated from the Flume thrift protocol.When a Flume source receives an event, it stores it into one or more channels. The channel is a passive store that keeps the event until it’s consumed by a Flume sink. The file channel is one example – it is backed by the local filesystem. The sink removes the event from the channel and puts it into an external repository like HDFS (via Flume HDFS sink) or forwards it to the Flume source of the next Flume agent (next hop) in the flow. The source and sink within the given agent run asynchronously with the events staged in the channel.\n\n- Flume中,数据流的单位称为事件(event)\n\n  它具有一个字节的有效载荷,还可以携带一系列的字符串属性.\n\n- Flume中Agent是一个JVM进程\n\n  Agent由一系列的组件组成,通过这些组件,将事件从外部数据源传到另一个位置\n\n- Flume源端部分(source)消费着来至外部数据源的事件,例如web服务端的\n\n  外部数据源需要以目标flume源端能够识别的形式发出.例如，Avro类型的flume源端可用于接收从Avro客户端发出的事件,或从其他的Avro类型的Flume发送端(sink)发出的事件\n\n- flume源端接收事件后,会将事件存入一个或者多个管道(Channel)\n\n  Flume的管道存储所源端接收到的事件,直到它Flume发送端消费了这些事件.File Channel就是一个例子,它将事件备份在本地文件系统中\n\n- flume的发送端消费着Channel中的事件,并把它推送HDFS等外部仓库中(通过HDFS类型的Sink发送)\n\n## Agent的三个组件\n\n**Source**\n\n数据源\n\n**Channel** \n\n通道,缓冲\n\n数据存在哪里\n\n**Sink** \n\n数据输出到哪里\n\n## 下载\n\n```bash\nwget http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2.tar.gz\n```\n\n## 配置\n\n所有的配置都参考官网的配置,只要配置agent,source,channel,sink即可\n\nhttp://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#setting-up-an-agent\n\n其中所有的粗体都是必须要配置的,其他普通的属性是可选的\n\n### 样例\n\n为了便于配置,可以先把这个模板copy到notepad,根据需要改一下就可以\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\n# agent a1的 source 的名字设置为r1\na1.sources = r1 \n# agent a1的 sinks 的名字设置为k1\na1.sinks = k1\n# agent a1的 channels 的名字设置为c1\na1.channels = c1\n\n# Describe/configure the source\n# agent a1的 source r1 的类型设置为netcat\na1.sources.r1.type = netcat\n# agent a1的 source r1 的绑定的host和port\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\n# agent a1的 sinks k1 的类型为logger类型,也就是控制台输出\na1.sinks.k1.type = logger\nc\n# Use a channel which buffers events in memory\n# agent a1的 channels c1 的类型为memory类型,也就是基于内存进行缓冲,所以flume对内存还是由一定要求的\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\n# agent a1的 source,channel,sink如何连接\n# 这里配置了agent a1的source r1的channel为c1\n# 也就是说,事件从在被sink消费之前被保存在c1的channel中\na1.sources.r1.channels = c1\n# 这里配置了agent a1的sink k1的channel为c1,\n# 也就是说agent a1的sink k1要从c1 channel消费事件\na1.sinks.k1.channel = c1\n```\n\n**参数说明**\n\n- a1\n\n  这个是agent的名字,启动flume必须指定agent的名字,独一无二的即可,因为agent是一个独立jvm进程\n\n### netcat样例\n\nnetcat.conf\n\n```properties\nmyConsoleAgent.sources = source1\nmyConsoleAgent.sinks = sink1\nmyConsoleAgent.channels = channel1\n\n# 数据从哪里来\nmyConsoleAgent.sources.source1.type = netcat\nmyConsoleAgent.sources.source1.bind = localhost\nmyConsoleAgent.sources.source1.port = 44444\n\n# 输出到哪里去\nmyConsoleAgent.sinks.sink1.type = logger\n\n# 读取数据之后先放到哪里\nmyConsoleAgent.channels.channel1.type = memory\n\n\n# 将source,channel,sink连接起来\nmyConsoleAgent.sources.source1.channels = channel1\nmyConsoleAgent.sinks.sink1.channel = channel1\n```\n\n启动agent\n\n```bash\n$FLUME_HOME/bin/flume-ng \\\n--conf $FLUME_HOME \\\n--conf-file ./netcat.conf \\\n--name myConsoleAgent\n```\n\n### exec 命令行输入\n\nexec.conf\n\n```properties\nmyConsoleAgent.sources = source1\nmyConsoleAgent.sinks = sink1\nmyConsoleAgent.channels = channel1\n\n# 数据从哪里来\nmyConsoleAgent.sources.source1.type = exec\nmyConsoleAgent.sources.source1.command = tail -F /usr/software/flume-1.6.0-cdh5.16.2/input/exec.log \n\n\n# 输出到哪里去\nmyConsoleAgent.sinks.sink1.type = hdfs\nmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/exec\nmyConsoleAgent.sinks.sink1.hdfs.batchSize = 10\nmyConsoleAgent.sinks.sink1.hdfs.fileType = DataStream \nmyConsoleAgent.sinks.sink1.writeFormat = Text \n\n# 读取数据之后先放到哪里\nmyConsoleAgent.channels.channel1.type = memory\n\n\n# 将source,channel,sink连接起来\nmyConsoleAgent.sources.source1.channels = channel1\nmyConsoleAgent.sinks.sink1.channel = channel1\n\n```\n\n启动agent\n\n```bash\n$FLUME_HOME/bin/flume-ng \\\n--conf $FLUME_HOME \\\n--conf-file ./exec.conf \\\n--name myConsoleAgent\n```\n\n\n\n### spooldir 监听文件夹\n\nspool.conf\n\n```properties\nmyConsoleAgent.sources = source1\nmyConsoleAgent.sinks = sink1\nmyConsoleAgent.channels = channel1\n\n# 数据从哪里来\nmyConsoleAgent.sources.source1.type = spooldir\nmyConsoleAgent.sources.source1.spoolDir = /usr/software/flume-1.6.0-cdh5.16.2/input\nmemory.sources.source1.includePattern = *.log\n\n# 输出到哪里去\nmyConsoleAgent.sinks.sink1.type = hdfs\nmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/spool/%Y%m%d%H%M\nmyConsoleAgent.sinks.sink1.hdfs.batchSize = 10\nmyConsoleAgent.sinks.sink1.hdfs.fileType = DataStream \nmyConsoleAgent.sinks.sink1.writeFormat = Text \nmyConsoleAgent.sinks.sink1.hdfs.rollInterval=30\nmyConsoleAgent.sinks.sink1.hdfs.rollSize=1024\nmyConsoleAgent.sinks.sink1.hdfs.hdfs.rollCount=100\nmyConsoleAgent.sinks.sink1.hdfs.useLocalTimeStamp =true\n\n\n# 读取数据之后先放到哪里\nmyConsoleAgent.channels.channel1.type = memory\n\n\n# 将source,channel,sink连接起来\nmyConsoleAgent.sources.source1.channels = channel1\nmyConsoleAgent.sinks.sink1.channel = channel1\n```\n\n启动agent\n\n```bash\n$FLUME_HOME/bin/flume-ng \\\n--conf $FLUME_HOME \\\n--conf-file ./spool.conf \\\n--name myConsoleAgent\n```\n\n### taildir 监听文件夹(最常用的)\n\n这个监听最常用,也是必须掌握的,因为spoolDir并没有偏移量的概念,使用有很大的局限性\n\nspooldir的缺点\n\n- 每次采集完成之后,会在文件结尾设置Complete的后缀,而且之后不能再使用相同的文件名,否则报错\n- 采集完之后的文件不能再次写入,否则报错\n\n而taildir很好的使用了偏移量的概念,记录在一个json文件中,可以实现断点还原\n\n\n\ntaildir.conf\n\n```properties\nmyConsoleAgent.sources = source1\nmyConsoleAgent.sinks = sink1\nmyConsoleAgent.channels = channel1\n\n# 数据从哪里来\nmyConsoleAgent.sources.source1.type = TAILDIR\nmyConsoleAgent.sources.source1.filegroups = f1 f2\nmyConsoleAgent.sources.source1.filegroups.f1 = input/taildir/test1/hello.txt\nmyConsoleAgent.sources.source1.headers.f1.headerKey1 = value1\nmyConsoleAgent.sources.source1.filegroups.f2 = input/taildir/test2/.*.log\nmyConsoleAgent.sources.source1.headers.f2.headerKey1 = value2-1\nmyConsoleAgent.sources.source1.headers.f2.headerKey2 = value2-2\nmyConsoleAgent..sources.source1.maxBatchCount = 1000\nmyConsoleAgent.sources.source1.fileHeader = true\n\n \n\n# 输出到哪里去\nmyConsoleAgent.sinks.sink1.type = hdfs\nmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/tailDir/%Y%m%d%H%M\nmyConsoleAgent.sinks.sink1.hdfs.batchSize = 100\nmyConsoleAgent.sinks.sink1.hdfs.fileType = DataStream \nmyConsoleAgent.sinks.sink1.writeFormat = Text \nmyConsoleAgent.sinks.sink1.hdfs.rollInterval=30\nmyConsoleAgent.sinks.sink1.hdfs.rollSize=10240\nmyConsoleAgent.sinks.sink1.hdfs.hdfs.rollCount=100\nmyConsoleAgent.sinks.sink1.hdfs.useLocalTimeStamp =true\n\n\n# 读取数据之后先放到哪里\nmyConsoleAgent.channels.channel1.type = memory\n\n\n# 将source,channel,sink连接起来\nmyConsoleAgent.sources.source1.channels = channel1\nmyConsoleAgent.sinks.sink1.channel = channel1\n\n```\n\n启动agent\n\n```bash\n$FLUME_HOME/bin/flume-ng \\\n--conf $FLUME_HOME \\\n--conf-file ./taildir.conf \\\n--name myConsoleAgent\n```\n\n","tags":["Flume"],"categories":["Flume"]},{"title":"Azkaband的安装和使用","url":"/2019/04/03/Azkaban-的安装和使用/","content":"\n# Azkaband的安装和使用\n\nAzkaban可以设计很复杂的工作流,解决任务之间的依赖关系,还是很方便的\n\nAzkaban是一个由LinkedIn公司开发的分布式工作流管理框架,主要是为了解决Hadoop工作之间的依赖关系.例如,我们需要有顺序地执行任务,把ETL任务的数据导入到RMBS中,以提供数据分析支持.\n\n![image-20200220101138796](/images/image/image-20200220101138796.png)\n\n- Azkaban是一个开源的工作流管理框架\n- Azkaban是由LinkedIn(领英,还有一个比较出名的产品Kafka)公司创建,用来管理批处理工作流的任务调度\n- Azkaban提供了WebUI接口来维护工作流\n\n## Feature\n\n- Compatible with any version of Hadoop\n\n  兼容Hadoop各个版本\n\n- Easy to use web UI\n\n  提供易用的WebUI\n\n- Simple web and http workflow uploads\n\n  web和http方式上传工作流(这个被诟病了,应该提供拖拉拽的,写了配置还要上传,确实不方便)\n\n- Project workspaces\n\n- Scheduling of workflows\n\n  工作流调度\n\n- Modular and pluginable\n\n  模块化,可插拔\n\n- Authentication and Authorization\n\n  提供认证和授权\n\n- Tracking of user actions\n\n- Email alerts on failure and successes\n\n  任务失败和成功邮件告警\n\n- SLA alerting and auto killing\n\n  SLA告警和自动结束任务\n\n- Retrying of failed jobs\n\n  任务失败重试\n\n## 版本选择\n\n- 2.x\t基本可以不用看了\n- 3.x    当前推荐的\n\n## 模式\n\n- stand alone mode(or solo-server) 单个\n\n  - web server和executor server运行在同一个进程里\n  - 数据存储的数据库是内嵌的,不需要自己安装\n\n  - 主要使用在小规模场景中\n\n- Multiple executor mode \n\n  - 多用于生产\n  - 存储的DB应该是一个主从结构的MySQL\n  - web server和executor server运行在不同的host上,这样升级和维护时,互不影响\n\n## 下载和编译\n\nAzkban不直接提供安装包,需要自己从源码编译\n\nAzkaban是Gradle构建的,Java版本需要1.8以上\n\n```bash\nwget https://github.com/azkaban/azkaban/archive/3.81.0.tar.gz\ntar -zxvf ./3.81.0.tar.gz -C /usr/software\ncd /usr/software/azkaban-3.81.0\n./gradlew build installDist -x test\n```\n\n这个需要git,如果不安装会报错Failed to apply plugin [id 'com.cinnober.gradle.semver-git']\n\n![image-20200220105916429](/images/image/image-20200220105916429.png)\n\n所以先得安装git\n\n```bash\nsudo yum install -y git\n```\n\n执行有报错....缺少g++:Could not find Linker 'g++' in system path\n\n![image-20200220135312805](/images/image/image-20200220135312805.png)\n\n好吧在安装g++\n\n```bash\nsudo yum install g++\nsudo yum install -y gcc-c++*\n```\n\n## 部署\n\n编译完成后会出现3个目录,一个是azkaban-solo-server,azkaban-web-server,azkaban-exec-server,分别对应了单机版的azkaban-executor-server,azkaban-web-server和多实例的azkaban-executor-server\n\n我们只需要把他们目录下的build/distributions的压缩包拷贝出来就行\n\n### 单机版\n\n```bash\ntar -zxvf ./azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz\n```\n\n修改azkban的配置文件azkaban.properties,用默认也可以\n\n```bash\nvi conf/azkaban.properties\n```\n\n```properties\n# Azkaban Personalization Settings\n# 设置azkban的名称,这个是UI首页左上角,图标右侧上方的文字\nazkaban.name=LRJ\n# UI首页左上角,图标右侧下方的文字\nazkaban.label=MyAzkaban\nazkaban.color=#FF3601\nazkaban.default.servlet.path=/index\nweb.resource.dir=web/\n# 修改一下时区,免得调度时间不对\ndefault.timezone.id=Asia/Shanghai\n# Azkaban UserManager class\nuser.manager.class=azkaban.user.XmlUserManager\nuser.manager.xml.file=conf/azkaban-users.xml\n# Loader for projects\nexecutor.global.properties=conf/global.properties\nazkaban.project.dir=projects\ndatabase.type=h2\nh2.path=./h2\nh2.create.tables=true\n# Velocity dev mode\nvelocity.dev.mode=false\n# Azkaban Jetty server properties.\njetty.use.ssl=false\njetty.maxThreads=25\n# Web端口,端口占用的时候改一改\njetty.port=8081\n# Azkaban Executor settings\nexecutor.port=12321\n# mail settings\n# 邮件服务器设置\nmail.sender=\nmail.host=\n# 开启SSL设置\n# User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.\n# enduser -> myazkabanhost:443 -> proxy -> localhost:8081\n# when this parameters set then these parameters are used to generate email links.\n# if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.\n# azkaban.webserver.external_hostname=myazkabanhost.com\n# azkaban.webserver.external_ssl_port=443\n# azkaban.webserver.external_port=8081\n# 全局邮箱设置\njob.failure.email=\njob.success.email=\nlockdown.create.projects=false\ncache.directory=cache\n# JMX stats\njetty.connector.stats=true\nexecutor.connector.stats=true\n# Azkaban plugin settings\n# 插件设置\nazkaban.jobtype.plugin.dir=plugins/jobtypes\n# Number of executions to be displayed\nazkaban.display.execution_page_size=16\nazkaban.use.multiple.executors=true\n# Azkaban Ramp Feature Configuration\n#Ramp Feature Related\nazkaban.ramp.enabled=true\nazkaban.ramp.status.polling.enabled=true\nazkaban.ramp.status.polling.interval.min=30\nazkaban.ramp.status.push.interval.threshold=15\nazkaban.ramp.status.pull.interval.threshold=100\n```\n\n添加个用户名和密码,也可以使用默认的\n\n```xml\n<azkaban-users>\n  <user groups=\"azkaban\" password=\"azkaban\" roles=\"admin\" username=\"azkaban\"/>\n    <!--添加自己的用户-->\n  <user groups=\"lurongjiang\" password=\"lurongjiang\" roles=\"admin\" username=\"lurongjiang\"/>\n  <user password=\"metrics\" roles=\"metrics\" username=\"metrics\"/>\n  <role name=\"admin\" permissions=\"ADMIN\"/>\n  <role name=\"metrics\" permissions=\"METRICS\"/>\n</azkaban-users>\n```\n\n### 启动\n\n```bash\n./bin/start-solo.sh\n# 查看一下\njps -m\n```\n\n访问webUI端口就可以显示了,可以看到我们改的两个位置\n\n![image-20200220185537929](/images/image/image-20200220185537929.png)\n\n输入用户名和密码就就可以进去了\n\n![image-20200220185655686](/images/image/image-20200220185655686.png)\n\n\n\n\n\n### 使用\n\n参考[Create Flows](https://azkaban.readthedocs.io/en/latest/createFlows.html)\n\n- 创建一个名字为`flow20.project`的文件,写入内容:\n\n  ```yml\n  azkaban-flow-version: 2.0\n  ```\n\n  \n\n- 创建一个`basic.flow`的文件,写入内容:\n\n  ```yml\n  nodes:\n    - name: jobA\n      type: command\n      config:\n        command: echo \"This is an echoed text.\"\n  ```\n\n- 把两个文件打个zip包\n\n![image-20200220190504445](/images/image/image-20200220190504445.png)\n\n\n\n- 在WebUI中创建工程\n\n![image-20200220190731303](/images/image/image-20200220190731303.png)\n\n- 上传打包好的附件\n\n![image-20200220190849991](/images/image/image-20200220190849991.png)\n\n这样project就有任务了\n\n![image-20200220190959064](/images/image/image-20200220190959064.png)\n\n点击运行后就有任务执行历史了\n\n![image-20200220191106770](/images/image/image-20200220191106770.png)\n\n### 依赖型任务\n\n依赖型可以使用dependsOn来进行关联\n\n```yml\nnodes:\n  - name: jobC\n    type: noop\n    # jobC depends on jobA and jobB\n    dependsOn:\n      - jobA\n      - jobB\n\n  - name: jobA\n    type: command\n    config:\n      command: echo \"This is an echoed text.\"\n\n  - name: jobB\n    type: command\n    config:\n      command: pwd\n  - name: jobD\n    type: command\n    config:\n      command: echo 'I am D job'\n  - name: jobE\n    type: command\n    dependsOn:\n      - jobC\n      - jobD\n    config:\n      command: echo 'I am E'\n```\n\n这样就配置三个任务,其中A,B,D任务互不干扰,同时运行,jobC必须要等任务A,B执行完了才执行,E任务需要任务C,D都执行完才执行,依赖关系\n\n![image-20200220192931508](/images/image/image-20200220192931508.png)\n\n> 注意:\n>\n> - 文件和yml文件格式类似,但是千万不要用tab,这个不识别\\t\n> - 不同的project,需要多次创建project,同一个project,后一次上传的会覆盖掉前一次上传的\n> - .flow文件的文件名就是project中显示的flow的name\n\n点击job进去,可以对参数进行设置\n\n![image-20200220193646623](/images/image/image-20200220193646623.png)\n\n### 定时调度\n\n点击Schedule可以设置定时任务的周期,![image-20200220194133237](/images/image/image-20200220194133237.png)\n\n![image-20200220194308982](/images/image/image-20200220194308982.png)\n\n这样就可在Scheduling下看到定时任务列表,等待下一个时钟来临就会执行\n\n![image-20200220194504136](/images/image/image-20200220194504136.png)\n\n如果需要移除,可以RemoveSchedule","tags":["Azkaban"],"categories":["Azkaban"]},{"title":"Scala基础-隐式转换","url":"/2019/02/26/scala基础-隐式转换/","content":"\n# scala基础-隐式转换\n\n隐式转换:A=>B\n\n发现A不满足的情况下,自动转成B\n\n通过隐式转换可以对原来的类进行增强\n\n大家可能经常在博客上看到这样的例子,File本身没有read方法,这时可以通过隐式转换来对File的宫娥能进行增强\n\n## 隐式类型转换\n\n### 例子1:\"人\"会飞\n\n普通的人是不会飞的,超人会飞\n\n所以怎么才能把人转成超人呢,看例子\n\n```scala\n/**\n * 人只有一个name属性,并没有任何方法\n *\n * @param name name\n */\nclass Man(val name: String) {\n}\n\nclass SuperMan(val name: String) {\n  def fly(): Unit = {\n    println(s\"${name} can fly.....\")\n  }\n}\nobject TestMan2SuperMan {\n  def main(args: Array[String]): Unit = {\n    // 定义一个从Man到SuperMan的隐式转换,implicit修饰\n    implicit def man2SuperMan(man: Man): SuperMan = {\n      new SuperMan(man.name)\n    }\n    //new一个人,你可以发现\"人\"可以飞了\n    new Man(\"Zhangsan\").fly()\n  }\n}\n```\n\n可以发现,scala在执行时,发现Man是没用fly方法的,会尝试找一个隐式转换,接收Man类型,并且有fly方法的隐式转换来进行隐式的转成目标类型,执行指定的方法\n\n> 隐式转换的公式\n>\n> implicit def x2y(x):y=new y\n>\n> 函数接收一个普通的x,转成另一个东西的y\n\n### 例子2:File.read\n\nFile本身没有read方法,那么我们可以使用隐式转换来增强File,使它\"具有\"read功能\n\n```scala\nclass HenceFile(val file: File) {\n  def read() = {\n    var source: BufferedSource = null\n    try {\n      source = Source.fromFile(file.getPath)\n      source.mkString\n    } finally {\n      source.close()\n    }\n  }\n}\n\nobject FileHenceTest {\n  def main(args: Array[String]): Unit = {\n    //定义一个方法,接收普通的File,返回HenceFile,并用implicit修饰\n    implicit def read(file: File): HenceFile = new HenceFile(file)\n    //这样普通的File就可以read了\n    println(new File(\"input/test.txt\").read())\n  }\n}\n```\n\n可以看的出,隐式转换的流程:\n\n- 使用implicit关键字定义一个函数\n- 这个函数接受你要转换的普通类A,返回你想要的类B,这样在A没有的功能,而B有的,A也可以使用,从而达到对A类的增强\n\n## 隐式参数\n\n隐式参数,当函数发现找不到参数时会去找对应的隐式参数来填充\n\n```scala\nobject ImplicitParamTest {\n  def main(args: Array[String]): Unit = {\n    implicit val start =0\n    def add(num:Int)(implicit step:Int) ={\n      num+step\n    }\n    //sum使用了柯里化,但是第二个参数是隐式参数\n    //scala在运行时发现不带第二个参数,会去找有没有隐式的Int类型参数\n    //如果发现存在,就使用那个隐式参数来作为第二个值\n    println(add(25)) //25 \n    println(add(23)(24)) //47\n  }\n}\n```\n\n注意隐式参数必须放在最后面,如果有多个隐式参数,只需要写一个implicit,参数合起来\n\n```scala\n//如果只有一个隐式参数,隐式参数必须是最后面的,也就是implicit修饰最后一个\n// 否则报错,如add(x: Int)(implicit y: Int)(z: Int)就是错的\n//标识y,z都是隐式参数,但是只能写一个implicit\n// add(x: Int)(implicit y: Int, z: Int)\n\nimplicit val num = 25\ndef add(x: Int)(implicit y: Int, z: Int) = x + y + z\n\nprintln(add(25)) //75,因为y,z都是25\nprintln(add(25)(10,14)) //49\n```\n\n## 隐式类\n\n不需要new新的类型了,直接对增强类进行implicit进行修饰,这样就可以粗暴的进行增强\n\n```scala\nobject AddTest {\n  implicit class AddClass(str: String) {\n    def add(num: Int) = num + str.toInt\n  }\n  def main(args: Array[String]): Unit = {\n    println(\"12\".add(15)) //27\n  }\n}\n```\n\n字符串本来是没用add方法的,但是通过隐式类,很方便的进行类型转换和计算\n\n但是这个implicit class只能定义在object内,外面会报错","tags":["Scala"],"categories":["Scala"]},{"title":"scala基础-高阶函数","url":"/2019/02/17/scala基础-高阶函数/","content":"\n# scala基础-高阶函数\n\n## \"_\" 的使用\n\n- 将函数赋值给变量\n\n```scala\ndef describe(firstWorker:String,secondWorker:String)={\n    println(\"the first worker is \"+firstWorker)\n      println(\"the second worker is \"+secondWorker)\n}\nval hello1 = describe _ \nval hello2 = describe(_) \n\nhello1(\"Zhangsan1\",\"Lisi1\")\nhello2(\"Zhangsan2\",\"Lisi2\")\n```\n\n\n\n## 匿名函数\n\n```scala\nval fun1=(x:Int,y:Int)=>x+y\nfun1(5,4)\n```\n\n## map\n\n对每一个元素作用一个函数\n\n```scala\nval list=List(1,2,3,4,5)\n\n// _代表迭代的每一个元素\nval list1=list.map(_*2)\nval list2=list.map(_+2)\n\nlist.mkString(\",\") //1,2,3,4,5\nlist1.mkString(\",\") //2,4,6,8,10\nlist2.mkString(\",\") //3,4,5,6,7\n```\n\n## foreach\n\n遍历每一个元素\n\n```scala\nval arr= 1 to 10\narr.foreach(println)\n```\n\n## filter\n\n过滤符合要求的元素\n\n```scala\nval arr=1 to 10\narr.filter(_%2==0).mkString(\",\") //2,4,6,8,10\n```\n\n## reduce\n\n合并结果\n\n```scala\nval arr=1 to 10\narr.reduce(_+_) //55\n\narr.reduce((a,b)=>{\n    println(a,b)\n    a-b\n}) // -53\n/*\n(1,2)\n(-1,3)\n(-4,4)\n(-8,5)\n(-13,6)\n(-19,7)\n(-26,8)\n(-34,9)\n(-43,10)\n*/\n\narr.reduceLeft((a,b)=>{\n    println(a,b)\n    a-b\n}) // -53\n/*\n(1,2)\n(-1,3)\n(-4,4)\n(-8,5)\n(-13,6)\n(-19,7)\n(-26,8)\n(-34,9)\n(-43,10)\n*/\narr.reduceRight((a,b)=>{\n    println(a,b)\n    a-b\n}) // -5\n/*\n(9,10)\n(8,-1)\n(7,9)\n(6,-2)\n(5,8)\n(4,-3)\n(3,7)\n(2,-4)\n(1,6)\n*/\n```\n\n## fold\n\n柯里化的sum\n\n```scala\nval arr=1 to 10\narr.fold(0)(_+_) //55\narr.fold(0)((a,b)=>{\n    println(a,b)\n    a-b\n}) //-55\n/*\n(0,1)\n(-1,2)\n(-3,3)\n(-6,4)\n(-10,5)\n(-15,6)\n(-21,7)\n(-28,8)\n(-36,9)\n(-45,10)\n*/\n```\n\n## zip\n\n拉链,只zip能对应的,缺的直接丢弃\n\n```scala\nval a=List(\"A\",\"B\",\"C\",\"D\")\nval b=List(\"a\",\"b\",\"c\",\"d\",\"e\",\"f\")\na.zip(b) //List((A,a), (B,b), (C,c), (D,d))\n```\n\n>注意,这个和Spark中不一样\n>\n>Spark要求两边必须保持相同的个数\n\n## flattern\n\n压平\n\n```scala\nval list=List(List(1,2,3),List(4,5,6),List(7,8,9))\nlist.flatten //List(1, 2, 3, 4, 5, 6, 7, 8, 9)\n\nval list=List(Array(1,2,3),Array(4,5,6),Array(7,8,9))\nlist.flatten //List(1, 2, 3, 4, 5, 6, 7, 8, 9)\n```\n\n## flatMap\n\n等于map+flatten\n\n```scala\nval list=List(List(1,2,3),List(4,5,6),List(7,8,9))\nlist.flatMap(_.map(_*2)) //List(2, 4, 6, 8, 10, 12, 14, 16, 18)\n```\n\n\n\n## groupBy\n\n````scala\nval list=List((\"math\",65),(\"english\",75),(\"math\",84),(\"physic\",75))\nval group=list.groupBy(_._1)\n````\n\n\n\n## mapValues\n\n```scala\nval list=List((\"math\",65),(\"english\",75),(\"math\",84),(\"physic\",75))\nval group=list.groupBy(_._1).mapValues(x=>x.map(_._2).sum)\n```\n\n\n\n## sortBy\n\n- 数值按大小\n- 字符串按字典顺序\n\n```scala\nval list=List(45,15,32,78,14)\nlist.sortBy(-_) //78, 45, 32, 15, 14\n\nval list=List((\"math\",65),(\"english\",75),(\"math\",84),(\"physic\",75))\nval group=list.groupBy(_._1).mapValues(x=>x.map(_._2).sum)\ngroup.toList.sortBy(-_._2)\n```\n\n\n\n## diff\n\n```scala\nval a= 5 to 10\nval b=7 to 12\n\na.diff(b) //Vector(5, 6)\nb.diff(a) //Vector(11, 12)\na.union(b) //Vector(5, 6, 7, 8, 9, 10, 7, 8, 9, 10, 11, 12)\na.union(b).distinct //Vector(5, 6, 7, 8, 9, 10, 11, 12)\n```\n\n\n\n## 柯里化-currying\n\n将接收多个参数的函数变成接受单个参数的函数\n\n```scala\ndef next(num:Int)(step:Int=1)={\n    step+num\n}\nnext(15)(5)\nnext(15)()\n```\n\n\n\n## 偏函数-PartialFunction\n\n和模式匹配类似,只是没有了match部分,只包含了大括号和里面的case代码片段\n\n偏函数就是对部分数据进行处理,因为只有符合case的才会被相应的处理函数处理\n\n集合的collect方法可以接收一个偏函数\n\n```scala\nval arr=List(1,2,3,4.5,\"AA\",'A')\n//筛选数值类型的数据进行相应的处理\nval result = arr collect {\n    case x:Int=>x+1\n    case x:Double=>x*x\n}\n//List[Double] = List(2.0, 3.0, 4.0, 20.25)\n\n\nval arr=List(1,2,3,4.5,6,7)\n//星期转换\nval result = arr collect {\n    case 1 => \"Monday\"\n    case 2=>\"Tuesday\"\n    case 3 => \"Wednesday\"\n    case 4=>\"Thursday\"\n    case 5 => \"Friday\"\n    case 6=>\"Saturday\"\n    case _ => \"Sunday\"\n}\n\n```\n\n","tags":["Scala"],"categories":["Scala"]},{"title":"Scala基础-模式匹配","url":"/2019/02/04/scala基础-模式匹配/","content":"# scala基础-模式匹配\n\n## match case\n\n标准调用格式\n\n```scala\n变量 match{\n    case 条件1=>逻辑1\n    case 条件2=>逻辑2\n    ...\n    _ =>逻辑xxx\n}\n```\n\n- 常量枚举\n\n  ```scala\n  val array=Array(1,5,6,8,11)\n  val index=scala.util.Random.nextInt(5)\n  index match {\n      case 0 => println(\"less than 4\")\n      case 1 => println(\"the num is\"+array(index))\n      case _ => println(\"default condition\")\n  }\n  ```\n\n- 类型匹配\n\n  ```scala\n  val tuple=(1,\"AB\",Array(\"Zhangsan\"),Array(\"Zhangsan\",\"male\"))\n  \n  def hello(obj:Any)={\n      obj match {\n          //==Array(\"Zhangsan\")\n          case Array(\"Zhangsan\")=>println(\"I am Zhangsan\")\n          //Zhangsan 开头的Array\n          case Array(\"Zhangsan\",_*)=>println(\"Hello,Zhangsan\")\n          case _:Int=> println(\"This is Int number:\"+obj)\n          case _=> println(\"Something else\")\n      }\n  }\n  hello(tuple.productElement(scala.util.Random.nextInt(4)))\n  ```\n\n- 异常捕获\n\n  ```scala\n  try{\n      val value=1/0\n  }catch{\n      case e:ArithmeticException=>println(\"divided num cannot be 0\")\n      case _:Exception=> println(\"Other Exception\")\n  }\n  ```\n\n  \n\n","tags":["Scala"],"categories":["Scala"]},{"title":"scala基础-数组和集合","url":"/2019/01/29/scala基础-集合/","content":"# scala基础-数组和集合\n\n## 定长数组-Array\n\n- new Array()需要声明类型和长度,否则泛型是Nothing,无法进行后续赋值\n\n  ```scala\n  val array=new Array[Int](5)\n  for (x<-0 until 5){\n      array(x)=x\n  }\n  array.foreach(println)\n  //输出:0,1,2,3,4\n  ```\n\n  \n\n- Array有apply方法,可以不需要new\n\n  ```scala\n  val array=Array(\"a\",\"b\",\"c\")\n  array.foreach(println)\n  ```\n\n- 取数据,直接下标取,注意下标是从0开始的\n\n  ```scala\n  val array=Array(\"a\",\"b\",\"c\")\n  println(array(0),array(2))\n  //输出(a,c),\n  //array(3)会报错java.lang.ArrayIndexOutOfBoundsException\n  ```\n\n- 遍历\n\n  ```scala\n  val array=Array(\"a\",\"b\",\"c\")\n  for(x<-array){\n      print(x)\n      print(\",\")\n  }\n  array.foreach(println)\n  ```\n\n- 反转 reverse\n\n  ```scala\n  val array=Array(\"a\",\"b\",\"c\")\n  array.foreach(println)\n  ```\n\n- 合并 ++\n\n  ```scala\n  val array1=Array(\"a\",\"b\")\n  val array2=Array(1,2,3)\n  val arr=array1 ++ array2\n  arr.foreach(println)\n  ```\n\n- 针对数值类型\n\n  ```scala\n  val array=Array(1,2,3,4,5)\n  println(array.min,array.max,array.sum)\n  ```\n\n- mkString\n\n  ```scala\n  val array=Array(1,2,3,4,5)\n  println(array.mkString(\",\"))\n  //带首尾\n  println(array.mkString(\"[\",\",\",\"]\"))\n  ```\n\n## 变长数组-ArrayBuffer\n\n- 初始化\n\n  ```scala\n  import scala.collection.mutable.ArrayBuffer\n  \n  //可以传入初始容量ArrayBuffer[String](2),默认是16\n  val array=ArrayBuffer[String]()\n  ```\n\n  \n\n- 添加单个元素 += 元素\n\n  ```scala\n  array+=\"a\"\n  array+=\"b\"\n  ```\n\n  \n\n- 加数组 ++ 数组\n\n  ```scala\n  //但是这个返回的是新的数组,并不是再array的基础上加的\n  val result=array ++ Array(\"1\",\"2\")\n  println(array.mkString(\",\"))\n  println(result.mkString(\",\"))\n  ```\n\n- 指定位置插入 insert\n\n  ```scala\n  //第一个是要插入的下标,后面为可变参数\n  array.insert(0,\"a\",\"b\",\"c\")\n  println(array.mkString(\",\"))\n  ```\n\n- 删除指定下标的元素,remove(index)\n\n  ```scala\n  array.remove(0)\n  println(array.mkString(\",\"))\n  ```\n\n- 从某个位置开始,删除N个,remove(index,count)\n\n  ```scala\n  val array=ArrayBuffer(\"a\",\"b\",\"c\",\"d\")\n  array.remove(0,3)\n  println(array.mkString(\",\"))\n  //输出 d\n  ```\n\n- 从尾部开始删除 trimEnd(count)\n\n  ```scala\n  val array=ArrayBuffer(\"a\",\"b\",\"c\",\"d\")\n  array.trimEnd(2)\n  println(array.mkString(\",\"))\n  //输出 a,b\n  ```\n\n- 变长到定长, toArray\n\n  ```scala\n  val array=ArrayBuffer(\"a\",\"b\",\"c\",\"d\")\n  array.toArray.mkString(\",\")\n  ```\n\n- 定长到变长\n\n  ```scala\n  val array=Array(\"a\",\"b\",\"c\",\"d\")\n  array.toBuffer.mkString(\",\")\n  ```\n\n- 遍历\n\n  ```scala\n  val array=ArrayBuffer(\"a\",\"b\",\"c\",\"d\")\n  for(x<-array){\n      println(x)\n  }\n  ```\n\n## 定长集合-List\n\n> List有两个子样例类,Nil,scala.::\n> Nil 是List为空的情形,\n>\n> scala.collection.immutable.Nil = List\\[Nothing]()\n\n- 初始化\n\n  ```scala\n  val list=List(1,2,3,4)\n  ```\n\n- 第一个元素 head\n\n  ```scala\n  list.head\n  //输出 1\n  ```\n\n  \n\n- tail\n\n  ```scala\n  list.tail\n  //List(2,3,4)\n  list.tail.tail\n  //List(3,4)\n  ```\n\n- 最后加一个元素 :+\n\n  ```scala\n  val list1=list :+ 4\n  list.mkString(\",\")\n  list1.mkString(\",\")\n  \n  //可以感受到 :+ 返回的是一个新的List,也就是list和list1不一样\n  ```\n\n- 再List前面加单个元素 +:\n\n  ```scala\n  val list1=list :+ 4\n  val list2= 4 +: list\n  \n  list.mkString(\",\") // 1,2,3,4\n  list1.mkString(\",\") // 1,2,3,4,4\n  list2.mkString(\",\") // 4,1,2,3,4\n  \n  //这个也是返回新的List,原来的没法变\n  ```\n\n- ::Nil添加\n\n  ```scala\n  1::2::3::Nil //List(1,2,3)\n  \n  val list1 = 1::list\n  val list2 = 1::2::list\n  list.mkString(\",\") // 1,2,3,4\n  list1.mkString(\",\") // 1,1,2,3,4\n  list2.mkString(\",\") // 1,2,1,2,3,4\n  \n  //这个也是返回新的List,原来的不变,总之定长List永远是不变的\n  ```\n\n## 变长集合-ListBuffer\n\n- 初始化\n\n  ```scala\n  import scala.collection.mutable.ListBuffer\n  \n  val list=ListBuffer[Int]()\n  ```\n\n  \n\n- 加单个元素 +=\n\n  ```scala\n  list+=2\n  list+=3\n  list.mkString(\",\") //2,3\n  ```\n\n  \n\n- 加定长集合 ++\n\n  ```scala\n  val list1=list ++ List(1,2,3,4)\n  list.mkString(\",\") //2,3\n  list1.mkString(\",\") //2,3,1,2,3,4\n  \n  //这个返回的也是新的 ListBuffer,原来的不变\n  ```\n\n- 删除元素 -= ,注意**这里并不是下标,而是元素**\n\n  ```scala\n  list1 -= 2 //3,1,2,3,4\n  list1 -= 2 //3,1,3,4\n  ```\n\n- 删除多个元素\n\n  ```scala\n  val list1= ListBuffer(1,6,2,7,1,2,1,3,4,3) \n  list1 --= List(1,2,3) //6, 7, 1, 2, 1, 4, 3\n  ```\n\n- head\n\n  ```scala\n  val list=ListBuffer(5)\n  list.head //5\n  ```\n\n  \n\n- tail\n\n  ```scala\n  val list=ListBuffer(5)\n  list.tail //ListBuffer()\n  //注意这里并不是Nil,Nil是List的子样例类\n  ```\n\n  \n\n- toList\n\n  ```scala\n  val list=ListBuffer(5,7,8)\n  list.toList //List(5,7,8)\n  ```\n\n  \n\n- isEmpty\n\n  ```scala\n  val list=ListBuffer(5,7,8)\n  list.isEmpty\n  ```\n\n  \n\n  \n\n  ","tags":["Scala"],"categories":["Scala"]},{"title":"scala基础-Queue,Set,Tuple,Map","url":"/2019/01/26/scala基础-队列/","content":"# scala基础-Queue,Set,Tuple,Map\n\n## Queue\n\n- 初始化\n\n  ```scala\n  import scala.collection.mutable.Queue\n  val queue=new Queue[Int]()\n  ```\n\n  \n\n- 添加单个元素 +=\n\n  ```scala\n  queue +=10\n  ```\n\n  \n\n- 添加多个元素 ++\n\n  ```scala\n  val q=new Queue[Int]()\n  q += 1\n  val q1=q ++ List(1,2,3)\n  val q2=q ++ Array(1,2,3,4)\n  \n  q.mkString(\",\") //1\n  q1.mkString(\",\") // 1,1,2,3\n  q2.mkString(\",\") // 1,1,2,3,4\n  \n  //很明显,++依旧是返回新的Queue\n  ```\n\n- 入队 enqueue\n\n  ```scala\n  val q=new Queue[Int]()\n  q.enqueue(2,3,4) //Queue(2, 3, 4)\n  ```\n\n  \n\n- 出队 dequeue\n\n  ```scala\n  val q=new Queue[Int]()\n  q.enqueue(2,3,4) //Queue(2, 3, 4)\n  //先进先出\n  q.dequeue // 2\n  q.dequeue // 3\n  ```\n\n## Set\n\nset也是有两种,一种是mutable,一种是immutable\n\n这里介绍mutable类型的\n\n- 初始化\n\n  ```scala\n  import scala.collection.mutable.Set\n  val set=Set(1,2,3)\n  ```\n\n  \n\n- 添加单个元素\n\n  ```scala\n  set += 1 //不变,Set是去重的\n  set += 5 //加 (1,2,3,5)\n  ```\n\n  \n\n- 添加多个元素\n\n  ```scala\n  val set=Set(1,2,3)\n  set += (6,7,8) //(1, 2, 6, 3, 7, 8)\n  \n  val s1=set ++ Array(4,5,6)\n  set.mkString(\",\") //6,3,7,8\n  s1.mkString(\",\") //5,6,3,7,4,8\n  \n  //很明显,++还是返回新的Set,原来的set并不会发生变化\n  ```\n\n- 移除元素\n\n  ```scala\n  val set=Set(1,2,3)\n  set += (6,7,8)\n  set.remove(1) //true,set=(2, 6, 3, 7, 8)\n  set -=2 //(6, 3, 7, 8)\n  ```\n\n## Tuple\n\n元组,并不关系数据类型,随便放什么\n\n注意,**Tuple最多只能放22个**\n\n```scala\n// 实例化Tuple的方法1\nval tuple=(1,'A',\"a\")\n\n//Tuple的取数是从1开始数的\ntuple._1 // 1\ntuple._2 // A\n\n// 实例化Tuple的方法2\nval tuple1=new Tuple3(1,2,3)\ntuple1._1\n\n// 实例化Tuple的方法3\nval tuple2 = 'a'-> 'A' //(a,A)\nval tuple3 = 'a'-> 'A'-> 97 -> 65 // (((a,A),97),65)\n```\n\n- 遍历\n\n  ```scala\n  val tuple=(1,2,'A',\"B\",5)\n  for(i<- 0 until tuple.productArity){\n      println(tuple.productElement(i))\n  }\n  ```\n\n- swap-针对Tuple2\n\n  ```scala\n  val tuple=(\"zhangsan\",18)\n  val t=tuple.swap\n  println(t) //(18,zhangsan)\n  println(tuple) //(zhangsan,18)\n  \n  //很显然swap返回的是一个新的Tuple2\n  ```\n\n## Map\n\nMap分mutable和immutable,默认是immutable类型\n\n- immutable\n\n  ```scala\n  val map=Map(\"name\"->\"zhangsan\",\"age\"->8,\"sex\"->\"male\")\n  \n  //immutable的Map是没法修改的\n  //如果 map(\"name\")=\"lisi\",会报错\n  ```\n\n- mutable\n\n  ```scala\n  val map=scala.collection.mutable.Map(\"name\"->\"zhangsan\",\"age\"->8,\"sex\"->\"male\")\n  map.mkString(\",\") // age -> 8,name -> zhangsan,sex -> male\n  \n  //修改\n  map(\"name\")=\"lisi\"\n  map.mkString(\",\") //age -> 8,name -> lisi,sex -> male\n  \n  //添加新的key,\n  map(\"key\")=1\n  map.mkString(\",\") //key -> 1,age -> 8,name -> lisi,sex -> male\n  //添加\n  map += (\"k1\"->\"v1\") //key -> 1, k1 -> v1, age -> 8, name -> lisi, sex -> male\n  \n  //删除\n  map -= \"k1\"\n  map --= Array(\"key\",\"sex\") //(age -> 8, name -> lisi)\n  \n  \n  //遍历\n  for ((k,v)<-map){\n      println(k+\":\"+v)\n  }\n  \n  for (k<-map.keySet){\n      println(k+\":\"+map.getOrElse(k,\"\"))\n  }\n  \n  for(v <-map.values){\n      println(v)\n  }\n  \n  ```\n\n  > 注意\n  >\n  > Map的get方法返回的是一个Option,所以要获得值,还需要再get一次,但这个get当不存在的时候会报错java.util.NoSuchElementException: None.get\n  >\n  > 为了避免这种情况,一般推荐使用Option.getOrElse来获取最终的结果\n\n","tags":["Scala"],"categories":["Scala"]},{"title":"Scala基础-构造器","url":"/2019/01/24/scala基础-构造器/","content":"# Scala基础-构造器\n\n## 构造器\n\nScala的构造器分为两种:\n\n- 主构造器\n\n- 附属构造器\n\n- 一个类没有显示定义主构造器的类,会有一个默认的无参构造器作为主构造器\n\n  ```java\n  class ConstructNoTest {\n    /**\n     * 两个附属构造器\n     * @param name\n     */\n    def this(name: String, age: Int) {\n      //def this定义的附属构造器第一行必须调用主构造器\n      this\n    }\n    /**\n     * 两个附属构造器\n     * @param name\n     */\n    def this(name:String){\n      this\n    }\n  }\n  ```\n\n  \n\n- 主构造器定义在类的后面\n\n  **附属构造器第一句必须先调用主构造器**\n\n  ```java\n/**\n   * 主构造器定义在类名的后面\n *\n   * @param name\n   * @param age\n   */\n  class ConstructTest(name: String, age: Int) {\n      \n    // _占位的必须指定数据类型\n    var sex: String = _\n  \n    /**\n     * def this 附属构造器\n     * 第一句必须先调用主构造器\n     * @param name\n     * @param age\n     * @param sex\n     */\n    def this(name: String, age: Int, sex: String) {\n      this(name, age)\n      this.sex = sex\n    }\n  }\n  ```\n  \n  \n  \n- 主构造器中使用val声明可以将参数设置为类的属性\n\n```java\n/**\n * 主构造器上声明val,可以将参数自动设置为类的属性\n *\n * @param name\n */\nclass ConstructPro(val name: String) {\n  var age: Int = _\n\n  def this(name: String, age: Int) {\n    //显式创建主构造器之后,无参构造器没了,不能this()了\n    this(name)\n    this.age = age\n  }\n}\n```\n\n```java\nobject ConstructNoTest {\n  def main(args: Array[String]): Unit = {\n    val zhangsan = new ConstructNoTest(\"zhangsan\")\n    println(zhangsan)\n    val lisi = new ConstructTest(\"lisi\", 18)\n    println(lisi.sex)\n    val wangwu = new ConstructPro(\"wangwu\")\n    //可以直接使用name属性\n    println(wangwu.name + \":\" + wangwu.age)\n  }\n}\n```\n\n\n\n### 构造器的执行顺序\n\n先执行父类构造器,再执行子类构造器java\n\n```java\nclass OrderClass(val name: String, age: Int) {\n  println(\"parent class start,age\" + age)\n  def this(name: String) {\n    this(name, 10)\n    println(\"parent this(name:String,age:Int)\")\n  }\n  println(\"parent class end\")\n}\nclass OrderClassChild(name: String, val sex: String) extends OrderClass(name) {\n  println(\"child class start\")\n  def this(name: String) {\n    this(name, \"male\")\n    println(\"child this(name:String,age:Int)\")\n  }\n  println(\"child class end\")\n}\nobject OrderClassTest {\n  def main(args: Array[String]): Unit = {\n    val zhangsan = new OrderClassChild(\"zhangsan\")\n    println(zhangsan.name + \":\" + zhangsan.sex)\n  }\n}\n```\n\n输出\n\n```\nparent class start,age10\nparent class end\nparent this(name:String,age:Int)\nchild class start\nchild class end\nchild this(name:String,age:Int)\nzhangsan:male\n```\n\n可以看出,先执行父类中的一些方法和属性,然后执行父类的构造器,退回来执行子类的属性和方法,最后是子类的构造方法\n\n\n\n## 继承\n\n父类的属性可以被继承,可修改的属性和方法,子类也可以覆盖和重写,必须使用`override`关键字\n\n```java\nclass OverrideTest(var name: String) {\n  val age = 45\n  def play(): Unit = {\n    println(\"parent is playing\")\n  }\n}\n\nclass OverrideChild(parentName: String) extends OverrideTest(parentName) {\n  override name = \"zhangsan\"\n  override def play(): Unit = {\n    println(\"child is playing\")\n  }\n}\n```","tags":["Scala"],"categories":["Scala"]},{"title":"Scala基础-类","url":"/2019/01/19/scala基础-类/","content":"# Scala基础-类\n\n## 抽象类 abstract\n\n- 抽象类可包含未实现的方法和未初始化的属性\n\n- 不可直接new,否则报错\n\n```scala\nabstract class People {\n  def eat()\n  def play()\n  //抽象类可以具有普通方法\n  def work(): Unit = {\n    println(name + \"is working.....\")\n  }\n\n  //不初始化,必须指定类型,否则报错\n  val name: String\n}\n```\n\n具体实现交给子类实现\n\n```scala\n\nclass Worker extends People{\n  override def eat(): Unit = {\n    println(name + \"is eating.....\")\n  }\n\n  override def play(): Unit = {\n    println(name + \"is playing.....\")\n  }\n\n  override val name: String = \"Zhangsan\"\n}\n```\n\n使用时直接new具体的实现类即可\n\n```scala\nobject PeopleTest {\n  def main(args: Array[String]): Unit = {\n    val worker = new Worker()\n    worker.play()\n    worker.work()\n  }\n}\n```\n\n## 伴生类和伴生对象-Companion\n\n其实在Scala中是不存在静态的属性和方法的概念\n\n但是scala中,object却是和java中类似的用法,不需要new对象,可以直接使用\n\n```scala\nobject ObjectTest {\n  val name = \"zhangsan\"\n\n  def play(name: String=\"War3\"): Unit = {\n    println(\"play game :\" + name)\n  }\n\n  def main(args: Array[String]): Unit = {\n      //不需要new直接用\n    ObjectTest.play()\n    ObjectTest.play(\"FIFA 2019\")\n  }\n}\n```\n\n\n\n- 名称相同的Class和Object互为伴生\n\n- Class称为Object的伴生类\n\n- Object称为Class的伴生对象\n\n\n\n但是有一个很特殊的方法`apply()`\n\n```scala\n\nclass CompanionTest{\n  println(\"class start----\")\n  def apply(): Unit = {\n    println(\"apply in class...\")\n  }\n  println(\"class end----\")\n}\n\nobject CompanionTest {\n  println(\"object start----\")\n  def apply(): Unit = {\n    println(\"apply in object...\")\n    new CompanionTest()\n  }\n  println(\"object end----\")\n}\n\nobject Companion {\n\n  def main(args: Array[String]): Unit = {\n    val test = new CompanionTest()\n      //对象()调用的是class中的apply方法\n    test()\n      //不需要new的是调用object中的apply方法\n    CompanionTest()\n  }\n}\n\n```\n\n因为object就是类的伴生对象,所以直接类()也相当于对一个具体的对象进行调用apply()\n\napply()方法很神奇,不同的类都提供了不同的实现,只需要看懂这种用法就行\n\n## 枚举类-Enumeration\n\n枚举类对于有限个数的类型十分有用\n\n需要通过继承Enumeration\n\n```scala\nobject WeekDay extends Enumeration {\n  val Mon = Value(1, \"星期一\")\n  val Tue = Value(2, \"星期二\")\n  val Wed = Value(3, \"星期三\")\n  val Thu = Value(4, \"星期四\")\n  val Fri = Value(5, \"星期五\")\n  val San = Value(6, \"星期六\")\n  val Sun = Value(7, \"星期日\")\n\n  def main(args: Array[String]): Unit = {\n    println(WeekDay.Mon)\n    println(WeekDay(2))\n    println(WeekDay.withName(\"星期三\"))\n    for (elem <- WeekDay.values) {\n      println(elem)\n    }\n  }\n}\n```\n\n但是枚举类使用不存在的值时会报错\n\n## 样例类-case class\n\nscala中case class称为样例类\n\n使用时不需要new,当然,想加也是ok的\n\n在sparkSQL中大量使用了case class\n\n**case class已经实现了序列化**,不需要实现序列化了\n\n```scala\ncase class CaseClass(name: String, age: Int)\nobject CaseClassTest {\n  def main(args: Array[String]): Unit = {\n    println(CaseClass(\"Jack\", 22))\n    println(new CaseClass(\"拉布拉多\", 15))\n  }\n}\n```\n\n当然了有case class也有case object\n\n需要注意的是\n\n**case object不能加参数**,否则报错\n\n```scala\ncase class CaseClass(name: String, age: Int)\ncase object CaseClass{\n  def apply(): Unit ={\n    println(\"case object\")\n  }\n}\nobject CaseClassTest {\n  def main(args: Array[String]): Unit = {\n    println(CaseClass(\"Jack\", 22))\n    println(new CaseClass(\"拉布拉多\", 15))\n    CaseClass()\n  }\n}\n```\n\n## 接口-trait\n\nScala中和Java接口概念对应的是*trait*\n\n- trait的用法和抽象类的用法类似\n\n- 实现也用extends关键字\n- 多实现,第二个trait开始,使用with进行连接\n\n```scala\n\ntrait TraitTest {\ndef play()\n}\ntrait TraitTest1 {\n  def play1()\n}\nclass TraitTestImpl extends TraitTest with TraitTest1{\n  override def play(): Unit = {\n    println(\"Hello,trait play.....\")\n  }\n\n  override def play1(): Unit = {\n    println(\"Hello,trait1 play.....\")\n  }\n}\nobject TraitTestA{\n  def main(args: Array[String]): Unit = {\n    val impl = new TraitTestImpl()\n    impl.play()\n  }\n}\n```\n\n注意,实现的两个trait不能有相同的方法签名,否则,编译时报错,运行时过不去\n\n例如,下面这个就运行不过去\n\n```scala\n//错误的案例\ntrait TraitTest {\ndef play()\n}\ntrait TraitTest1 {\n  def play()\n}\nclass TraitTestImpl extends TraitTest with TraitTest1{\n  override def play(): Unit = {\n    println(\"Hello,trait play.....\")\n  }\n}\nobject TraitTestA{\n  def main(args: Array[String]): Unit = {\n    val impl = new TraitTestImpl()\n    impl.play()\n  }\n}\n```\n\n","tags":["Scala"],"categories":["Scala"]},{"title":"Scala基础-循环","url":"/2019/01/18/scala基础-循环/","content":"\n# Scala基础-循环\n\n## to\n\n- ### 1 to 5\n\n  ```\n  1,2,3,4,5\n  ```\n\n  \n\n- ### 1.to(5)\n\n  ```\n  1,2,3,4,5\n  ```\n\n- ### 1 to 5 by 2\n\n  带步长\n\n  ```\n  1,3,5\n  ```\n\n- ### 1.to(5).by(2)\n\n  ```\n  1,3,5\n  ```\n\n  \n\n## Range\n\n- ### Range(1,5)\n\n  ```\n  1,2,3,4\n  ```\n\n- ### Range(1,5,2) \n\n  带步长的\n\n  ```\n  1,3\n  ```\n\n- ### Range(10,4,-2)\n\n  倒着来\n\n  ```\n  10,8,6\n  ```\n\n\n\n## util\n\n- ### 1.until(5)\n\n  util=Range\n\n  ```\t\n  1,2,3,4\n  ```\n\n- ### 1.until(5).by(2)\n\n  ```\n  1,3\n  ```\n\n  \n\n- ### 1 util 5 by 2\n\n  ```\n  1,3\n  ```\n\n  \n\n----\n\n> 区别,Range是左闭右开的范围,[)\n>\n> to是全闭合的范围,[]\n\n----\n\n\n\n## 数组遍历\n\n- ### 增强for\n\n```scala\nval array=Array(1,2,3,4)\n//不需要声明x为val还是var,默认是val\nfor(x<-array){\n    println(x)\n}\n//输出1,2,3,4\n```\n\n- ### 下标遍历\n\n```scala\nval array=Array(1,2,3,4)\nfor(x<- 0 until array.length ){\n    println(array(x))\n}\n//输出1,2,3,4\n```\n\n- ### 带条件的for--守卫\n\n```scala\nval array=Array(1,2,3,4,5,6,7)\n//以x%2==0作为守卫条件\nfor(x<- 0 until array.length if x%2==0 ){\n    println(array(x))\n}\n//输出1,3,5,7\n```\n\n- ### yield推导\n\n```scala\nval array=Array(1,2,3,4,5,6,7)\nval result=for (x<- array if x%2==0) yield x*2\nresult.foreach(println)\n//输出( 4,  8, 14)\n```\n\n##  while\n\n```scala\n//100以内求和\nvar (num,sum)=(100,0)\nwhile(num>0){\n    sum = sum + num\n    num = num-1\n}\nprintln(num,sum)\n//(0,100)\n```\n\n\n\n## 方法的默认参数\n\n```scala\ndef hello(name:String=\"zhangsan\"){\n    println(\"Hello,\"+name)\n}\n\n//默认参数也是有参数的,不能省略括号,否则报错\nhello()\nhello(\"lisi\")\n\n//输出 Hello,zhangsan\n// Hello,lisi\n```\n\n方法的默认参数很有用,Spark大量使用了这种默认参数,当用户不传时,会执行默认逻辑,如分区数,存储级别\n\n\n\n## 方法的可变参数\n\n```scala\ndef add(arr:Int*)={\n    var sum=0\n    for(x<-arr){\n        sum=sum+x\n    }\n    sum\n}\n\nadd(1,2,3)\n//scala特殊语法, :_*\nadd(1 to 10 : _*)\nadd(1 to 100 : _*)\n\n// 输出  6,55,5050\n```\n\n## 命名参数\n\n```scala\ndef add(x:Int,y:Int)={\n    x+y\n}\nadd(10,y=20)\n//输出 30\n```\n\n","tags":["Scala"],"categories":["Scala"]},{"title":"Hive的一些坑","url":"/2018/10/13/Hive-一些坑/","content":"\n# Hive的一些坑\n\n1. specified datastore driver(\"com.mysql.jdbc.Driver\") was not found\n\n![image-20200126114217615](/images/image/image-20200126114217615.png)\n\n这个是因为驱动不对,下载了个新的就行了\n\n2. Unable to open a test connection to the given database. JDBC url = jdbc:mysql://hadoop001:3306/test?useSSL=true&serverTimezone=GMT%2B8, username = lrj. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception:\n\n![image-20200126114500594](/images/image/image-20200126114500594.png)\n\n这个需要把ssl禁用了,在jdbcUrl上指定useSSL=false\n\n3. MetaException(message:Version information not found in metastore. )\n\n![image-20200126114708126](/images/image/image-20200126114708126.png)\n\n这个需要将hive-site.xml中的hive.metastore.schema.verification设置为false\n\n4. Required table missing : \"`VERSION`\" in Catalog \"\" Schema \"\". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable \"datanucleus.autoCreateTables\"\n\n![image-20200126114930654](/images/image/image-20200126114930654.png)\n\n这个需要初始化一下schema,执行\n\nschematool -dbType mysql -initSchema\n\n----\n\n之后就可以启动metastore + hiveserver2服务\n\n```bash\nnohup hive --service  metastore > ~/metastore.log 2>&1 &\nnohup  hiveserver2  > ~/hiveserver2.log 2>&1 &\n```\n\n测试hiveserver2服务是否ok\n\n```bash\nbeeline\n```\n\n打印日志\n\n```\nwhich: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/mysql/bin:/home/lurongjiang/.local/bin:/home/lurongjiang/bin:/usr/software/hadoop-2.6.0-cdh5.16.2/bin:/usr/software/hadoop-2.6.0-cdh5.16.2/sbin:/usr/software/jdk1.8.0_231/bin:/usr/software/apache-maven-3.6.3/bin:/usr/software/scala-2.11.12/bin:/usr/software/hive-1.1.0-cdh5.16.2/bin)\nBeeline version 1.1.0-cdh5.16.2 by Apache Hive\n# 查看下数据库,此时发现没连接\nbeeline> show databases;\nNo current connection\n# 尝试连接数据库,只需要输入用户名就行,不需要密码\nbeeline> !connect jdbc:hive2://hadoop001:10000/default\nConnecting to jdbc:hive2://hadoop001:10000/default\nEnter username for jdbc:hive2://hadoop001:10000/default: lrj\nEnter password for jdbc:hive2://hadoop001:10000/default: \nError: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=lrj, access=EXECUTE, inode=\"/tmp\":lurongjiang:supergroup:drwx\n```\n\n5. java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=lrj, access=EXECUTE, inode=\"/tmp\":lurongjiang:supergroup:drwx------\n\n![image-20200126115801228](/images/image/image-20200126115801228.png)\n\n 这个是没权限\n\nhadoop fs -chmod -R 777  /tmp\n\n再次启动就ok了.\n\n","tags":["Hive"],"categories":["Hive"]},{"title":"Hive UDF","url":"/2018/10/10/Hive-UDF/","content":"\n# Hive UDF\n\nhive内置函数并不一定满足我们的业务要求,所以需要拓展,即用户自定义函数\n\n**UDF**\n\nUser Defined Function\n\n- UDF (one-to-one)\n- UDAF(many-to-one)\n- UDTF(one-to-many)\n\n## 创建UDF步骤\n\n- 添加依赖\n\n  ```xml\n      <dependency>\n        <groupId>org.apache.hive</groupId>\n        <artifactId>hive-exec</artifactId>\n        <version>${hive.cdh.version}</version>\n      </dependency>\n  ```\n\n  \n\n- 创建自定义类,继承UDF","tags":["hive","udf","user-defined-function"],"categories":["Hive"]},{"title":"HDFS块损坏修复","url":"/2018/09/14/Hadoop-HDFS块损坏修复/","content":"\n# HDFS块损坏修复\n\n## 准备\n\n创建一个文件夹\n\n```bash\nhdfs dfs -mkdir /blockrecover\n```\n\n准备一个文件\n\n```bash\necho \"hello world\" >> test.txt\n```\n\n上传文件到hdfs\n\n```bash\nhdfs dfs -put test.txt/blockrecover\n```\n\n![image-20200316133740804](/images/image/image-20200316133740804.png)\n\n## 检查健康状态\n\n```bash\nhdfs fsck / \n```\n\n![image-20200316133827968](/images/image/image-20200316133827968.png)\n\n## 查看一下block信息\n\n![image-20200316135332081](/images/image/image-20200316135332081.png)\n\n## 在data目录查找block信息\n\n```bash\nfind ./data -name \"blk_1073741826_1002*\"\n```\n\n![image-20200316140233083](/images/image/image-20200316140233083.png)\n\n## 删除block和meta信息\n\n找到之后cd进去吧meta和对应的block删除了,模拟块丢失\n\n![image-20200316140320006](/images/image/image-20200316140320006.png)\n\n重启hdfs(因为默认fsck间隔时间是6个小时,这里重启)\n\n## 再来检查\n\n```bash\nhdfs fsck /blockrecover\n```\n\n![image-20200316141718499](/images/image/image-20200316141718499.png)\n\n## 手动修复\n\nhdfs debug\n\n```\nUsage: hdfs debug <command> [arguments]\n\nThese commands are for advanced users only.\n\nIncorrect usages may result in data loss. Use at your own risk.\n\nverifyMeta -meta <metadata-file> [-block <block-file>]\ncomputeMeta -block <block-file> -out <output-metadata-file>\nrecoverLease -path <path> [-retries <num-retries>]\n```\n\n---\n\n这个命令是隐藏的,可能是为了防止滥用,只让专业人员知道\n\n修复命令\n\n```bash\nhdfs debug recoverLease -path /blockrecover/test.txt -retries 3\n```\n\n这个可能会成功,可能会失败,所以多试几次\n\n![image-20200316143310820](/images/image/image-20200316143310820.png)\n\n注意这个必须要指定到文件,目录是不行的\n\n这样再次检查,就发现是ok了\n\n![image-20200316143427089](/images/image/image-20200316143427089.png)\n\nblock也恢复了\n\n![image-20200316143447945](/images/image/image-20200316143447945.png)","tags":["Hadoop","BlockRecovery"],"categories":["Hadoop"]},{"title":"Hadoop HA部署完整文档","url":"/2018/09/10/Hadoop-HA部署文档/","content":"\n# Hadoop HA部署完整文档\n\n## 主机规划\n\n| 公网          | 内网           | hostname  | role                                       |\n| ------------- | -------------- | --------- | ------------------------------------------ |\n| 47.108.63.184 | 172.24.144.125 | hadoop001 | NN,ZKFC,JN,DN,JobHistory,NM,QuorumPeerMain |\n| 47.108.92.139 | 172.24.144.126 | hadoop002 | NN,ZKFC,JN,DN,JobHistory,NM,QuorumPeerMain |\n| 47.108.51.164 | 172.24.144.127 | hadoop003 | DN,JN,NM                                   |\n\n## 目录规划\n\n|                 |                            |                                 |\n| --------------- | -------------------------- | ------------------------------- |\n| $HAOOP_HOME     | /home/hadoop/app/hadoop    |                                 |\n| Data            | $HADOOP_HOME/data          |                                 |\n| Log             | $HADOOP_HOME/logs          |                                 |\n| hadoop.tmp.dir  | $HADOOP_HOME/tmp           | 需要手工创建,权限 777,root:root |\n| $ZOOKEEPER_HOME | /home/hadoop/app/zookeeper |                                 |\n\n\n三台同时操作\n```bash\nmkdir -p /usr/java\n```\n\n## 配置host\n\n172.24.144.125 hadoop001\n172.24.144.126 hadoop002\n172.24.144.127 hadoop003\n\n```bash\nehco \"172.24.144.125 hadoop001\" >> /etc/hosts\nehco \"172.24.144.126 hadoop002\" >> /etc/hosts\nehco \"172.24.144.127 hadoop003\" >> /etc/hosts\n```\n\n## 添加hadoop用户\n\nzk和hadoop只需要hadoop用户安装就可以了,先新建一个hadoop用户\n\n三台同时操作\n```bash\nuseradd hadoop\nsu - hadoop\nmkdir app\n```\n## hadoop用户免密配置\n\n生成密钥,一路enter,三台同时操作\n\n```bash\nsu - hadoop\nssh-keygen\n```\n\n因为hadoop用户没用设置登录密码,所以无法外部登录,只能把公钥拷贝出来\n\n```bash\ncat ~/.ssh/id_rsa.pub\n```\n\n```\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDTosbqrdcGhqcTc+VS4pU87bW0E0jAOxnnX0aqYVv0mWFl2qhm5EJwzserJ1ucpw91pBMt7iAYZ9Mi0SKVRdJ9rGKDtxKO2W5m06A1fY4/FK2iuuoCsRW79Dyl5Y26/+J6trxjWOI6uGL7PBLReoJQ6iO34R6lq+ejKRhoblUMtSwIVPr/kGzlYLRrc5SaBe4d4LuaHA+/jB5UDDarZmqTQambwevOPGl8IVfp0ute/NZySFBb5+1VjwB7L/GarNPY7eFEY5LhRgnlqd6f4chnzFMBQagyJTOOZYHTmkWYH+wjsTohwWgXBU0CemMNxY3Y0fG6/8qOvCxDntX+Ma7F hadoop@hadoop001\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDmDr8N0Pg8/X13NgEUOLOeiWy/PjAI59uaREnNjj+jagpi6ED+LOpflzZwypKGiu0EecX7MrC2ct6ElJeGzvI6ZigiE6brt2ufWjGGy2IITxgl6lvAxbC84WvLOkfXqyaydmv/omOrxtshyKbzjDgNyEqUb+3fGtYz59rW+MKIG9nekWvGBZcA4zo46g/kUmra/9/UUdzGAe5CF0tKheeB62uP7JBmv4dIhrR2R7B5BCgc7KAuZrNn/DAFLJOIb8P87MVABkLiFwFIHxJV41OKq4RMx7D64VAJdtemRKlTTiA9Zz1alTzcgipWadwRpjqJPpfzu+s2LFirmS/l1Btj hadoop@hadoop002\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCys26gHCMPDMdoltsk3Y7cgVSBV0DxM89EpzFLmBejJh11xCtyL20BbTZfRefMN9qd6lYlh6HFPLIpPgXpbtZXwnKUP5gM4eHCFFvzPC5AQDvvuXOmKmv9gkEjr30bCGx8uUuMPuF42YDmgyumz+x1O/OFck6PzQh5SorcxoXyB+u2XbXXqrTpvx0rTJPGxJv41t1BmcIqCK4xiYVwR8D+12QomX/jGn83Lh3Jurds6+ywbxCzmMxRew4NIDDjBJaJwPtikkfhd+RPMNKf+bCy40D3RhOFWsJ3hjPgbUSHH0jBjo6GRjg6wmVJxiBv1sXHss4N/vdwvaBh6F2rcVhT hadoop@hadoop003\n```\n\n```bash\ntouch authorized_keys\nvi authorized_keys\n#把三台的公钥复制进去,wq!\n# 修改权限\nchmod 0600 ~/.ssh/authorized_keys\n```\n\n接着ssh进行验证,第一次需要yes\n\n```bash\nssh hadoop001 date\nssh hadoop002 date\nssh hadoop003 date\n```\n\n## 上传安装包\n\njdk,zk,hadoop\n\n```bash\nsu root\nyum install -y lrzsz\nrz\n```\n\n## 解压\n\n```bash\nsu root\nchown -R root:root ./jdk-8u231-linux-x64.tar.gz\nchown -R hadoop:hadoop ./zookeeper-3.4.5-cdh5.16.2.tar.gz\nchown -R hadoop:hadoop ./hadoop-2.6.0-cdh5.16.2.tar.gz\n\ntar -zxvf ./jdk-8u231-linux-x64.tar.gz -C /usr/java/\nsu - hadoop\ntar -zxvf ./zookeeper-3.4.5-cdh5.16.2.tar.gz -C ~/app\ntar -zxvf ./hadoop-2.6.0-cdh5.16.2.tar.gz -C ~/app\n# 软连接\nsu hadoop\nln -s  $HOME/app/zookeeper-3.4.5-cdh5.16.2 $HOME/app/zookeeper\nln -s  $HOME/app/hadoop-2.6.0-cdh5.16.2 $HOME/app/hadoop\nsu root\nln -s /usr/java/jdk1.8.0_231 /usr/java/java8\n\n# 权限\nchmod -R 777 /opt/software/hadoop\nchmod -R 777 /opt/software/zookeeper\n```\n\n\n\n## 配置环境变量\n\n```bash\nsu root\nvi /etc/profile\n# 追加\nexport JAVA_HOME=/usr/java/java8\nsource /etc/profile\n\nsu hadoop\nvi ~/.bash_profile\n\nexport HADOOP_HOME=$HOME/app/hadoop\nexport ZOOKEEPER_HOME=$HOME/app/zookeeper\nexport PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${ZOOKEEPER_HOME}/bin\n\nsource ~/.bash_profile\n```\n\n## 安装Zookeeper\n\n```bash\ncd $HOME/app/zookeeper/conf\ncp zoo_sample.cfg zoo.cfg\nvi zoo.cfg\n```\n\n修改zk配置\n\n```properties\ndataDir=/home/hadoop/app/zookeeper/data\n#增加\nserver.1=hadoop001:2888:3888\nserver.2=hadoop002:2888:3888\nserver.3=hadoop003:2888:3888\n```\n\n增加myid,分别在三台机器执行对应命令\n\n```bash\ncd ~/app/zookeeper/data\n\n# 这里注意,1,2,3,>前后的空格\necho 1 > ./myid\necho 2 > ./myid\necho 3 > ./myid\n```\n\n启动zk,三台执行\n\n```bash\nzkServer.sh start\n# 查看状态\nzkServer.sh status\n```\n\n## 部署Hadoop\n\nhdfs-site.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n\n<!-- Put site-specific property overrides in this file. -->\n\n<configuration>\n\n\t<!--HDFS超级用户 -->\n\t<property>\n\t\t<name>dfs.permissions.superusergroup</name>\n\t\t<value>hadoop</value>\n\t</property>\n\n\t<!--开启web hdfs -->\n\t<property>\n\t\t<name>dfs.webhdfs.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.name.dir</name>\n\t\t<value>/home/hadoop/app/hadoop/data/name</value>\n\t\t<description> namenode 存放name table(fsimage)本地目录（需要修改）</description>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.edits.dir</name>\n\t\t<value>${dfs.namenode.name.dir}</value>\n\t\t<description>namenode粗放 transaction file(edits)本地目录（需要修改）</description>\n\t</property>\n\t<property>\n\t\t<name>dfs.datanode.data.dir</name>\n\t\t<value>/home/hadoop/app/hadoop/data/data</value>\n\t\t<description>datanode存放block本地目录（需要修改）</description>\n\t</property>\n\t<property>\n\t\t<name>dfs.replication</name>\n\t\t<value>3</value>\n\t</property>\n\t<!-- 块大小128M （默认128M） -->\n\t<property>\n\t\t<name>dfs.blocksize</name>\n\t\t<value>134217728</value>\n\t</property>\n\t<!--======================================================================= -->\n\t<!--HDFS高可用配置 -->\n\t<!--指定hdfs的nameservice为ruozeclusterg7,需要和core-site.xml中的保持一致 -->\n\t<property>\n\t\t<name>dfs.nameservices</name>\n\t\t<value>hadoopha</value>\n\t</property>\n\t<property>\n\t\t<!--设置NameNode IDs 此版本最大只支持两个NameNode -->\n\t\t<name>dfs.ha.namenodes.hadoopha</name>\n\t\t<value>nn1,nn2</value>\n\t</property>\n\n\t<!-- Hdfs HA: dfs.namenode.rpc-address.[nameservice ID] rpc 通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.hadoopha.nn1</name>\n\t\t<value>hadoop001:8020</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.hadoopha.nn2</name>\n\t\t<value>hadoop002:8020</value>\n\t</property>\n\n\t<!-- Hdfs HA: dfs.namenode.http-address.[nameservice ID] http 通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.hadoopha.nn1</name>\n\t\t<value>hadoop001:50070</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.http-address.hadoopha.nn2</name>\n\t\t<value>hadoop002:50070</value>\n\t</property>\n\n\t<!--==================Namenode editlog同步 ===================== -->\n\t<!--保证数据恢复 -->\n\t<property>\n\t\t<name>dfs.journalnode.http-address</name>\n\t\t<value>0.0.0.0:8480</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.journalnode.rpc-address</name>\n\t\t<value>0.0.0.0:8485</value>\n\t</property>\n\t<property>\n\t\t<!--设置JournalNode服务器地址，QuorumJournalManager 用于存储editlog -->\n\t\t<!--格式：qjournal://<host1:port1>;<host2:port2>;<host3:port3>/<journalId> 端口同journalnode.rpc-address -->\n\t\t<name>dfs.namenode.shared.edits.dir</name>\n\t\t<value>qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/hadoopha</value>\n\t</property>\n\n\t<property>\n\t\t<!--JournalNode存放数据地址 -->\n\t\t<name>dfs.journalnode.edits.dir</name>\n\t\t<value>/home/hadoop/app/hadoop/data/jn</value>\n\t</property>\n\t<!--==================DataNode editlog同步 =========================== -->\n\t<property>\n\t\t<!--DataNode,Client连接Namenode识别选择Active NameNode策略 -->\n\t                         <!-- 配置失败自动切换实现方式 -->\n\t\t<name>dfs.client.failover.proxy.provider.hadoopha</name>\n\t\t<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n\t</property>\n\t<!--==================Namenode fencing：====================== -->\n\t<!--Failover后防止停掉的Namenode启动，造成两个服务 -->\n\t<property>\n\t\t<name>dfs.ha.fencing.methods</name>\n\t\t<value>sshfence</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.ha.fencing.ssh.private-key-files</name>\n\t\t<value>/home/hadoop/.ssh/id_rsa</value>\n\t</property>\n\t<property>\n\t\t<!--多少milliseconds 认为fencing失败 -->\n\t\t<name>dfs.ha.fencing.ssh.connect-timeout</name>\n\t\t<value>30000</value>\n\t</property>\n\n\t<!--==========NameNode auto failover base ZKFC and Zookeeper============== -->\n\t<!--开启基于Zookeeper  -->\n\t<property>\n\t\t<name>dfs.ha.automatic-failover.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<!--动态许可datanode连接namenode列表 -->\n\t <property>\n\t   <name>dfs.hosts</name>\n\t   <value>/home/hadoop/app/hadoop/etc/hadoop/slaves</value>\n\t </property>\n\n\t<property>\n\t\t<name>dfs.client.use.datanode.hostname</name>\n\t\t<value>true</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.datanode.use.datanode.hostname</name>\n\t\t<value>true</value>\n\t</property>\n\n</configuration>\n\n```\n\ncore-site.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n\n<!-- Put site-specific property overrides in this file. -->\n\n<configuration>\n\n  <!--Yarn 需要使用 fs.defaultFS 指定NameNode URI -->\n  <property>\n          <name>fs.defaultFS</name>\n          <value>hdfs://hadoopha</value>\n  </property>\n  <!--==============================Trash机制======================================= -->\n  <property>\n          <!--多长时间创建CheckPoint NameNode截点上运行的CheckPointer 从Current文件夹创建CheckPoint;默认：0 由fs.trash.interval项指定 -->\n          <name>fs.trash.checkpoint.interval</name>\n          <value>0</value>\n  </property>\n  <property>\n          <!--多少分钟.Trash下的CheckPoint目录会被删除,该配置服务器设置优先级大于客户端，默认：0 不删除 -->\n          <name>fs.trash.interval</name>\n          <value>1440</value>\n  </property>\n\n   <!--指定hadoop临时目录, hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配 置namenode和datanode的存放位置，默认就放在这>个路径中 -->\n  <property>   \n          <name>hadoop.tmp.dir</name>\n          <value>/home/hadoop/app/hadoop/tmp</value>\n  </property>\n\n   <!-- 指定zookeeper地址 -->\n  <property>\n          <name>ha.zookeeper.quorum</name>\n          <value>hadoop001:2181,hadoop002:2181,hadoop003:2181</value>\n  </property>\n   <!--指定ZooKeeper超时间隔，单位毫秒 -->\n  <property>\n          <name>ha.zookeeper.session-timeout.ms</name>\n          <value>2000</value>\n  </property>\n\n  <property>\n     <name>hadoop.proxyuser.hadoop.hosts</name>\n     <value>*</value> \n  </property> \n  <property> \n      <name>hadoop.proxyuser.hadoop.groups</name> \n      <value>*</value> \n  </property> \n\n\n  <property>\n    <name>io.compression.codecs</name>\n    <value>org.apache.hadoop.io.compress.GzipCodec,\n    org.apache.hadoop.io.compress.DefaultCodec,\n    org.apache.hadoop.io.compress.BZip2Codec,\n    org.apache.hadoop.io.compress.SnappyCodec\n  </value>\n  </property>\n</configuration>\n\n```\n\nmapred-site.xml\n\n```xml\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n\n<!-- Put site-specific property overrides in this file. -->\n\n<configuration>\n\t<!-- 配置 MapReduce Applications -->\n\t<property>\n\t\t<name>mapreduce.framework.name</name>\n\t\t<value>yarn</value>\n\t</property>\n\t<!-- JobHistory Server ============================================================== -->\n\t<!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 -->\n\t<property>\n\t\t<name>mapreduce.jobhistory.address</name>\n\t\t<value>hadoop001:10020</value>\n\t</property>\n\t<!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 -->\n\t<property>\n\t\t<name>mapreduce.jobhistory.webapp.address</name>\n\t\t<value>hadoop001:19888</value>\n\t</property>\n\n<!-- 配置 Map段输出的压缩,snappy-->\n  <property>\n      <name>mapreduce.map.output.compress</name> \n      <value>true</value>\n  </property>\n              \n  <property>\n      <name>mapreduce.map.output.compress.codec</name> \n      <value>org.apache.hadoop.io.compress.SnappyCodec</value>\n   </property>\n\n</configuration>\n\n```\n\nyarn-site.xml\n\n```xml\n<?xml version=\"1.0\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n<configuration>\n\n\t<!-- nodemanager 配置 ================================================= -->\n\t<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n\t\t<value>org.apache.hadoop.mapred.ShuffleHandler</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.nodemanager.localizer.address</name>\n\t\t<value>0.0.0.0:23344</value>\n\t\t<description>Address where the localizer IPC is.</description>\n\t</property>\n\t<property>\n\t\t<name>yarn.nodemanager.webapp.address</name>\n\t\t<value>0.0.0.0:23999</value>\n\t\t<description>NM Webapp address.</description>\n\t</property>\n\n\t<!-- HA 配置 =============================================================== -->\n\t<!-- Resource Manager Configs -->\n\t<property>\n\t\t<name>yarn.resourcemanager.connect.retry-interval.ms</name>\n\t\t<value>2000</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.ha.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.ha.automatic-failover.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 使嵌入式自动故障转移。HA环境启动，与 ZKRMStateStore 配合 处理fencing -->\n\t<property>\n\t\t<name>yarn.resourcemanager.ha.automatic-failover.embedded</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 集群名称，确保HA选举时对应的集群 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.cluster-id</name>\n\t\t<value>yarn-cluster</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.ha.rm-ids</name>\n\t\t<value>rm1,rm2</value>\n\t</property>\n\n\n\t<!--这里RM主备结点需要单独指定,（可选）\n\t     \t<property>\n\t\t <name>yarn.resourcemanager.ha.id</name>\n\t\t <value>rm2</value>\n\t </property>\n\t -->\n\n\t<property>\n\t\t<name>yarn.resourcemanager.scheduler.class</name>\n\t\t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.recovery.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>\n\t\t<value>5000</value>\n\t</property>\n\t<!-- ZKRMStateStore 配置 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.store.class</name>\n\t\t<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.zk-address</name>\n\t\t<value>hadoop001:2181,hadoop002:2181,hadoop003:2181</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.zk.state-store.address</name>\n\t\t<value>hadoop001:2181,hadoop002:2181,hadoop003:2181</value>\n\t</property>\n\t<!-- Client访问RM的RPC地址 (applications manager interface) -->\n\t<property>\n\t\t<name>yarn.resourcemanager.address.rm1</name>\n\t\t<value>hadoop001:23140</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.address.rm2</name>\n\t\t<value>hadoop002:23140</value>\n\t</property>\n\t<!-- AM访问RM的RPC地址(scheduler interface) -->\n\t<property>\n\t\t<name>yarn.resourcemanager.scheduler.address.rm1</name>\n\t\t<value>hadoop001:23130</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.scheduler.address.rm2</name>\n\t\t<value>hadoop002:23130</value>\n\t</property>\n\t<!-- RM admin interface -->\n\t<property>\n\t\t<name>yarn.resourcemanager.admin.address.rm1</name>\n\t\t<value>hadoop001:23141</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.admin.address.rm2</name>\n\t\t<value>hadoop002:23141</value>\n\t</property>\n\t<!--NM访问RM的RPC端口 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.resource-tracker.address.rm1</name>\n\t\t<value>hadoop001:23125</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.resource-tracker.address.rm2</name>\n\t\t<value>hadoop002:23125</value>\n\t</property>\n\t<!-- RM web application 地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.webapp.address.rm1</name>\n\t\t<value>hadoop001:10086</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.webapp.address.rm2</name>\n\t\t<value>hadoop002:10086</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.webapp.https.address.rm1</name>\n\t\t<value>hadoop001:23189</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.webapp.https.address.rm2</name>\n\t\t<value>hadoop002:23189</value>\n\t</property>\n\n\t<property>\n\t   <name>yarn.log-aggregation-enable</name>\n\t   <value>true</value>\n\t</property>\n\t<property>\n\t\t <name>yarn.log.server.url</name>\n\t\t <value>http://hadoop001:19888/jobhistory/logs</value>\n\t</property>\n\n\n\t<property>\n\t\t<name>yarn.nodemanager.resource.memory-mb</name>\n\t\t<value>2048</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.scheduler.minimum-allocation-mb</name>\n\t\t<value>1024</value>\n\t\t<discription>单个任务可申请最少内存，默认1024MB</discription>\n\t </property>\n\n\n\t<property>\n\t<name>yarn.scheduler.maximum-allocation-mb</name>\n\t<value>2048</value>\n\t<discription>单个任务可申请最大内存，默认8192MB</discription>\n\t</property>\n\n\t<property>\n\t   <name>yarn.nodemanager.resource.cpu-vcores</name>\n\t   <value>2</value>\n\t</property>\n\n</configuration>\n\n```\n\n## 初始化\n\n**启动zk,三台启动**\n\n```bash\nzkServer.sh start\nzkServer.sh status\n```\n\n**启动jn,三台**\n\n```bash\nhadoop-daemon.sh start journalnode\n```\n\n**初始化zkfc,一台**\n\n```bash\nhdfs zkfc -formatZK\n```\n\n**格式化namenode,一台**\n\n```bash\nhdfs namenode -format\n# 复制namenode情况到另一个namenode\nscp -r ./data/name hadoop002:~/app/hadoop/data\n```\n\n**启动hdfs**\n\n```bash\nstart-dfs.sh\n```\n\n**启动yarn**\n\n```bash\nstart-yarn.sh\n```\n\n**jps检查进程情况**\n\nhadoop001:\n\n-----------\n\n13776 NameNode\n14179 ResourceManager\n13220 QuorumPeerMain\n14295 NodeManager\n14074 DFSZKFailoverController\n13852 DataNode\n13948 JournalNode\n14942 Jps\n\nhadoop002:\n\n-------\n\n14100 NodeManager\n12420 QuorumPeerMain\n13814 DFSZKFailoverController\n13992 ResourceManager\n15101 Jps\n13437 DataNode\n13629 JournalNode\n\nhadoop003:\n\n-----\n\n9155 JournalNode\n9059 DataNode\n9379 NodeManager\n9606 Jps\n8329 QuorumPeerMain\n\n------\n\n\n\n## hadoop HA遇到的问题\n\n![image-20200316105241456](/images/image/image-20200316105241456.png)\n\n这个好像是我提前创建data目录和tmp目录造成的,我重新删除hadoop的data,tmp目录,以及zk的hadoop-ha的znode,删除zk的data/version-2,重新启动zk,启动jn,formatZK,format namenode就好了\n\n## 测试\n\n- 测试standby是否可以创建目录\n\n  ![image-20200316123403542](/images/image/image-20200316123403542.png)\n\n  发现不可以创建目录\n\n- 测试standby是否可写\n\n  ![image-20200316123535913](/images/image/image-20200316123535913.png)\n\n  发现不可以写\n\n- 测试standby是否可读\n\n  ![image-20200316123634814](/images/image/image-20200316123634814.png)\n\n  发现不可以读\n\n- 测试故障是否能会转移\n\n  ![image-20200316123843722](/images/image/image-20200316123843722.png)\n\n  发现故障转移正常\n\n## 几个有用的命令\n\n- hdfs haadmin\n- hdfs getconf\n- hdfs dfsadmin","tags":["Hadoop","HA"],"categories":["Hadoop"]},{"title":"Hadoop MapReduce编程核心","url":"/2018/09/06/Hadoop-MapReduce编程核心/","content":"\n# Hadoop MapReduce编程核心\n\n## Partitioner 分区\n\n```java\n/** \n * Partitions the key space.\n * \n * <p><code>Partitioner</code> controls the partitioning of the keys of the \n * intermediate map-outputs. The key (or a subset of the key) is used to derive\n * the partition, typically by a hash function. The total number of partitions\n * is the same as the number of reduce tasks for the job. Hence this controls\n * which of the <code>m</code> reduce tasks the intermediate key (and hence the \n * record) is sent for reduction.</p>\n * partitioner是控制中间map阶段输出结果的key的分区.key通常被hash,分发到各个分区\n * 分区数一般和reduce job的个数相等,\n * @see Reducer\n */\n@InterfaceAudience.Public\n@InterfaceStability.Stable\npublic interface Partitioner<K2, V2> extends JobConfigurable {\n  \n  /** \n   * Get the paritition number for a given key (hence record) given the total \n   * number of partitions i.e. number of reduce-tasks for the job.\n   * \n   * <p>Typically a hash function on a all or a subset of the key.</p>\n   * 根据分区总数,例如reduce job个数,获取分区的编号.一般是对所有key或者key的一部分进行进行hash处理\n   * @param key the key to be paritioned.\n   * @param value the entry value.\n   * @param numPartitions the total number of partitions.\n   * @return the partition number for the <code>key</code>.\n   */\n  int getPartition(K2 key, V2 value, int numPartitions);\n}\n/**\n* hash分区的实现就是key取hashCode和reduce个数进行取模\n*/\npublic class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n\n  public void configure(JobConf job) {}\n\n  /** Use {@link Object#hashCode()} to partition. */\n  public int getPartition(K2 key, V2 value,\n                          int numReduceTasks) {\n    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n  }\n\n}\n```\n\n\n\n>  **需要注意的是**\n>\n> - 分区数一般和reduce job个数相等\n> - 如果分区数<reduce job个数,将导致输出有很多无用的空文件\n> - 如果分区数>reduce job个数,将导致有些map输出找不到hash路径,出现java.io.IOException: Illegal partition for xxx的异常\n\n## Combiner 局部汇总\n\nCombiner是hadoop对map阶段输出结果进行本地局部聚合,提高后面reduce的效率,避免大量数据进行网络传输.\n\n> **需要注意的是**\n>\n> - 并非所有的任务都适用于Combiner\n> - 求和等操作,局部聚合可以有效的提高后面reduce的效率\n> - 平均值等操作,这种并不适用,因为局部平均值和全局平均值还是有差异的","tags":["hadoop","MapReduce"],"categories":["Hadoop"]},{"title":"MapReduce教程","url":"/2018/08/17/Hadoop-MapReduce Tutorial/","content":"\n\n# MapReduce Tutorial\n\n## Overview\n\n- Hadoop MapReduce是一个运行在集群上,并行处理大量数据(TB级别)的框架\n- MapReduce任务通常讲输入切分成多个独立的块,这些数据块被独立的map任务并行的处理\n- 该框架会对map输出进行排序,作为reduce任务的输入\n- 该框架负责调度任务,监视任务并重新执行失败的任务\n- 通常,计算的节点和数据存储节点是同一个节点,也就是说,MapReduce框架和HDFS都运行在同一些列节点中.这个约束使得框架在数据已经存在的节点上有效地调度任务,从而产生跨集群的非常高的聚合带宽\n- MapReduce框架由一个ResourceManager,集群每个节点的NodeManager和每个应用程序的MRAppMaster组成\n- 必须指定输入输出路径,实现指定的接口或者抽象类,覆写map和reduce方法\n- hadoop任务客户端提交任务和相关配置到ResouceManager,ResouceManager负责把任务/配置分发到其他的从节点,并调度和监控任务,给客户端提供任务的状态和诊断信息\n- hadoop stream允许用户使用任何可执行的程序来作为mapper/reducer任务\n- hadoop pipes工具可以使用C++ API来实现mapper/reducer\n\n## Inputs and Outputs\n\n- MapReduce框架只针对<Key,Value>键值对类型操作.也就是说,每个MapReduce任务的输入是<Key,Value>形式,输入也是<Key,Value>形式,输入输出类型可不相同\n- Key,Value的类型必须是可以被框架序列化的类型,因此他们必须实现Writable接口.\n- Key的类型除了实现Writable接口之外,还需要实现WritableComparable接口,这样才能被排序\n- (input) ` <k1,v1> ->` **map** `-> <k2,v2> ->` **combine** `->  <k2,v2>  ->` **reduce** `-> <k3,v3>  `  (output)\n\n**hadoop jar的一些参数**\n\n- -files 可以使用逗号分隔,指定多个文件\n- -libjars 可以添加jar包到map和reduce类路径下\n- -archives 可以使用逗号分隔传入多个压缩包路径\n\n## MapReduce - User Interfaces\n\n- 实现Mapper和Reducer接口吗,并提供map/reduce的实现是任务的核心\n\n#### Mapper\n\n- Mapper将输入的,Key/Value键值对类型映射成中间结果的Key/Value键值对类型\n- Maps是独立的任务,负责将输入转成中间结果\n- 中间结果的类型无需和输入的类型一样\n- 一个输入可能对应0,1,或者多个输出\n- 每个InputSplit(由InputFormat产生)都有一个map任务\n- 可以通过[Job.setMapperClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html) 来传入Mapper的实现.框架将对每个键值对形式的InputSplit调用[map(WritableComparable, Writable, Context)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Mapper.html) 方法.如果需要清理一些必要资源,可以覆写`cleanup(Context)`方法\n- map的输出可以通过调用context.write(WritableComparable, Writable)来收集\n- 所有的中间结果会被框架分组,然后传给Reducer.用户使用 [Job.setGroupingComparatorClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)指定比较器Comparator来控制分组\n- Mapper的输出会被排序(sort)和打散(partitioner)分发给每一个Reducer.partitioner数目和reduce任务的数量相同.用户可以实现Partitioner接口来自定义打散规则,控制不同的Key分到对应的reduce任务中\n- 用户可以使用[Job.setCombinerClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)对中间输出结果进行本地聚合,这可以减少从Mapper传到Reduce的传输量\n- 中间结果都是以简单的 (key-len, key, value-len, value) 形式存储,也可通过Configuration设置对中间结果进行压缩\n\n##### How Many Maps?\n\n- map任务的通常是由输入数据的大小来决定的,也就是输入文件的块数\n- 对于cpu轻量级任务来说,每个节点map的并行度可达300,但是一般情况下并行度在10-100之间.任务的启动需要一定的时间,所以map任务至少需要1min的执行时间\n\n#### Reducer\n\n- Reducer将相同key的中间结果集进行处理\n- reduce任务的个数是通过[Job.setNumReduceTasks(int)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)来设置的\n- 通过 [Job.setReducerClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)来设置Reducer的实现类.框架对每组<key, (list of values)>的输入进行调用[reduce(WritableComparable, Iterable, Context)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Reducer.html) 方法进行处理,需要清理资源可以覆写cleanup(Context)\n\n##### Shuffle\n\n- 传到Reducer的输入是经过排序后的mapper的输出.shuffle阶段,框架将通过http获取相关partition的mapper输出\n\n##### Sort\n\n- 排序阶段,框架将Reducer的输入进行按Key进行分组\n- shuffle和sort同时进行.在map输出被拉取时,他们进行合并\n\n##### Secondary Sort\n\n- 如果中间结果key的分组规则需要和进入reducer前的keys的分组规则不一样,那么可以通过[Job.setSortComparatorClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)来设置比较器.因为[Job.setSortComparatorClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)时用来控制中间结果的keys是怎么分组的,所以可以用这个来对值进行二次排序\n\n##### Reduce\n\n- reduce阶段,将对每一组<key, (list of values)>输入调用reduce(WritableComparable, Iterable\\<Writable>, Context)方法\n- reduce任务通过 Context.write(WritableComparable, Writable)将输出结果写入文件系统\n- 输出结果并不会进行排序\n\n##### How Many Reduces?\n\n- 比较合理的reduce任务的个数计算公式是:0.95(或1.75)×节点数(注意,不是每个节点的最大container数)\n- 0.95系数可以使得reduce任务在map任务的输出传输结束后同时开始运行\n- 1.75系数可以使得计算快的节点在一批reduce任务计算结束之后开始计算第二批 reduce任务,实现负载均衡\n- 增加reduce的数量虽然会增加负载，但是可以改善负载匀衡，降低任务失败带来的负面影响\n- 放缩系数要比整数略小是因为要给推测性任务和失败任务预留reduce位置\n\n##### Reducer NONE\n\n- 如果不需要reduce任务,将reduce任务个数设置为0是合法的\n- 这种情况下,map任务的输出会直接写入文件系统的指定输出路径[FileOutputFormat.setOutputPath(Job, Path)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html).在写入文件系统前,map的输出是进行排序的\n\n#### Partitioner\n\n- partitioner控制中间map输出的key的分区\n- 可以按照key(或者key的一部分)来产生分区,默认是使用hash进行分区\n- 分区数和reduce任务的个数相等\n- 控制发送给reduce的任务个数\n\n#### Counter\n\n- Counter是一个公共基础工具,用来报告MapReduce应用的统计信息\n- Mapper和Reducer实现类都可以使用Counter来报告统计\n\n### Job Configuration\n\n- Job就是MapReduce任务的job配置代表\n- 一般MapReduce框架会严格按照Job的配置执行,但是有几种情况例外\n  - 某些配置参数被标记为final类型,所以是修改配置是没法达到目的的,例如1.1比例\n  - 某些配置虽然可以直接配置,但是还需要配合其他的参数一起配置才能生效\n- Job通常会指定Mapper,combiner(有必要的话),Partitioner,Reducer,InputFormat,OutputFormat的实现类\n- 输入可以使用下列方式指定输入数据文件集\n  - ([FileInputFormat.setInputPaths(Job, Path…)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html)/ [FileInputFormat.addInputPath(Job, Path)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html))\n  - ([FileInputFormat.setInputPaths(Job, String…)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html)/ [FileInputFormat.addInputPaths(Job, String)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html)\n\n- 输出可以使用([FileOutputFormat.setOutputPath(Path)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html))来指定输出文件集\n- 其他配置都是可选的,如Caparator的使用,将文件放置到DistributeCache,是否中间结果或者最终输出结果需要压缩,是否允许推测模式,最大任务重试次数等\n- 可以通过[Configuration.set(String, String)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/conf/Configuration.html)/ [Configuration.get(String)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/conf/Configuration.html)来设置和获取任意需要的参数.但是对于大的只读数据集,还是要用DistributedCache\n\n### Task Execution & Environment\n\n- MRAppMaster在独立的JVM中执行每个Mapper/Reducer任务(任务进程级别)\n- 子任务继承了MRAppMaster的环境.\n- 用户可以通过 `mapreduce.{map|reduce}.java.opts` 给子任务添加额外的参数\n- 运行时非标准类库路径可以通过-Djava.library.path=<>指定\n- 如果mapreduce.{map|reduce}.java.opts参数配置包含了*@taskid@*则在运行时被替换成taskId\n- 显示JVM GC,JVM JMX无密代理(这样可以结合jconsole,查看内存,线程,线程垃圾回收),最大堆内存,添加其他路径到任务java.library.path的例子\n\n```xml\n<property>\n  <name>mapreduce.map.java.opts</name>\n  <value>\n  -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc\n  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false\n  </value>\n</property>\n\n<property>\n  <name>mapreduce.reduce.java.opts</name>\n  <value>\n  -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc\n  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false\n  </value>\n</property>\n```\n\n#### Memory Management\n\n- 用户可以通过`mapreduce.{map|reduce}.memory.mb`指定子任务的最大虚拟内存.注意这个设置是进程级别的\n- 注意这个参数不要大于-Xmx的参数,否则VM可能会无法启动\n- `mapreduce.{map|reduce}.java.opt`只能配置MRAppMaster的子任务.配置守护线程的需要参考 [Configuring the Environment of the Hadoop Daemons](https://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-common/ClusterSetup.html#Configuring_Environment_of_Hadoop_Daemons)\n\n- map/reduce任务的性能,可能会被并发数,写入磁盘的频率影响.检查文件系统的统计报告,尤其是从map进入reduce的字节数,这参数是非常宝贵的.\n\n#### Map Parameters\n\n- map输出的记录会被序列化到缓冲区,元数据存储在统计缓冲区\n- 当缓冲区或者元数据超过一定的阈值,缓冲区的内容会被排序然后存储和写入磁盘\n- 如果缓冲区一直是满状态的,map线程将被阻塞\n- map结束后,没有写入磁盘的map输出记录继续写入.\n- 磁盘上所有的map输出文件段会合并成单个文件\n- 减少写入磁盘的次数,可以减少map的次数,但是加大缓存区会压缩mapper的可用内存\n\n| Name                             | Type  | Description                                            |\n| :------------------------------- | :---- | :----------------------------------------------------- |\n| mapreduce.task.io.sort.mb        | int   | 序列化和map输出到缓冲区的记录预排序的累计大小,单位为MB |\n| mapreduce.map.sort.spill.percent | float | 序列化缓冲区spill阈值比例,超过会将缓冲区内容写入磁盘   |\n\n- spill之后,如果在写入磁盘过程中,map的输出没有超过spill阈值,则会继续收集到spill结束\n- 如果是spill设置为0.33,在spill到磁盘的过程,缓冲区继续会被map的输出填充,下一次spill的时候再将这期间填充的内容写到磁盘\n- 如果spill设置为0.66,则不会触发下一次spill.也就是说,spill可以触发,但是不会阻塞\n- 一条记录大于缓冲区的会先触发spill,而且会被spill到一个单独的文件.无论这条记录有没有定义combiner,它都会被combiner传输\n\n#### Shuffle/Reduce Parameters\n\n- reduce将partitioner通过http指派给自己的map输出加载到内存,并定期合并输出到磁盘.\n- 如果中间结果是压缩输出,那么输出也是被reduce压缩的读进内存中,减少了内存的压力\n\n| Name                                          | Type  | Description                                                  |\n| :-------------------------------------------- | :---- | :----------------------------------------------------------- |\n| mapreduce.task.io.soft.factor                 | int   | 每次合并磁盘上段的数目.如果超过这个设置会分多次进行合并      |\n| mapreduce.reduce.merge.inmem.thresholds       | int   | 在合并写入磁盘之前,将排序后的map输出加载到内存的map输出数目.这个值通常设置很大(1000)或者直接禁用(0),因为内存合并要比磁盘合并的代价小得多.这个阈值只影响shuffle期间内存中合并的频率 |\n| mapreduce.reduce.shuffle.merge.percent        | float | 在内存合并之前,读取map输出的内存阈值,代表着用于存储map输出在内存中的百分比.因为map的输出并不适合存储在内存,所以设置很高会知道使得获取和合并的并行度下降.相反,设置为1可以使得内存运行的reduce更快.这个参数只影响shuffle期间的内存内合并频率 |\n| mapreduce.reduce.shuffle.input.buffer.percent | float | 在shuffle期间,可以分配来存储map输出的内存百分比,相对于`mapreduce.reduce.java.opts`指定的最大堆内存.把这个值设的大一点可以存储更多的map输出,但是也应该为框架预留一些内存 |\n| mapreduce.reduce.input.buffer.percent         | float | 相当于reduce阶段,用于存储map输出的最大堆内存的内存百分比.reduce开始的时候,map的输出被合并到磁盘,知道map输出在一定的阈值之内.默认情况下,在reduce开始之前,map的输出都会被合并到磁盘,这样才能使得reduce充分的利用到内存.对于只要内存密集型的reduce任务,应该增加这个值,减少磁盘的的往返时间 |\n\n#### Configured Parameters\n\n这些参数都是局部的,每个任务的\n\n| Name                       | Type    | Description                                    |\n| :------------------------- | :------ | :--------------------------------------------- |\n| mapreduce.job.id           | String  | The job id                                     |\n| mapreduce.job.jar          | String  | job.jar location in job directory              |\n| mapreduce.job.local.dir    | String  | The job specific shared scratch space          |\n| mapreduce.task.id          | String  | The task id                                    |\n| mapreduce.task.attempt.id  | String  | The task attempt id                            |\n| mapreduce.task.is.map      | boolean | Is this a map task                             |\n| mapreduce.task.partition   | int     | The id of the task within the job              |\n| mapreduce.map.input.file   | String  | The filename that the map is reading from      |\n| mapreduce.map.input.start  | long    | The offset of the start of the map input split |\n| mapreduce.map.input.length | long    | The number of bytes in the map input split     |\n| mapreduce.task.output.dir  | String  | The task’s temporary output directory          |\n\n在流任务执行过程中,这些参数会被转化.点(.)会被转成下划线(_),所以要想在流任务的mapper/reducer中获得这些值,需要使用下划线形式.\n\n#### Distributing Libraries\n\n- [DistributedCache](https://hadoop.apache.org/docs/r2.8.5/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#DistributedCache)分布式缓存可以分发jars和本地类库给map/reduce任务使用.\n- child-jvm总将自己的工作目录添加到java.library.path和LD_LIBRARY_PATH\n- 缓存中的类库可以通过System.loadLibrary或者System.load\n\n### Job Submission and Monitoring\n\n- Job是用户任务和ResourceManager交互的主要接口\n- Job的提交流程包括\n  - 检查输入输出路径\n  - 计算任务的InputSplit\n  - 有必要的话,设置必要的分布式缓存\n  - 拷贝任务的jar和配置到MapReduce系统目录\n  - 提交任务到ResourceManager.监控任务状态是可选的\n  - 任务的执行记录历史存放在 `mapreduce.jobhistory.intermediate-done-dir` 和`mapreduce.jobhistory.done-dir`\n\n#### Job Control\n\n- 对于单个MapReduce任务无法完成的任务,用户可能需要执行MapReduce任务链,才能完成.这还是非常容易的,因为任务的输出一般是存储在分布式文件系统中,所以一个任务的输出可以作为另一个任务的输入.这也就使得判断任务是否完成,不管成功或者失败,都需要用户来控制.主要有两种控制手段\n  - Job.submit() 提交任务到集群中,立即返回\n  - Job.waitForCompletion(boolean) 提交任务到集群中,等待其完成\n\n### Job Input\n\n- InputFormat描述了MapReduce任务的输入规范\n- InputFormat的职责是:\n  - 校验输入是否合法\n  - 将输入逻辑切分成InputSplit实例,之后将它们发送到独立的Mapper\n  - RecordReader 实现了从符合框架逻辑的InputSplit实例收集输入的记录,提供给Mapper进行处理\n\n- 默认的InputFormat是基于输入文件的总字节大小,将输入文件切分成逻辑的InputSplit实例,例如FileInputFormat的子类.然而,文件系统的blocksize只是split的上限,下限需要通过`mapreduce.input.fileinputformat.split.minsize`来设置\n- 压缩文件并不一定可以被切分,如.gz文件会把完整的文件交给一个mapper来处理\n\n#### InputSplit\n\n- InputSplit代表了一个独立Mapper处理的输入数据\n- 通常InputSplit是面向字节的,把面向字节转为面向记录是RecordReader的职责\n\n- FileSplit是默认的InputSplit实现,它把输入设置成mapreduce.map.input.file 属性,用于进行逻辑分割\n\n#### RecordReader\n\n- RecordReader负责将InputSplit的面向字节的输入转换成面向记录,提供给Mapper实现去处理每一条记录.因此RecordReader承担了从记录中提取出键值对的任务\n\n### Job Output\n\n- OutputFormat描述了MapReduce输出的规范\n- OutputFormat的职责:\n  - 校验任务的输出,例如输出目录是否存在\n  - RecordWriter实现可以将任务的输出写入到文件,存储在文件系统中\n- TextOutputFormat是默认的OutputFormat实现\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Hadoop,MapReduce"],"categories":["Hadoop","MapReduce"]},{"title":"Hello World","url":"/2018/07/18/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","tags":["PlayStation","Games"],"categories":["TestNest","test1","nest1","nest2"]}]