[{"title":"Gradle的安装和使用","url":"/2020/01/15/Gradle的安装和使用/","content":"# Gradle的安装和使用\n\n## 下载\n\n可以去Gradle官网下载最新的稳定版本,目前是6.2,我自己下的4.8.1\n\nhttps://downloads.gradle-dn.com/distributions/gradle-4.8.1-bin.zip\n\n## 安装\n\nGradle的安装和Maven类似,就简单的解压,配置环境变量到Path就ok了\n\n![image-20200221102147192](/images/image/image-20200221102147192.png)\n\n![image-20200221102231848](/images/image/image-20200221102231848.png)\n\n配置完成之后,打开cmd查看一下是否配置好了\n\n![image-20200221102351240](/images/image/image-20200221102351240.png)\n\n看到正确输出了gradle的版本就说明配置好了\n\n## IDEA配置Gradle\n\nGradle和Maven这一点不同,Gradle无需再IDEA中进行配置操作,本地仓库地址的配置可以再IDEA中配置\n\n![image-20200221102917822](/images/image/image-20200221102917822.png)\n\n这个我用的是环境变量来配置的,IDEA会自动识别,只需要在环境变量中新建一个GRADLE_USER_HOME变量指向自己的本地仓库地址就可以了\n\n![image-20200221103016741](/images/image/image-20200221103016741.png)\n\n## Gradle初体验\n\n### 新建Gradle工程\n\n选择Gradle和JDK\n\n![image-20200221103418062](/images/image/image-20200221103418062.png)\n\n### 填写项目的GAV\n\n填写项目的GAV坐标,点Finished\n\n![image-20200221103642260](/images/image/image-20200221103642260.png)\n\n### Gradle的目录\n\nGradle的目录结构和Maven类似\n\n![image-20200221104845138](/images/image/image-20200221104845138.png)\n\n- src就是source目录\n  - src/main放代码目录\n    - src/main/java 放java代码目录\n    - src/main/resouces放资源文件\n\n- src/test是测试目录\n  - src/test/java 放测试的java代码目录\n  - src/test/resouces放测试的资源文件\n\n## Groovy编程\n\n### 打开Groovy Console\n\nTools->Groovy Console\n\n![image-20200221105555961](/images/image/image-20200221105555961.png)\n\n### HelloWorld\n\n凡事先HelloWorld一下\n\n![image-20200221105836306](/images/image/image-20200221105836306.png)\n\n![image-20200221105915909](/images/image/image-20200221105915909.png)\n\n### Groovy语法\n\n- Groovy可以省略最末尾的分号\n\n- Groovy可以省略小括号\n\n  ![image-20200221110227268](/images/image/image-20200221110227268.png)\n\n> 这两个特性可以看出,Groovy的书写更加自由,随意\n\n- 定义变量 def\n\n  ![image-20200221110338615](/images/image/image-20200221110338615.png)\n\n  groovy会根据数据自动推断类型\n\n- 定义集合\n\n  ![image-20200221110630481](/images/image/image-20200221110630481.png)\n\n- 定义Map\n\n  ![image-20200221110825151](/images/image/image-20200221110825151.png)\n\n- 闭包\n\n  闭包就是一段代码块,在Gradle中主要是把闭包当参数使用\n\n  ![image-20200221111242382](/images/image/image-20200221111242382.png)\n\n- 带参数的闭包\n\n  ![image-20200221111511073](/images/image/image-20200221111511073.png)\n\n## Gradle配置文件\n\n- build.gradle 构建项目配置\n\n  ![image-20200221112432889](/images/image/image-20200221112432889.png)\n\n## 优先本地加载\n\n在repositories中指定先从本地加载\n\n![image-20200221113447193](/images/image/image-20200221113447193.png)","tags":["Gradle"],"categories":["Gradle"]},{"title":"Spring解析xml成BeanDefinition的过程","url":"/2019/05/05/Spring Bean的解析过程/","content":"# Spring Bean的解析过程\n\n## xml文件的读取\n\n从我们的入口开始\n\n```java\n@Test\npublic void testXml() {\n    ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\");\n    Student student = context.getBean(Student.class);\n    System.out.println(student.getUsername());\n}\n```\n\n先进入ClassPathXmlApplicationContext的构造器\n\n```java\npublic ClassPathXmlApplicationContext(String configLocation) throws BeansException {\n   this(new String[] {configLocation}, true, null);\n}\n```\n\n继续调用另一个构造器\n\n```java\npublic ClassPathXmlApplicationContext(\n      String[] configLocations, boolean refresh, @Nullable ApplicationContext parent)\n      throws BeansException {\n   super(parent);\n   //创建解析器，解析configLocations\n   setConfigLocations(configLocations);\n   if (refresh) {\n      refresh();\n   }\n}\n```\n\n这个refresh()方法是核心方法,点进去\n\n```java\npublic void refresh() throws BeansException, IllegalStateException {\n   synchronized (this.startupShutdownMonitor) {\n      //为容器初始化做准备，重要程度：0\n      // Prepare this context for refreshing.\n      prepareRefresh();\n\n      /*\n         重要程度：5\n        1、创建BeanFactory对象\n      * 2、xml解析\n      *  传统标签解析：bean、import等\n      *  自定义标签解析 如：<context:component-scan base-package=\"com.xiangxue.jack\"/>\n      *  自定义标签解析流程：\n      *     a、根据当前解析标签的头信息找到对应的namespaceUri\n      *     b、加载spring所以jar中的spring.handlers文件。并建立映射关系\n      *     c、根据namespaceUri从映射关系中找到对应的实现了NamespaceHandler接口的类\n      *     d、调用类的init方法，init方法是注册了各种自定义标签的解析类\n      *     e、根据namespaceUri找到对应的解析类，然后调用paser方法完成标签解析\n      *\n      * 3、把解析出来的xml标签封装成BeanDefinition对象\n      * */\n      // Tell the subclass to refresh the internal bean factory.\n      ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();\n\t\t\n       ....\n   }\n}\n```\n\n继续看obtainFreshBeanFactory()方法\n\n```java\nprotected ConfigurableListableBeanFactory obtainFreshBeanFactory() {\n   /**\n    * 核心方法，必须读，重要程度：5\n    * 这里使用了模板设计模式,Spring使用最多的设计模式,父类定义了模板,子类具体实现\n    * 其实反过来看,AbstractApplicationContext的refresh()同样也是定义了模板,onFresh()方法交给子类去实现\n    * 例如SpringBoot中嵌入式Tomcat启动就是覆写了onFresh()方法\n    * @see AbstractApplicationContext#onRefresh()\n    * */\n   refreshBeanFactory();\n   return getBeanFactory();\n}\n\n\t/**\n\t * 因为ClassPathXmlApplicationContext是AbstractRefreshableApplicationContext的子类\n\t * 所以跳转到AbstractRefreshableApplicationContext\n\t * @see AbstractRefreshableApplicationContext#refreshBeanFactory()\n\t * @throws BeansException        if initialization of the bean factory failed\n\t * @throws IllegalStateException if already initialized and multiple refresh\n\t *                               attempts are not supported\n\t *                               <p>\n\t * \n\t */\nprotected abstract void refreshBeanFactory() throws BeansException, IllegalStateException;\n\n```\n\n跳转到AbstractRefreshableApplicationContext的refreshBeanFactory()\n\n```java\nprotected final void refreshBeanFactory() throws BeansException {\n\n   //如果BeanFactory不为空，则清除BeanFactory和里面的实例\n   if (hasBeanFactory()) {\n      destroyBeans();\n      closeBeanFactory();\n   }\n   try {\n      //创建DefaultListableBeanFactory\n      //BeanFactory实例工厂,不管什么实例都可以从BeanFactory获取到\n      DefaultListableBeanFactory beanFactory = createBeanFactory();\n      beanFactory.setSerializationId(getId());\n\n      //设置是否可以循环依赖 allowCircularReferences\n      //是否允许使用相同名称重新注册不同的bean实现.\n      customizeBeanFactory(beanFactory);\n\n      /**\n       * 解析xml，并把xml中的标签封装成BeanDefinition对象\n       * 因为ClassPathXmlApplication是继承自 AbstractXmlApplicationContext\n       * 所以进入AbstractXmlApplicationContext,又是一个模板\n       * @see AbstractXmlApplicationContext#loadBeanDefinitions(org.springframework.beans.factory.support.DefaultListableBeanFactory)\n       */\n      loadBeanDefinitions(beanFactory);\n      synchronized (this.beanFactoryMonitor) {\n         this.beanFactory = beanFactory;\n      }\n   } catch (IOException ex) {\n      throw new ApplicationContextException(\"I/O error parsing bean definition source for \" + getDisplayName(), ex);\n   }\n}\n\nprotected abstract void loadBeanDefinitions(DefaultListableBeanFactory beanFactory)\n\t\t\tthrows BeansException, IOException;\n```\n\n继续看AbstractXmlApplicationContext的loadBeanDefinitions(...)\n\nxml的解析交给了XmlBeanDefinitionReader来解析\n\n```java\nprotected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException {\n   // Create a new XmlBeanDefinitionReader for the given BeanFactory.\n   //创建xml的解析器，这里是一个委托模式\n   //xml的解析工作,委托给XmlBeanDefinitionReader来解析\n   XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory);\n\n....\n    \n   //主要看这个方法  重要程度 5\n   loadBeanDefinitions(beanDefinitionReader);\n}\n```\n\n继续看loadBeanDefinitions(...)\n\n```java\nprotected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException {\n   Resource[] configResources = getConfigResources();\n   if (configResources != null) {\n      reader.loadBeanDefinitions(configResources);\n   }\n   //获取需要加载的xml配置文件\n   String[] configLocations = getConfigLocations();\n   if (configLocations != null) {\n       //委托给reader来解析xml\n      reader.loadBeanDefinitions(configLocations);\n   }\n}\n```\n\n点进去\n\n```java\n@Override\npublic int loadBeanDefinitions(String location) throws BeanDefinitionStoreException {\n   return loadBeanDefinitions(location, null);\n}\n\npublic int loadBeanDefinitions(String location, @Nullable Set<Resource> actualResources) throws BeanDefinitionStoreException {\n    ResourceLoader resourceLoader = getResourceLoader();\n    if (resourceLoader == null) {\n        throw new BeanDefinitionStoreException(\n            \"Cannot load bean definitions from location [\" + location + \"]: no ResourceLoader available\");\n    }\n\n    if (resourceLoader instanceof ResourcePatternResolver) {\n        // Resource pattern matching available.\n        try {\n            //把字符串类型的xml文件路径，形如：classpath*:user/**/*-context.xml,转换成Resource对象类型，其实就是用流\n            //的方式加载配置文件，然后封装成Resource对象，不重要，可以不看\n            Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location);\n\n            //主要看这个方法 ** 重要程度 5\n            int count = loadBeanDefinitions(resources);\n           ....\n            return count;\n        }\n        catch (IOException ex) {\n            throw new BeanDefinitionStoreException(\n                \"Could not resolve bean definition resource pattern [\" + location + \"]\", ex);\n        }\n    }\n    else {\n        // Can only load single resources by absolute URL.\n        Resource resource = resourceLoader.getResource(location);\n        int count = loadBeanDefinitions(resource);\n....\n        return count;\n    }\n}\n```\n\n把xml读出来之后封装成了Resource对象,开始解析Resource\n\n```java\n@Override\npublic int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException {\n   Assert.notNull(resources, \"Resource array must not be null\");\n   int count = 0;\n   for (Resource resource : resources) {\n      //模板设计模式，调用到子类中的方法\n      //又是一个模板,因为委托给了XmlBeanDefinitionReader\n      /**@see org.springframework.beans.factory.xml.XmlBeanDefinitionReader#loadBeanDefinitions(org.springframework.core.io.Resource)*/\n      count += loadBeanDefinitions(resource);\n   }\n   return count;\n}\n```\n\nxml读出来之后,又把Resource封装成带编码的对象,委托给XmlBeanDefinitionReader进行解析\n\n```java\n@Override\npublic int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException {\n   //EncodedResource带编码的对Resource对象的封装\n   //把资源流对象又做了编码的封装\n   return loadBeanDefinitions(new EncodedResource(resource));\n}\n```\n\n再看如何解析Resource的\n\n```java\npublic int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException {\n  ...\n      \n   Set<EncodedResource> currentResources = this.resourcesCurrentlyBeingLoaded.get();\n   if (currentResources == null) {\n      currentResources = new HashSet<>(4);\n      this.resourcesCurrentlyBeingLoaded.set(currentResources);\n   }\n   if (!currentResources.add(encodedResource)) {\n      throw new BeanDefinitionStoreException(\n            \"Detected cyclic loading of \" + encodedResource + \" - check your import definitions!\");\n   }\n   try {\n      //获取Resource对象中的xml文件流对象\n      InputStream inputStream = encodedResource.getResource().getInputStream();\n      try {\n         //InputSource是jdk中的sax xml文件解析对象\n         InputSource inputSource = new InputSource(inputStream);\n         if (encodedResource.getEncoding() != null) {\n            inputSource.setEncoding(encodedResource.getEncoding());\n         }\n         //主要看这个方法 **  重要程度 5\n         return doLoadBeanDefinitions(inputSource, encodedResource.getResource());\n      }\n      finally {\n         inputStream.close();\n      }\n   }\n   catch (IOException ex) {\n    ...\n   }\n   finally {\n      currentResources.remove(encodedResource);\n      if (currentResources.isEmpty()) {\n         this.resourcesCurrentlyBeingLoaded.remove();\n      }\n   }\n}\n```\n\n把InputStream流对象从Resouce中读出来,封装成InputSource对象\n\n```java\nprotected int doLoadBeanDefinitions(InputSource inputSource, Resource resource)\n      throws BeanDefinitionStoreException {\n\n   try {\n      //把inputSource 封装成Document文件对象，这是jdk的API\n      Document doc = doLoadDocument(inputSource, resource);\n\n      //主要看这个方法，根据解析出来的document对象，拿到里面的标签元素封装成BeanDefinition\n      int count = registerBeanDefinitions(doc, resource);\n      if (logger.isDebugEnabled()) {\n         logger.debug(\"Loaded \" + count + \" bean definitions from \" + resource);\n      }\n      return count;\n   }\n   catch (BeanDefinitionStoreException ex) {\n ....\n}\n```\n\n把流对象InputSource使用SAX进行解析成Document对象,对Document对象进行解析\n\n```java\npublic int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException {\n    //xml解析成Document之后,又将Document委托给BeanDefinitionDocumentReader来解析Document成BeanDefinition\n    //又来一记委托模式，BeanDefinitionDocumentReader委托这个类进行document的解析\n    BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader();\n    int countBefore = getRegistry().getBeanDefinitionCount();\n    //主要看这个方法，createReaderContext(resource) XmlReaderContext上下文，封装了XmlBeanDefinitionReader对象\n    documentReader.registerBeanDefinitions(doc, createReaderContext(resource));\n    return getRegistry().getBeanDefinitionCount() - countBefore;\n}\n```\n\nDocument委托给BeanDefinitionDocumentReader来解析Document成BeanDefinition\n\n```java\nvoid registerBeanDefinitions(Document doc, XmlReaderContext readerContext)\n      throws BeanDefinitionStoreException;\n\n\n@Override\npublic void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) {\n    this.readerContext = readerContext;\n    //主要看这个方法，把root节点传进去\n    doRegisterBeanDefinitions(doc.getDocumentElement());\n}\n\nprotected void doRegisterBeanDefinitions(Element root) {\n    BeanDefinitionParserDelegate parent = this.delegate;\n    this.delegate = createDelegate(getReaderContext(), root, parent);\n...\n    //又是模板,冗余设计,空实现\n    preProcessXml(root);\n\n    //主要看这个方法，标签具体解析过程\n    parseBeanDefinitions(root, this.delegate);\n    postProcessXml(root);\n\n    this.delegate = parent;\n}\n```\n\n把Document的根传进去,开始解析Document\n\n```java\nprotected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) {\n   if (delegate.isDefaultNamespace(root)) {\n      //获取根节点下的所有子节点\n      //遍历所有子节点,依次解析\n      NodeList nl = root.getChildNodes();\n      for (int i = 0; i < nl.getLength(); i++) {\n         Node node = nl.item(i);\n         if (node instanceof Element) {\n            Element ele = (Element) node;\n            if (delegate.isDefaultNamespace(ele)) {\n               //默认标签解析,import,alias,bean,beans\n               parseDefaultElement(ele, delegate);\n            }\n            else {\n               //自定义标签解析,委托给BeanDefinitionParserDelegate来解析\n               //context:component-scan等,使用了namespaceUri\n               delegate.parseCustomElement(ele);\n            }\n         }\n      }\n   }\n   else {\n      delegate.parseCustomElement(root);\n   }\n}\n```\n\n标签的解析分为默认标签(包括import,alias,bean,beans)和自定义标签(如context:componet-scan,mvc:annotation-drive等,这类带前缀的标签需要namespaceUri来指定实现,使用了SPI思想)\n\n## 默认标签的解析\n\n先看默认标签的解析\n\n```java\nprivate void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) {\n   //import标签解析  重要程度 1 ，可看可不看\n   if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) {\n      importBeanDefinitionResource(ele);\n   }\n   //alias标签解析 别名标签  重要程度 1 ，可看可不看\n   else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) {\n      processAliasRegistration(ele);\n   }\n   //bean标签，重要程度  5，必须看\n   else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) {\n      processBeanDefinition(ele, delegate);\n   }\n   else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) {\n      // recurse\n      doRegisterBeanDefinitions(ele);\n   }\n}\n```\n\n核心方法processBeanDefinition(...)\n\n```java\nprotected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) {\n   //重点看这个方法，重要程度 5 ，解析document，封装成BeanDefinition\n   BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele);\n   if (bdHolder != null) {\n\n      //该方法功能不重要，设计模式重点看一下，装饰者设计模式，加上SPI设计思想\n      bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder);\n      try {\n\n         //完成document到BeanDefinition对象转换后，对BeanDefinition对象进行缓存注册\n         // Register the final decorated instance.\n         BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry());\n      }\n      catch (BeanDefinitionStoreException ex) {\n         getReaderContext().error(\"Failed to register bean definition with name '\" +\n               bdHolder.getBeanName() + \"'\", ele, ex);\n      }\n      // Send registration event.\n      getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder));\n   }\n}\n```\n\n继续看BeanDefinitionParserDelegate是如何解析的parseBeanDefinitionElement(ele)\n\n```java\npublic BeanDefinitionHolder parseBeanDefinitionElement(Element ele) {\n   return parseBeanDefinitionElement(ele, null);\n}\n```\n\n```java\npublic BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) {\n   String id = ele.getAttribute(ID_ATTRIBUTE);\n   String nameAttr = ele.getAttribute(NAME_ATTRIBUTE);\n\n   List<String> aliases = new ArrayList<>();\n   if (StringUtils.hasLength(nameAttr)) {\n      String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS);\n      aliases.addAll(Arrays.asList(nameArr));\n   }\n\n   String beanName = id;\n   if (!StringUtils.hasText(beanName) && !aliases.isEmpty()) {\n      beanName = aliases.remove(0);\n     ...\n   }\n\n   //检查beanName是否重复\n   if (containingBean == null) {\n      checkNameUniqueness(beanName, aliases, ele);\n   }\n\n    //继续点进去看\n   AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean);\n   if (beanDefinition != null) {\n      if (!StringUtils.hasText(beanName)) {\n         try {\n            if (containingBean != null) {\n               beanName = BeanDefinitionReaderUtils.generateBeanName(\n                     beanDefinition, this.readerContext.getRegistry(), true);\n            }\n            else {\n               beanName = this.readerContext.generateBeanName(beanDefinition);\n               // Register an alias for the plain bean class name, if still possible,\n               // if the generator returned the class name plus a suffix.\n               // This is expected for Spring 1.2/2.0 backwards compatibility.\n               String beanClassName = beanDefinition.getBeanClassName();\n               if (beanClassName != null &&\n                     beanName.startsWith(beanClassName) && beanName.length() > beanClassName.length() &&\n                     !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) {\n                  aliases.add(beanClassName);\n               }\n            }\n            if (logger.isTraceEnabled()) {\n               logger.trace(\"Neither XML 'id' nor 'name' specified - \" +\n                     \"using generated bean name [\" + beanName + \"]\");\n            }\n         }\n         catch (Exception ex) {\n            error(ex.getMessage(), ele);\n            return null;\n         }\n      }\n      String[] aliasesArray = StringUtils.toStringArray(aliases);\n      return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray);\n   }\n\n   return null;\n}\n```\n\n继续点\n\n```java\npublic AbstractBeanDefinition parseBeanDefinitionElement(\n      Element ele, String beanName, @Nullable BeanDefinition containingBean) {\n\n   this.parseState.push(new BeanEntry(beanName));\n\n   String className = null;\n   if (ele.hasAttribute(CLASS_ATTRIBUTE)) {\n      className = ele.getAttribute(CLASS_ATTRIBUTE).trim();\n   }\n   String parent = null;\n   if (ele.hasAttribute(PARENT_ATTRIBUTE)) {\n      parent = ele.getAttribute(PARENT_ATTRIBUTE);\n   }\n\n   try {\n      //创建GenericBeanDefinition对象,设置parent和className\n      AbstractBeanDefinition bd = createBeanDefinition(className, parent);\n\n      //解析bean标签的属性，并把解析出来的属性设置到BeanDefinition对象中\n      parseBeanDefinitionAttributes(ele, beanName, containingBean, bd);\n      bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT));\n\n      //解析bean中的meta标签\n      parseMetaElements(ele, bd);\n\n      //解析bean中的lookup-method标签  重要程度：2，可看可不看\n      parseLookupOverrideSubElements(ele, bd.getMethodOverrides());\n\n      //解析bean中的replaced-method标签  重要程度：2，可看可不看\n      parseReplacedMethodSubElements(ele, bd.getMethodOverrides());\n\n      //解析bean中的constructor-arg标签  重要程度：2，可看可不看\n      parseConstructorArgElements(ele, bd);\n\n      //解析bean中的property标签  重要程度：2，可看可不看\n      parsePropertyElements(ele, bd);\n\n      //可以不看，用不到\n      parseQualifierElements(ele, bd);\n\n      bd.setResource(this.readerContext.getResource());\n      bd.setSource(extractSource(ele));\n\n      return bd;\n   }\n   catch (ClassNotFoundException ex) {\n   ...\n   }\n   finally {\n      this.parseState.pop();\n   }\n\n   return null;\n}\n```\n\n先创建BeanDefinition的封装GenericBeanDefinition\n\n### 属性解析\n\n解析每个节点的属性\n\n```java\npublic AbstractBeanDefinition parseBeanDefinitionAttributes(Element ele, String beanName,\n                                                            @Nullable BeanDefinition containingBean, AbstractBeanDefinition bd) {\n    //如果有singleton属性,先提示一下,建议使用scope属性\n    if (ele.hasAttribute(SINGLETON_ATTRIBUTE)) {\n        error(\"Old 1.x 'singleton' attribute in use - upgrade to 'scope' declaration\", ele);\n    }\n    else if (ele.hasAttribute(SCOPE_ATTRIBUTE)) {\n        //如果有scope属性,设置scope\n        bd.setScope(ele.getAttribute(SCOPE_ATTRIBUTE));\n    }\n    else if (containingBean != null) {\n        // Take default from containing bean in case of an inner bean definition.\n        bd.setScope(containingBean.getScope());\n    }\n\n    //设置abstract属性,不实例化,子类需要parent标签引用,父类提供了公共的属性,子类不需要写那么多了\n    if (ele.hasAttribute(ABSTRACT_ATTRIBUTE)) {\n        bd.setAbstract(TRUE_VALUE.equals(ele.getAttribute(ABSTRACT_ATTRIBUTE)));\n    }\n\n    //设置lazy-init属性\n    String lazyInit = ele.getAttribute(LAZY_INIT_ATTRIBUTE);\n    if (DEFAULT_VALUE.equals(lazyInit)) {\n        lazyInit = this.defaults.getLazyInit();\n    }\n    bd.setLazyInit(TRUE_VALUE.equals(lazyInit));\n\n    //设置autowired属性\n    String autowire = ele.getAttribute(AUTOWIRE_ATTRIBUTE);\n    bd.setAutowireMode(getAutowireMode(autowire));\n\n    //设置depends-on属性\n    if (ele.hasAttribute(DEPENDS_ON_ATTRIBUTE)) {\n        String dependsOn = ele.getAttribute(DEPENDS_ON_ATTRIBUTE);\n        bd.setDependsOn(StringUtils.tokenizeToStringArray(dependsOn, MULTI_VALUE_ATTRIBUTE_DELIMITERS));\n    }\n\n    //设置autowired-candidate\n    String autowireCandidate = ele.getAttribute(AUTOWIRE_CANDIDATE_ATTRIBUTE);\n    if (\"\".equals(autowireCandidate) || DEFAULT_VALUE.equals(autowireCandidate)) {\n        String candidatePattern = this.defaults.getAutowireCandidates();\n        if (candidatePattern != null) {\n            String[] patterns = StringUtils.commaDelimitedListToStringArray(candidatePattern);\n            bd.setAutowireCandidate(PatternMatchUtils.simpleMatch(patterns, beanName));\n        }\n    }\n    else {\n        bd.setAutowireCandidate(TRUE_VALUE.equals(autowireCandidate));\n    }\n\n    //设置primary\n    if (ele.hasAttribute(PRIMARY_ATTRIBUTE)) {\n        bd.setPrimary(TRUE_VALUE.equals(ele.getAttribute(PRIMARY_ATTRIBUTE)));\n    }\n\n    //设置init-method\n    if (ele.hasAttribute(INIT_METHOD_ATTRIBUTE)) {\n        String initMethodName = ele.getAttribute(INIT_METHOD_ATTRIBUTE);\n        bd.setInitMethodName(initMethodName);\n    }\n    else if (this.defaults.getInitMethod() != null) {\n        bd.setInitMethodName(this.defaults.getInitMethod());\n        bd.setEnforceInitMethod(false);\n    }\n\n    //设置destroy-method\n    if (ele.hasAttribute(DESTROY_METHOD_ATTRIBUTE)) {\n        String destroyMethodName = ele.getAttribute(DESTROY_METHOD_ATTRIBUTE);\n        bd.setDestroyMethodName(destroyMethodName);\n    }\n    else if (this.defaults.getDestroyMethod() != null) {\n        bd.setDestroyMethodName(this.defaults.getDestroyMethod());\n        bd.setEnforceDestroyMethod(false);\n    }\n\n    //设置factory-method,指定生成实例的工厂方法\n    if (ele.hasAttribute(FACTORY_METHOD_ATTRIBUTE)) {\n        bd.setFactoryMethodName(ele.getAttribute(FACTORY_METHOD_ATTRIBUTE));\n    }\n    //设置factory-bean属性,指定生成实例的工厂,这个需要配合factory-method使用\n    if (ele.hasAttribute(FACTORY_BEAN_ATTRIBUTE)) {\n        bd.setFactoryBeanName(ele.getAttribute(FACTORY_BEAN_ATTRIBUTE));\n    }\n\n    return bd;\n}\n```\n\nparseBeanDefinitionAttributes(..)方法主要是解析Node的属性并设置了BeanDefinition的一些属性\n\n### meta标签解析\n\n再看meta标签的解析,其实就是把bean标签的meta属性的key,value读取出来,设置到BeanDefinition,没啥用,一个标识而已\n\n```java\npublic void parseMetaElements(Element ele, BeanMetadataAttributeAccessor attributeAccessor) {\n   NodeList nl = ele.getChildNodes();\n   for (int i = 0; i < nl.getLength(); i++) {\n      Node node = nl.item(i);\n      if (isCandidateElement(node) && nodeNameEquals(node, META_ELEMENT)) {\n         Element metaElement = (Element) node;\n         String key = metaElement.getAttribute(KEY_ATTRIBUTE);\n         String value = metaElement.getAttribute(VALUE_ATTRIBUTE);\n         BeanMetadataAttribute attribute = new BeanMetadataAttribute(key, value);\n         attribute.setSource(extractSource(metaElement));\n         attributeAccessor.addMetadataAttribute(attribute);\n      }\n   }\n}\n```\n\n### lookup-method标签解析\n\n同样的lookup-method标签的解析也是类似的\n\n```java\npublic void parseLookupOverrideSubElements(Element beanEle, MethodOverrides overrides) {\n    NodeList nl = beanEle.getChildNodes();\n    for (int i = 0; i < nl.getLength(); i++) {\n        Node node = nl.item(i);\n        if (isCandidateElement(node) && nodeNameEquals(node, LOOKUP_METHOD_ELEMENT)) {\n            Element ele = (Element) node;\n            //获取name属性\n            String methodName = ele.getAttribute(NAME_ATTRIBUTE);\n            //获取bean属性\n            String beanRef = ele.getAttribute(BEAN_ELEMENT);\n            //封装成 LookupOverride\n            LookupOverride override = new LookupOverride(methodName, beanRef);\n            override.setSource(extractSource(ele));\n            //可能有多个lookup-method标签,所以用list装起来\n            overrides.addOverride(override);\n        }\n    }\n}\n```\n\n----\n\n不过值得一提的是,这个lookup-method的设计精髓主要是代理思想,很方便的实现了多态\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd\">\n\n    <bean class=\"com.lrj.test.bean.Student\" id=\"student\"/>\n\n    <bean class=\"com.lrj.test.bean.Women\" id=\"women\"/>\n    <!--实现多态,传入什么就是什么-->\n    <bean class=\"com.lrj.test.bean.AbstractClass\">\n        <lookup-method name=\"getPeople\" bean=\"women\"/>\n    </bean>\n</beans>\n```\n\n```java\npublic abstract class People {\n    public void show(){}\n}\npublic class Women extends People {\n    @Override\n    public void show() {\n        System.out.println(\"I am women\");\n    }\n}\npublic abstract class AbstractClass {\n    public void show() {\n        getPeople().show();\n    }\n\n    public abstract People getPeople();\n}\n//这个方法最终打印的是I am women\n@Test\npublic void testXml() {\n    ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\");\n    AbstractClass abstractClass=context.getBean(AbstractClass.class);\n    abstractClass.show();\n}\n```\n\n----\n\n### replace-method标签解析\n\n再看看parseReplacedMethodSubElements(...)解析replace-method属性\n\n```java\npublic void parseReplacedMethodSubElements(Element beanEle, MethodOverrides overrides) {\n   NodeList nl = beanEle.getChildNodes();\n   for (int i = 0; i < nl.getLength(); i++) {\n      Node node = nl.item(i);\n      if (isCandidateElement(node) && nodeNameEquals(node, REPLACED_METHOD_ELEMENT)) {\n         Element replacedMethodEle = (Element) node;\n         String name = replacedMethodEle.getAttribute(NAME_ATTRIBUTE);\n         String callback = replacedMethodEle.getAttribute(REPLACER_ATTRIBUTE);\n\n         //一个replaced-method标签封装成一个ReplaceOverride对象，最后加入到BeanDefinition对象中\n         ReplaceOverride replaceOverride = new ReplaceOverride(name, callback);\n         // Look for arg-type match elements.\n         List<Element> argTypeEles = DomUtils.getChildElementsByTagName(replacedMethodEle, ARG_TYPE_ELEMENT);\n         for (Element argTypeEle : argTypeEles) {\n\n            //根据方法参数类型来区分同名的不同的方法\n            String match = argTypeEle.getAttribute(ARG_TYPE_MATCH_ATTRIBUTE);\n            match = (StringUtils.hasText(match) ? match : DomUtils.getTextValue(argTypeEle));\n            if (StringUtils.hasText(match)) {\n               replaceOverride.addTypeIdentifier(match);\n            }\n         }\n         replaceOverride.setSource(extractSource(replacedMethodEle));\n         overrides.addOverride(replaceOverride);\n      }\n   }\n}\n```\n\n可以发现lookup-method和replace-method都放入了BeanDefinition的**MethodOverrides**类型的overrides属性中,也就是说,MethodOverrides包含了LookupOverride和ReplaceOverride两种类型的对象\n\n----\n\n```xml\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd\">\n    <bean class=\"com.lrj.test.bean.Replacement\" id=\"replacement\"/>\n    <bean class=\"com.lrj.test.bean.Origin\">\n        <replaced-method name=\"show\" replacer=\"replacement\">\n            <arg-type match=\"int\"/>\n        </replaced-method>\n    </bean>\n</beans>\n```\n\n```java\npublic class Origin {\n    public void show(String str) {\n        System.out.println(\"show str:\" + str);\n    }\n\n    public void show(int str) {\n        System.out.println(\"show int:\" + str);\n    }\n}\n\npublic class Replacement implements MethodReplacer {\n    @Override\n    public Object reimplement(Object obj, Method method, Object[] args) throws Throwable {\n        System.out.println(\"I am a placement method......\");\n        return null;\n    }\n}\n\n//这个打印的将会是show str:hello和I am a placement method......\n//说明int的被替换了,但是String的没有被替换,\n@Test\npublic void testReplace() {\n    ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(\"spring.xml\");\n    Origin origin = context.getBean(Origin.class);\n    origin.show(\"hello\");\n    origin.show(555);\n}\n```\n\n这个听说是在项目封版之后,不想改代码了,直接改配置,符合开闭原则,但是这个Replacement必须要实现MethodReplacer感觉有点鸡肋\n\n### constructor-arg标签解析\n\n这个没啥讲的,无非是根据index或者name来设置\n\n不过需要注意ConstructorArgumentValues对象保存了ValueHolder集合\n\n解析construct-arg标签,读取下标或者name封装成**ValueHolder**,构成BeanDefinition的**ConstructorArgumentValues**\n\n```java\npublic void parseConstructorArgElements(Element beanEle, BeanDefinition bd) {\n   NodeList nl = beanEle.getChildNodes();\n   for (int i = 0; i < nl.getLength(); i++) {\n      Node node = nl.item(i);\n      if (isCandidateElement(node) && nodeNameEquals(node, CONSTRUCTOR_ARG_ELEMENT)) {\n         parseConstructorArgElement((Element) node, bd);\n      }\n   }\n}\n\npublic void parseConstructorArgElement(Element ele, BeanDefinition bd) {\n    String indexAttr = ele.getAttribute(INDEX_ATTRIBUTE);\n    String typeAttr = ele.getAttribute(TYPE_ATTRIBUTE);\n    String nameAttr = ele.getAttribute(NAME_ATTRIBUTE);\n    if (StringUtils.hasLength(indexAttr)) {\n        try {\n            int index = Integer.parseInt(indexAttr);\n            if (index < 0) {\n                error(\"'index' cannot be lower than 0\", ele);\n            }\n            else {\n                try {\n                    this.parseState.push(new ConstructorArgumentEntry(index));\n                    Object value = parsePropertyValue(ele, bd, null);\n                    ConstructorArgumentValues.ValueHolder valueHolder = new ConstructorArgumentValues.ValueHolder(value);\n                    if (StringUtils.hasLength(typeAttr)) {\n                        valueHolder.setType(typeAttr);\n                    }\n                    if (StringUtils.hasLength(nameAttr)) {\n                        valueHolder.setName(nameAttr);\n                    }\n                    valueHolder.setSource(extractSource(ele));\n                    if (bd.getConstructorArgumentValues().hasIndexedArgumentValue(index)) {\n                        error(\"Ambiguous constructor-arg entries for index \" + index, ele);\n                    }\n                    else {\n                        //将ValueHolder添加到BeanDefinition\n                        bd.getConstructorArgumentValues().addIndexedArgumentValue(index, valueHolder);\n                    }\n                }\n                finally {\n                    this.parseState.pop();\n                }\n            }\n        }\n        catch (NumberFormatException ex) {\n            error(\"Attribute 'index' of tag 'constructor-arg' must be an integer\", ele);\n        }\n    }\n    else {\n        try {\n            this.parseState.push(new ConstructorArgumentEntry());\n            Object value = parsePropertyValue(ele, bd, null);\n            ConstructorArgumentValues.ValueHolder valueHolder = new ConstructorArgumentValues.ValueHolder(value);\n            if (StringUtils.hasLength(typeAttr)) {\n                valueHolder.setType(typeAttr);\n            }\n            if (StringUtils.hasLength(nameAttr)) {\n                valueHolder.setName(nameAttr);\n            }\n            valueHolder.setSource(extractSource(ele));\n            //将ValueHolder添加到BeanDefinition\n            bd.getConstructorArgumentValues().addGenericArgumentValue(valueHolder);\n        }\n        finally {\n            this.parseState.pop();\n        }\n    }\n}\n```\n\n### property标签解析\n\n```java\npublic void parsePropertyElements(Element beanEle, BeanDefinition bd) {\n   NodeList nl = beanEle.getChildNodes();\n   for (int i = 0; i < nl.getLength(); i++) {\n      Node node = nl.item(i);\n      if (isCandidateElement(node) && nodeNameEquals(node, PROPERTY_ELEMENT)) {\n         parsePropertyElement((Element) node, bd);\n      }\n   }\n}\n\npublic void parsePropertyElement(Element ele, BeanDefinition bd) {\n    //获取name属性\n    String propertyName = ele.getAttribute(NAME_ATTRIBUTE);\n    if (!StringUtils.hasLength(propertyName)) {\n        error(\"Tag 'property' must have a 'name' attribute\", ele);\n        return;\n    }\n    this.parseState.push(new PropertyEntry(propertyName));\n    try {\n        if (bd.getPropertyValues().contains(propertyName)) {\n            error(\"Multiple 'property' definitions for property '\" + propertyName + \"'\", ele);\n            return;\n        }\n        Object val = parsePropertyValue(ele, bd, propertyName);\n        //将属性设置包装成 PropertyValue\n        PropertyValue pv = new PropertyValue(propertyName, val);\n        parseMetaElements(ele, pv);\n        pv.setSource(extractSource(ele));\n        //将 PropertyValue添加到BeanDefinition\n        bd.getPropertyValues().addPropertyValue(pv);\n    }\n    finally {\n        this.parseState.pop();\n    }\n}\n```\n\n和解析构造函数的参数一样,对于property标签的解析,同样是将key,value封装成**PropertyValue**,添加到BeanDefinition中,形成**MutablePropertyValues**类型\n\n### qualifier标签解析\n\n```java\npublic void parseQualifierElements(Element beanEle, AbstractBeanDefinition bd) {\n   NodeList nl = beanEle.getChildNodes();\n   for (int i = 0; i < nl.getLength(); i++) {\n      Node node = nl.item(i);\n      if (isCandidateElement(node) && nodeNameEquals(node, QUALIFIER_ELEMENT)) {\n         parseQualifierElement((Element) node, bd);\n      }\n   }\n}\n\npublic void parseQualifierElement(Element ele, AbstractBeanDefinition bd) {\n    String typeName = ele.getAttribute(TYPE_ATTRIBUTE);\n    if (!StringUtils.hasLength(typeName)) {\n        error(\"Tag 'qualifier' must have a 'type' attribute\", ele);\n        return;\n    }\n    this.parseState.push(new QualifierEntry(typeName));\n    try {\n        AutowireCandidateQualifier qualifier = new AutowireCandidateQualifier(typeName);\n        qualifier.setSource(extractSource(ele));\n        String value = ele.getAttribute(VALUE_ATTRIBUTE);\n        if (StringUtils.hasLength(value)) {\n            qualifier.setAttribute(AutowireCandidateQualifier.VALUE_KEY, value);\n        }\n        NodeList nl = ele.getChildNodes();\n        for (int i = 0; i < nl.getLength(); i++) {\n            Node node = nl.item(i);\n            if (isCandidateElement(node) && nodeNameEquals(node, QUALIFIER_ATTRIBUTE_ELEMENT)) {\n                Element attributeEle = (Element) node;\n                String attributeName = attributeEle.getAttribute(KEY_ATTRIBUTE);\n                String attributeValue = attributeEle.getAttribute(VALUE_ATTRIBUTE);\n                if (StringUtils.hasLength(attributeName) && StringUtils.hasLength(attributeValue)) {\n                    BeanMetadataAttribute attribute = new BeanMetadataAttribute(attributeName, attributeValue);\n                    attribute.setSource(extractSource(attributeEle));\n                    qualifier.addMetadataAttribute(attribute);\n                }\n                else {\n                    error(\"Qualifier 'attribute' tag must have a 'name' and 'value'\", attributeEle);\n                    return;\n                }\n            }\n        }\n        bd.addQualifier(qualifier);\n    }\n    finally {\n        this.parseState.pop();\n    }\n}\n```\n\n至此,BeanDefinition的解析完成,此时再回到BeanDefinitionParserDelegate.parseBeanDefinitionElement(org.w3c.dom.Element, org.springframework.beans.factory.config.BeanDefinition)\n\n```java\npublic BeanDefinitionHolder parseBeanDefinitionElement(Element ele, @Nullable BeanDefinition containingBean) {\n   String id = ele.getAttribute(ID_ATTRIBUTE);\n   String nameAttr = ele.getAttribute(NAME_ATTRIBUTE);\n\n   List<String> aliases = new ArrayList<>();\n   if (StringUtils.hasLength(nameAttr)) {\n      String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS);\n      aliases.addAll(Arrays.asList(nameArr));\n   }\n\n   String beanName = id;\n   if (!StringUtils.hasText(beanName) && !aliases.isEmpty()) {\n      beanName = aliases.remove(0);\n      if (logger.isTraceEnabled()) {\n         logger.trace(\"No XML 'id' specified - using '\" + beanName +\n               \"' as bean name and \" + aliases + \" as aliases\");\n      }\n   }\n\n   //检查beanName是否重复\n   if (containingBean == null) {\n      checkNameUniqueness(beanName, aliases, ele);\n   }\n\n   //点进去\n   AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean);\n   if (beanDefinition != null) {\n      if (!StringUtils.hasText(beanName)) {\n         try {\n            if (containingBean != null) {\n               beanName = BeanDefinitionReaderUtils.generateBeanName(\n                     beanDefinition, this.readerContext.getRegistry(), true);\n            }\n            else {\n               beanName = this.readerContext.generateBeanName(beanDefinition);\n               // Register an alias for the plain bean class name, if still possible,\n               // if the generator returned the class name plus a suffix.\n               // This is expected for Spring 1.2/2.0 backwards compatibility.\n               String beanClassName = beanDefinition.getBeanClassName();\n               if (beanClassName != null &&\n                     beanName.startsWith(beanClassName) && beanName.length() > beanClassName.length() &&\n                     !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) {\n                  aliases.add(beanClassName);\n               }\n            }\n            if (logger.isTraceEnabled()) {\n               logger.trace(\"Neither XML 'id' nor 'name' specified - \" +\n                     \"using generated bean name [\" + beanName + \"]\");\n            }\n         }\n         catch (Exception ex) {\n            error(ex.getMessage(), ele);\n            return null;\n         }\n      }\n      String[] aliasesArray = StringUtils.toStringArray(aliases);\n      return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray);\n   }\n\n   return null;\n}\n```\n\n这里返回的是,又对BeanDefinition做了一层包装,成BeanDefinitionHolder,形成name->BeanDefinition的映射","tags":["Spring"],"categories":["Spring"]},{"title":"SpringCloud概述","url":"/2019/04/05/SpringCloud/","content":"# SpringCloud\n\n## SpringCloud概述\n\n### 官网\n\n- [官网](https://spring.io/projects/spring-cloud)\n\n### 主要功能\n![img](/images/image/image-20200207192330288.png)\n\n\n\n### 常用子项目\n\n![img](/images/image/image-20200207192553245.png)\n\n### 版本与兼容\n\n- SpringCloud的版本命名\n  - **版本命名**\n\n    SpringCloud的版本,前半部分(如Hoxton,Greenwich),意思是发布列车(ReleaseTrain),以伦敦地铁的站名命名,因为SpringCloud有很多的子项目,每个项目都有自己的版本管理,按照发布顺序以A,B,C等为首字母依次命名,已经发布的版本顺序为:\n\n  `Angel -> Brixton -> Camden -> Dalston -> Edgware -> Finchley -> Greenwich -> Hoxton`\n  ![img](/images/image/londontuberail-1.png)\n\n\n\n  后半部分(如SR,SR1,SR2),意思是服务发布(ServiceRelease),即重大Bug修复\n  - **版本发布流程**\n\n  `SNAPSHOT -> Mx -> RELEASE -> SRx`,其中x就是一些数字序号,例如M1,M2,SR1,SR2.SNAPSHOT为快照版本(开发版本),Mx为里程碑版本,此时并不是正式版本,但是已经接近正式版,经过多个版本迭代之后,发布第一个RELEASE版本,正式版本;在RELEASE版本之后如果有重大bug修复就会发布SR版本\n\n| Hoxton SR1 **CURRENT** **GA** | [ Reference Doc.](https://cloud.spring.io/spring-cloud-static/Hoxton.SR1/reference/html/spring-cloud.html) |\n| ----------------------------- | ------------------------------------------------------------ |\n| Hoxton **SNAPSHOT**           | [ Reference Doc.](https://spring.io/projects/spring-cloud)   |\n| Greenwich SR5 **GA**          | [ Reference Doc.](https://cloud.spring.io/spring-cloud-static/Greenwich.SR5/) |\n| Greenwich **SNAPSHOT**        | [ Reference Doc.](https://spring.io/projects/spring-cloud)   |\n\n----\n\n- SpringCloud的版本生命周期\n  - **版本发布规划**\n\n    https://github.com/spring-cloud/spring-cloud-release/milestones\n\n  - **版本发布记录**\n\n    https://github.com/spring-cloud/spring-cloud-release/releases\n\n  - **版本终止声明**\n\n    https://spring.io/projects/spring-cloud#overview\n\n- SpringBoot与SpringCloud的兼容性\n  - 版本兼容性非常重要\n    https://spring.io/projects/spring-cloud#overview\n\n| Release Train | Boot Version |\n| ------------- | ------------ |\n| Hoxton        | 2.2.x        |\n| Greenwich     | 2.1.x        |\n| Finchley      | 2.0.x        |\n| Edgware       | 1.5.x        |\n| Dalston       | 1.5.x        |\n\n- 生产环境如何选择选择\n  - 坚决不适用非稳定版本\n  - 坚决不适用end-of-life版本\n  - 尽量使用最新版本\n    - RELEASE版本可以观望/调研,因为是第一个正式版,并没有在生产上得以广泛应用\n    - SR2之后可以大规模使用\n\n### 版本选择\n\n- SpringCloud Hoxton SR1\n- SpringBoot 2.2.4.RELEASE\n\n```xml\n<parent>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>2.2.4.RELEASE</version>\n    <relativePath/> <!-- lookup parent from repository -->\n</parent> \n\n<dependencyManagement>\n    <dependencies>\n        <!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-dependencies -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-dependencies</artifactId>\n            <version>Hoxton.SR1</version>\n            <type>pom</type>\n            <scope>import</scope>\n        </dependency>\n    </dependencies>\n</dependencyManagement>\n```\n\n检查项目是否能运行\n\n```bash\nmvn clean install -U\n```\n\n## SpringCloud服务注册与发现\n\n- 使得服务消费者总能找到服务提供者\n\n### Consul单机版安装\n\n#### Consul下载\n\n- 下载Consoule https://releases.hashicorp.com/consul/1.6.3/consul_1.6.3_linux_amd64.zip\n\n#### 需要的端口\n\n| Use                                                          | Default Ports    |\n| :----------------------------------------------------------- | :--------------- |\n| DNS: The DNS server (TCP and UDP)                            | 8600             |\n| HTTP: The HTTP API (TCP Only)                                | 8500             |\n| HTTPS: The HTTPs API                                         | disabled (8501)* |\n| gRPC: The gRPC API                                           | disabled (8502)* |\n| LAN Serf: The Serf LAN port (TCP and UDP)                    | 8301             |\n| Wan Serf: The Serf WAN port TCP and UDP)                     | 8302             |\n| server: Server RPC address (TCP Only)                        | 8300             |\n| Sidecar Proxy Min: Inclusive min port number to use for automatically assigned sidecar service registrations. | 21000            |\n| Sidecar Proxy Max: Inclusive max port number to use for automatically assigned sidecar service registrations. | 21255            |\n\n检查端口是否被占用的方法\n\n```shell\nWindows:\n# 如果没有结果说明没有被占用\nnetstat -ano| findstr \"8500\"\n\nLinux:\n# 如果没有结果说明没有被占用\nnetstat -antp |grep 8500\n\nmacOS:\n# 如果没有结果说明没有被占用\nnetstat -ant | grep 8500\n或\nlsof -i:8500\n```\n\n#### 安装和启动\n\n- 解压\n\n```shell\n./consul agent -dev -client 0.0.0.0\n```\n\n- 严重是否成功\n\n```shell\n./consul -v\n```\n\n- 访问Consul首页`localhost:8500`\n\n**启动参数**\n\n- -ui 开启ui\n- -client 让consul拥有client功能,接受服务注册;0.0.0.0允许任意ip注册,不写只能使用localhost连接\n- -dev 以开发模式运行consul\n\n### 整合Consul\n\n- 添加依赖\n\n  ```xml\n  <dependency>\n      <groupId>org.springframework.cloud</groupId>\n      <artifactId>spring-cloud-starter-consul-discovery</artifactId>\n  </dependency>\n  ```\n\n  \n\n- 配置\n\n  ```yml\n  spring:\n   application:\n      # 指定注册到consul的服务名称,分隔符不能是下划线\n      # 如果服务发现组件是Consul,会强制转换成中划线,导致找不到服务\n      # 如果服务发现组件是Ribbon,则因为Ribbon的问题(把默认名称当初虚拟主机名,而虚拟主机名不能用下划线),会造成微服务之间无法调用\n      name: micro-service-user\n  \n    cloud:\n      consul:\n        host: 192.168.238.128\n        port: 8500\n  ```\n\n- 启动,检查consul ui的服务上线情况\n\n### Consul健康检查\n![img](/images/image/image-20200207231837317.png)\n\n\n- 添加健康检查依赖\n\n```xml\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n\n- 配置\n\n  ```yml\n  management:\n    endpoints:\n      web:\n        exposure:\n          include: '*'\n  ```\n\n  \n\n- 端点\n\n  http://localhost:8080/actuator 查看端点\n\n  http://localhost:8080/actuator/health 健康检查\n\n  添加详情配置,可以检查详细的健康情况\n\n  ```yml\n  management:\n      endpoint:\n        health:\n          show-details: always\n  ```\n  \n  ![img](/images/image/image-20200207233909996.png)\n\n\n\n\n- 简单研究一下健康检查的源码\n\n  以磁盘检查的为例\n\n  健康检查的类都继承了`AbstractHealthIndicator`抽象类,而`AbstractHealthIndicator`实现了`HealthIndicator`接口,所有健康检查实现类都必须实现`doHealthCheck(Health.Builder builder)`方法\n\n![img](/images/image/image-20200207235333557.png)\n\n\n```java\npublic class DiskSpaceHealthIndicator extends AbstractHealthIndicator {\n\n\tprivate static final Log logger = LogFactory.getLog(DiskSpaceHealthIndicator.class);\n\n\tprivate final File path;\n\n\tprivate final DataSize threshold;\n\n\t/**\n\t * Create a new {@code DiskSpaceHealthIndicator} instance.\n\t * @param path the Path used to compute the available disk space\n\t * @param threshold the minimum disk space that should be available\n\t */\n\tpublic DiskSpaceHealthIndicator(File path, DataSize threshold) {\n\t\tsuper(\"DiskSpace health check failed\");\n\t\tthis.path = path;\n\t\tthis.threshold = threshold;\n\t}\n\n\t@Override\n\tprotected void doHealthCheck(Health.Builder builder) throws Exception {\n        //获取可用的空间字节数\n\t\tlong diskFreeInBytes = this.path.getUsableSpace();\n        //如果可用的字节数大于预留字节数阈值则认为是健康的,设置status为UP\n\t\tif (diskFreeInBytes >= this.threshold.toBytes()) {\n\t\t\tbuilder.up();\n\t\t}\n\t\telse {\n            //否则任务是不健康的,设置status为DOWN\n\t\t\tlogger.warn(LogMessage.format(\"Free disk space below threshold. Available: %d bytes (threshold: %s)\",\n\t\t\t\t\tdiskFreeInBytes, this.threshold));\n\t\t\tbuilder.down();\n\t\t}\n        //输出总空间,可用空间和预留阈值\n\t\tbuilder.withDetail(\"total\", this.path.getTotalSpace()).withDetail(\"free\", diskFreeInBytes)\n\t\t\t\t.withDetail(\"threshold\", this.threshold.toBytes());\n\t}\n\n}\n\n//这个获取可用字节数还是挺好的,直接利用了File提供的方法\npublic long getUsableSpace() {\n    SecurityManager sm = System.getSecurityManager();\n    if (sm != null) {\n        sm.checkPermission(new RuntimePermission(\"getFileSystemAttributes\"));\n        sm.checkRead(path);\n    }\n    if (isInvalid()) {\n        return 0L;\n    }\n    //fs是默认的文件系统FileSystem fs = DefaultFileSystem.getFileSystem();\n    return fs.getSpace(this, FileSystem.SPACE_USABLE);\n}\n\n\n```\n\n健康检查使用了建造者模式,对于不同的健康指标非常方便,值得学习\n![img](/images/image/image-20200208000623044.png)\n\n\n- 整合Consul和SpringCloud的actuator\n\n  修改配置\n\n  ```yml\n  spring:\n      cloud:\n        consul:\n          host: 192.168.238.128\n          port: 8500\n          discovery:\n            health-check-path: /actuator/health\n  ```\n\n这样启动之后,再检查consul ui就可以发现没有红色的叉了\n![img](/images/image/image-20200208001532048.png)\n\n\n其他的健康检查配置\n![img](/images/image/image-20200208001745953.png)\n\n\n### 注册课程微服务到Consul\n\n- 添加依赖\n\n```xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-consul-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n\n- 配置\n\n```yml\nspring:\n  datasource:\n    url: jdbc:mysql://192.168.238.128:3306/ms?serverTimezone=GMT%2B8&characterEncoding=utf8&useSSL=false\n    hikari:\n      username: lrj\n      password: lu11221015\n      driver-class-name: com.mysql.cj.jdbc.Driver\n# JPA配置\n  jpa:\n    hibernate:\n      ddl-auto: update\n    show-sql: true\n  application:\n    name: micro-service-class\n  cloud:\n    consul:\n      host: 192.168.238.128\n      port: 8500\n      discovery:\n        health-check-path: /actuator/health\n# 暴露所有的actuator端点\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: '*'\n  # 开启健康检查详细信息\n  endpoint:\n    health:\n      show-details: always\nserver:\n  port: 8081\n```\n\n- 重构用户微服务\n\n```java\n@RestController\n@RequestMapping(\"user\")\npublic class UserController {\n    @Resource\n    private UserService userService;\n\n    @Resource\n    private DiscoveryClient discoveryClient;\n\n    @GetMapping(\"{id}\")\n    public Object findUserById(@PathVariable(\"id\") Integer id) {\n        return userService.findUserById(id);\n    }\n\n    @GetMapping(\"discoveryTest\")\n    public Object discoveryTest() {\n        return discoveryClient.getInstances(\"micro-service-class\");\n    }\n}\n\n```\n\n- 访问端点,可以发现不需要指定课程微服务的主机和端口就可以拿到相关信息,实现了服务发现\n![img](/images/image/image-20200208003543107.png)\n\n\n### 重构课程微服务\n\n- 原来\n\n  ```java\n  @Service\n  public class LessonServiceImpl implements LessonService {\n      @Resource\n      private LessonRepository lessonRepository;\n      @Resource\n      private LessonUserRepository lessonUserRepository;\n      @Resource\n      private RestTemplate restTemplate;\n  \n  \n      @Override\n      public Lesson buyById(Integer id) {\n          // 1. 根据课程id查询课程\n          Lesson lesson = lessonRepository.findById(id).orElseThrow(() -> new IllegalArgumentException(\"该课程不存在\"));\n          //根据课程查询是否已经购买过\n          LessonUser lessonUser = lessonUserRepository.findByLessonId(id);\n          if (lessonUser != null) {\n              return lesson;\n          }\n          //todo 2.登录之后获取userId\n          String userId = \"1\";\n          // 3. 如果没有购买过,查询用户余额\n          UserDTO userDTO = restTemplate.getForObject(\"http://localhost:8080/user/{userId}\", UserDTO.class, userId);\n          if (userDTO != null && userDTO.getMoney() != null &&\n                  userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() < 0) {\n              throw new IllegalArgumentException(\"余额不足\");\n          }\n          //4. 购买逻辑\n          //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录\n          return lesson;\n      }\n  \n  }\n  ```\n\n  这个写死了主机地址,无法动态获取微服务路径\n\n- 重构\n\n  ```java\n  public class LessonServiceImpl implements LessonService {\n      @Resource\n      private LessonRepository lessonRepository;\n      @Resource\n      private LessonUserRepository lessonUserRepository;\n      @Resource\n      private DiscoveryClient discoveryClient;\n      @Resource\n      private RestTemplate restTemplate;\n  \n      @Override\n      public Lesson buyById(Integer id) {\n          // 1. 根据课程id查询课程\n          Lesson lesson = lessonRepository.findById(id).orElseThrow(() -> new IllegalArgumentException(\"该课程不存在\"));\n          //根据课程查询是否已经购买过\n          LessonUser lessonUser = lessonUserRepository.findByLessonId(id);\n          if (lessonUser != null) {\n              return lesson;\n          }\n          //todo 2.登录之后获取userId\n          String userId = \"1\";\n          List<ServiceInstance> instances = discoveryClient.getInstances(\"micro-service-user\");\n          if (instances != null && !instances.isEmpty()) {\n              //todo 需要改进,如果存在多个实例,需要考虑负载均衡\n              ServiceInstance instance = instances.get(0);\n              URI uri = instance.getUri();\n              UserDTO userDTO = restTemplate.getForObject(uri + \"/user/{userId}\", UserDTO.class, userId);\n              if (userDTO != null && userDTO.getMoney() != null &&\n                      userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() < 0) {\n                  throw new IllegalArgumentException(\"余额不足\");\n              }\n              //4. 购买逻辑\n              //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录\n              return lesson;\n          }\n          throw new IllegalArgumentException(\"用户微服务异常,无法购买课程\");\n      }\n  \n  }\n  \n  ```\n\n  可以动态的获取到用户微服务的地址,请求正常\n  ![img](/images/image/image-20200208004838563.png)\n\n\n### 元数据\n\nConsul是没有元数据的概念的,所以SpringCloud做了个适配,在consul下设置tags作为元数据.\n\n元数据可以对微服务添加描述,标识,例如机房在哪里,这样可以进行就近判断,或者当就近机房不可用时才检查远程机房,当两者都不可用时才认为服务不可用等实现容灾或者跨机房\n\n- 配置\n\n  ```yml\n  spring:\n    cloud:\n      consul:\n        host: 192.168.238.128\n        port: 8500\n        discovery:\n          health-check-path: /actuator/health\n          tags: JiFang=Beijing,JiFang=Shanghai\n  ```\n\n- 实现机房选择\n\n  ```java\n  @GetMapping(\"discoveryTest\")\n  public Object discoveryTest() {\n      List<ServiceInstance> instances = discoveryClient.getInstances(\"micro-service-class\");\n      if (instances != null) {\n          List<ServiceInstance> shanghaiInstances = instances.stream()\n                  .filter(s -> s.getMetadata().containsKey(\"Shanghai\")).collect(Collectors.toList());\n          if (!shanghaiInstances.isEmpty()) {\n              return shanghaiInstances;\n          }\n      }\n      return instances;\n  }\n  ```\n\n\n\n","tags":["SpringCloud"],"categories":["SpringCloud"]},{"title":"Flume使用教程","url":"/2019/04/04/Flume/","content":"\n# Flume\n\nFlume是高可靠,高可用的,分布式的海量日志收集,聚合和传输的系统,\n\n## 数据流模型\n\n![image-20200225205404604](/images/image/image-20200225205404604.png)\n\nA Flume event is defined as a unit of data flow having a byte payload and an optional set of string attributes. A Flume agent is a (JVM) process that hosts the components through which events flow from an external source to the next destination (hop).\n\nA Flume source consumes events delivered to it by an external source like a web server. The external source sends events to Flume in a format that is recognized by the target Flume source. For example, an Avro Flume source can be used to receive Avro events from Avro clients or other Flume agents in the flow that send events from an Avro sink. A similar flow can be defined using a Thrift Flume Source to receive events from a Thrift Sink or a Flume Thrift Rpc Client or Thrift clients written in any language generated from the Flume thrift protocol.When a Flume source receives an event, it stores it into one or more channels. The channel is a passive store that keeps the event until it’s consumed by a Flume sink. The file channel is one example – it is backed by the local filesystem. The sink removes the event from the channel and puts it into an external repository like HDFS (via Flume HDFS sink) or forwards it to the Flume source of the next Flume agent (next hop) in the flow. The source and sink within the given agent run asynchronously with the events staged in the channel.\n\n- Flume中,数据流的单位称为事件(event)\n\n  它具有一个字节的有效载荷,还可以携带一系列的字符串属性.\n\n- Flume中Agent是一个JVM进程\n\n  Agent由一系列的组件组成,通过这些组件,将事件从外部数据源传到另一个位置\n\n- Flume源端部分(source)消费着来至外部数据源的事件,例如web服务端的\n\n  外部数据源需要以目标flume源端能够识别的形式发出.例如，Avro类型的flume源端可用于接收从Avro客户端发出的事件,或从其他的Avro类型的Flume发送端(sink)发出的事件\n\n- flume源端接收事件后,会将事件存入一个或者多个管道(Channel)\n\n  Flume的管道存储所源端接收到的事件,直到它Flume发送端消费了这些事件.File Channel就是一个例子,它将事件备份在本地文件系统中\n\n- flume的发送端消费着Channel中的事件,并把它推送HDFS等外部仓库中(通过HDFS类型的Sink发送)\n\n## Agent的三个组件\n\n**Source**\n\n数据源\n\n**Channel** \n\n通道,缓冲\n\n数据存在哪里\n\n**Sink** \n\n数据输出到哪里\n\n## 下载\n\n```bash\nwget http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2.tar.gz\n```\n\n## 配置\n\n所有的配置都参考官网的配置,只要配置agent,source,channel,sink即可\n\nhttp://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#setting-up-an-agent\n\n其中所有的粗体都是必须要配置的,其他普通的属性是可选的\n\n### 样例\n\n为了便于配置,可以先把这个模板copy到notepad,根据需要改一下就可以\n\n```properties\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\n# agent a1的 source 的名字设置为r1\na1.sources = r1 \n# agent a1的 sinks 的名字设置为k1\na1.sinks = k1\n# agent a1的 channels 的名字设置为c1\na1.channels = c1\n\n# Describe/configure the source\n# agent a1的 source r1 的类型设置为netcat\na1.sources.r1.type = netcat\n# agent a1的 source r1 的绑定的host和port\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\n# agent a1的 sinks k1 的类型为logger类型,也就是控制台输出\na1.sinks.k1.type = logger\nc\n# Use a channel which buffers events in memory\n# agent a1的 channels c1 的类型为memory类型,也就是基于内存进行缓冲,所以flume对内存还是由一定要求的\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\n# agent a1的 source,channel,sink如何连接\n# 这里配置了agent a1的source r1的channel为c1\n# 也就是说,事件从在被sink消费之前被保存在c1的channel中\na1.sources.r1.channels = c1\n# 这里配置了agent a1的sink k1的channel为c1,\n# 也就是说agent a1的sink k1要从c1 channel消费事件\na1.sinks.k1.channel = c1\n```\n\n**参数说明**\n\n- a1\n\n  这个是agent的名字,启动flume必须指定agent的名字,独一无二的即可,因为agent是一个独立jvm进程\n\n### netcat样例\n\nnetcat.conf\n\n```properties\nmyConsoleAgent.sources = source1\nmyConsoleAgent.sinks = sink1\nmyConsoleAgent.channels = channel1\n\n# 数据从哪里来\nmyConsoleAgent.sources.source1.type = netcat\nmyConsoleAgent.sources.source1.bind = localhost\nmyConsoleAgent.sources.source1.port = 44444\n\n# 输出到哪里去\nmyConsoleAgent.sinks.sink1.type = logger\n\n# 读取数据之后先放到哪里\nmyConsoleAgent.channels.channel1.type = memory\n\n\n# 将source,channel,sink连接起来\nmyConsoleAgent.sources.source1.channels = channel1\nmyConsoleAgent.sinks.sink1.channel = channel1\n```\n\n启动agent\n\n```bash\n$FLUME_HOME/bin/flume-ng \\\n--conf $FLUME_HOME \\\n--conf-file ./netcat.conf \\\n--name myConsoleAgent\n```\n\n### exec 命令行输入\n\nexec.conf\n\n```properties\nmyConsoleAgent.sources = source1\nmyConsoleAgent.sinks = sink1\nmyConsoleAgent.channels = channel1\n\n# 数据从哪里来\nmyConsoleAgent.sources.source1.type = exec\nmyConsoleAgent.sources.source1.command = tail -F /usr/software/flume-1.6.0-cdh5.16.2/input/exec.log \n\n\n# 输出到哪里去\nmyConsoleAgent.sinks.sink1.type = hdfs\nmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/exec\nmyConsoleAgent.sinks.sink1.hdfs.batchSize = 10\nmyConsoleAgent.sinks.sink1.hdfs.fileType = DataStream \nmyConsoleAgent.sinks.sink1.writeFormat = Text \n\n# 读取数据之后先放到哪里\nmyConsoleAgent.channels.channel1.type = memory\n\n\n# 将source,channel,sink连接起来\nmyConsoleAgent.sources.source1.channels = channel1\nmyConsoleAgent.sinks.sink1.channel = channel1\n\n```\n\n启动agent\n\n```bash\n$FLUME_HOME/bin/flume-ng \\\n--conf $FLUME_HOME \\\n--conf-file ./exec.conf \\\n--name myConsoleAgent\n```\n\n\n\n### spooldir 监听文件夹\n\nspool.conf\n\n```properties\nmyConsoleAgent.sources = source1\nmyConsoleAgent.sinks = sink1\nmyConsoleAgent.channels = channel1\n\n# 数据从哪里来\nmyConsoleAgent.sources.source1.type = spooldir\nmyConsoleAgent.sources.source1.spoolDir = /usr/software/flume-1.6.0-cdh5.16.2/input\nmemory.sources.source1.includePattern = *.log\n\n# 输出到哪里去\nmyConsoleAgent.sinks.sink1.type = hdfs\nmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/spool/%Y%m%d%H%M\nmyConsoleAgent.sinks.sink1.hdfs.batchSize = 10\nmyConsoleAgent.sinks.sink1.hdfs.fileType = DataStream \nmyConsoleAgent.sinks.sink1.writeFormat = Text \nmyConsoleAgent.sinks.sink1.hdfs.rollInterval=30\nmyConsoleAgent.sinks.sink1.hdfs.rollSize=1024\nmyConsoleAgent.sinks.sink1.hdfs.hdfs.rollCount=100\nmyConsoleAgent.sinks.sink1.hdfs.useLocalTimeStamp =true\n\n\n# 读取数据之后先放到哪里\nmyConsoleAgent.channels.channel1.type = memory\n\n\n# 将source,channel,sink连接起来\nmyConsoleAgent.sources.source1.channels = channel1\nmyConsoleAgent.sinks.sink1.channel = channel1\n```\n\n启动agent\n\n```bash\n$FLUME_HOME/bin/flume-ng \\\n--conf $FLUME_HOME \\\n--conf-file ./spool.conf \\\n--name myConsoleAgent\n```\n\n### taildir 监听文件夹(最常用的)\n\n这个监听最常用,也是必须掌握的,因为spoolDir并没有偏移量的概念,使用有很大的局限性\n\nspooldir的缺点\n\n- 每次采集完成之后,会在文件结尾设置Complete的后缀,而且之后不能再使用相同的文件名,否则报错\n- 采集完之后的文件不能再次写入,否则报错\n\n而taildir很好的使用了偏移量的概念,记录在一个json文件中,可以实现断点还原\n\n\n\ntaildir.conf\n\n```properties\nmyConsoleAgent.sources = source1\nmyConsoleAgent.sinks = sink1\nmyConsoleAgent.channels = channel1\n\n# 数据从哪里来\nmyConsoleAgent.sources.source1.type = TAILDIR\nmyConsoleAgent.sources.source1.filegroups = f1 f2\nmyConsoleAgent.sources.source1.filegroups.f1 = input/taildir/test1/hello.txt\nmyConsoleAgent.sources.source1.headers.f1.headerKey1 = value1\nmyConsoleAgent.sources.source1.filegroups.f2 = input/taildir/test2/.*.log\nmyConsoleAgent.sources.source1.headers.f2.headerKey1 = value2-1\nmyConsoleAgent.sources.source1.headers.f2.headerKey2 = value2-2\nmyConsoleAgent..sources.source1.maxBatchCount = 1000\nmyConsoleAgent.sources.source1.fileHeader = true\n\n \n\n# 输出到哪里去\nmyConsoleAgent.sinks.sink1.type = hdfs\nmyConsoleAgent.sinks.sink1.hdfs.path = hdfs://hadoop001:9000/flume/tailDir/%Y%m%d%H%M\nmyConsoleAgent.sinks.sink1.hdfs.batchSize = 100\nmyConsoleAgent.sinks.sink1.hdfs.fileType = DataStream \nmyConsoleAgent.sinks.sink1.writeFormat = Text \nmyConsoleAgent.sinks.sink1.hdfs.rollInterval=30\nmyConsoleAgent.sinks.sink1.hdfs.rollSize=10240\nmyConsoleAgent.sinks.sink1.hdfs.hdfs.rollCount=100\nmyConsoleAgent.sinks.sink1.hdfs.useLocalTimeStamp =true\n\n\n# 读取数据之后先放到哪里\nmyConsoleAgent.channels.channel1.type = memory\n\n\n# 将source,channel,sink连接起来\nmyConsoleAgent.sources.source1.channels = channel1\nmyConsoleAgent.sinks.sink1.channel = channel1\n\n```\n\n启动agent\n\n```bash\n$FLUME_HOME/bin/flume-ng \\\n--conf $FLUME_HOME \\\n--conf-file ./taildir.conf \\\n--name myConsoleAgent\n```\n\n","tags":["Flume"],"categories":["Flume"]},{"title":"Azkaband的安装和使用","url":"/2019/04/03/Azkaband的安装和使用/","content":"\n# Azkaband的安装和使用\n\nAzkaban可以设计很复杂的工作流,解决任务之间的依赖关系,还是很方便的\n\nAzkaban是一个由LinkedIn公司开发的分布式工作流管理框架,主要是为了解决Hadoop工作之间的依赖关系.例如,我们需要有顺序地执行任务,把ETL任务的数据导入到RMBS中,以提供数据分析支持.\n\n![image-20200220101138796](/images/image/image-20200220101138796.png)\n\n- Azkaban是一个开源的工作流管理框架\n- Azkaban是由LinkedIn(领英,还有一个比较出名的产品Kafka)公司创建,用来管理批处理工作流的任务调度\n- Azkaban提供了WebUI接口来维护工作流\n\n## Feature\n\n- Compatible with any version of Hadoop\n\n  兼容Hadoop各个版本\n\n- Easy to use web UI\n\n  提供易用的WebUI\n\n- Simple web and http workflow uploads\n\n  web和http方式上传工作流(这个被诟病了,应该提供拖拉拽的,写了配置还要上传,确实不方便)\n\n- Project workspaces\n\n- Scheduling of workflows\n\n  工作流调度\n\n- Modular and pluginable\n\n  模块化,可插拔\n\n- Authentication and Authorization\n\n  提供认证和授权\n\n- Tracking of user actions\n\n- Email alerts on failure and successes\n\n  任务失败和成功邮件告警\n\n- SLA alerting and auto killing\n\n  SLA告警和自动结束任务\n\n- Retrying of failed jobs\n\n  任务失败重试\n\n## 版本选择\n\n- 2.x\t基本可以不用看了\n- 3.x    当前推荐的\n\n## 模式\n\n- stand alone mode(or solo-server) 单个\n\n  - web server和executor server运行在同一个进程里\n  - 数据存储的数据库是内嵌的,不需要自己安装\n\n  - 主要使用在小规模场景中\n\n- Multiple executor mode \n\n  - 多用于生产\n  - 存储的DB应该是一个主从结构的MySQL\n  - web server和executor server运行在不同的host上,这样升级和维护时,互不影响\n\n## 下载和编译\n\nAzkban不直接提供安装包,需要自己从源码编译\n\nAzkaban是Gradle构建的,Java版本需要1.8以上\n\n```bash\nwget https://github.com/azkaban/azkaban/archive/3.81.0.tar.gz\ntar -zxvf ./3.81.0.tar.gz -C /usr/software\ncd /usr/software/azkaban-3.81.0\n./gradlew build installDist -x test\n```\n\n这个需要git,如果不安装会报错Failed to apply plugin [id 'com.cinnober.gradle.semver-git']\n\n![image-20200220105916429](/images/image/image-20200220105916429.png)\n\n所以先得安装git\n\n```bash\nsudo yum install -y git\n```\n\n执行有报错....缺少g++:Could not find Linker 'g++' in system path\n\n![image-20200220135312805](/images/image/image-20200220135312805.png)\n\n好吧在安装g++\n\n```bash\nsudo yum install g++\nsudo yum install -y gcc-c++*\n```\n\n## 部署\n\n编译完成后会出现3个目录,一个是azkaban-solo-server,azkaban-web-server,azkaban-exec-server,分别对应了单机版的azkaban-executor-server,azkaban-web-server和多实例的azkaban-executor-server\n\n我们只需要把他们目录下的build/distributions的压缩包拷贝出来就行\n\n### 单机版\n\n```bash\ntar -zxvf ./azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz\n```\n\n修改azkban的配置文件azkaban.properties,用默认也可以\n\n```bash\nvi conf/azkaban.properties\n```\n\n```properties\n# Azkaban Personalization Settings\n# 设置azkban的名称,这个是UI首页左上角,图标右侧上方的文字\nazkaban.name=LRJ\n# UI首页左上角,图标右侧下方的文字\nazkaban.label=MyAzkaban\nazkaban.color=#FF3601\nazkaban.default.servlet.path=/index\nweb.resource.dir=web/\n# 修改一下时区,免得调度时间不对\ndefault.timezone.id=Asia/Shanghai\n# Azkaban UserManager class\nuser.manager.class=azkaban.user.XmlUserManager\nuser.manager.xml.file=conf/azkaban-users.xml\n# Loader for projects\nexecutor.global.properties=conf/global.properties\nazkaban.project.dir=projects\ndatabase.type=h2\nh2.path=./h2\nh2.create.tables=true\n# Velocity dev mode\nvelocity.dev.mode=false\n# Azkaban Jetty server properties.\njetty.use.ssl=false\njetty.maxThreads=25\n# Web端口,端口占用的时候改一改\njetty.port=8081\n# Azkaban Executor settings\nexecutor.port=12321\n# mail settings\n# 邮件服务器设置\nmail.sender=\nmail.host=\n# 开启SSL设置\n# User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.\n# enduser -> myazkabanhost:443 -> proxy -> localhost:8081\n# when this parameters set then these parameters are used to generate email links.\n# if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.\n# azkaban.webserver.external_hostname=myazkabanhost.com\n# azkaban.webserver.external_ssl_port=443\n# azkaban.webserver.external_port=8081\n# 全局邮箱设置\njob.failure.email=\njob.success.email=\nlockdown.create.projects=false\ncache.directory=cache\n# JMX stats\njetty.connector.stats=true\nexecutor.connector.stats=true\n# Azkaban plugin settings\n# 插件设置\nazkaban.jobtype.plugin.dir=plugins/jobtypes\n# Number of executions to be displayed\nazkaban.display.execution_page_size=16\nazkaban.use.multiple.executors=true\n# Azkaban Ramp Feature Configuration\n#Ramp Feature Related\nazkaban.ramp.enabled=true\nazkaban.ramp.status.polling.enabled=true\nazkaban.ramp.status.polling.interval.min=30\nazkaban.ramp.status.push.interval.threshold=15\nazkaban.ramp.status.pull.interval.threshold=100\n```\n\n添加个用户名和密码,也可以使用默认的\n\n```xml\n<azkaban-users>\n  <user groups=\"azkaban\" password=\"azkaban\" roles=\"admin\" username=\"azkaban\"/>\n    <!--添加自己的用户-->\n  <user groups=\"lurongjiang\" password=\"lurongjiang\" roles=\"admin\" username=\"lurongjiang\"/>\n  <user password=\"metrics\" roles=\"metrics\" username=\"metrics\"/>\n  <role name=\"admin\" permissions=\"ADMIN\"/>\n  <role name=\"metrics\" permissions=\"METRICS\"/>\n</azkaban-users>\n```\n\n### 启动\n\n```bash\n./bin/start-solo.sh\n# 查看一下\njps -m\n```\n\n访问webUI端口就可以显示了,可以看到我们改的两个位置\n\n![image-20200220185537929](/images/image/image-20200220185537929.png)\n\n输入用户名和密码就就可以进去了\n\n![image-20200220185655686](/images/image/image-20200220185655686.png)\n\n\n\n\n\n### 使用\n\n参考[Create Flows](https://azkaban.readthedocs.io/en/latest/createFlows.html)\n\n- 创建一个名字为`flow20.project`的文件,写入内容:\n\n  ```yml\n  azkaban-flow-version: 2.0\n  ```\n\n  \n\n- 创建一个`basic.flow`的文件,写入内容:\n\n  ```yml\n  nodes:\n    - name: jobA\n      type: command\n      config:\n        command: echo \"This is an echoed text.\"\n  ```\n\n- 把两个文件打个zip包\n\n![image-20200220190504445](/images/image/image-20200220190504445.png)\n\n\n\n- 在WebUI中创建工程\n\n![image-20200220190731303](/images/image/image-20200220190731303.png)\n\n- 上传打包好的附件\n\n![image-20200220190849991](/images/image/image-20200220190849991.png)\n\n这样project就有任务了\n\n![image-20200220190959064](/images/image/image-20200220190959064.png)\n\n点击运行后就有任务执行历史了\n\n![image-20200220191106770](/images/image/image-20200220191106770.png)\n\n### 依赖型任务\n\n依赖型可以使用dependsOn来进行关联\n\n```yml\nnodes:\n  - name: jobC\n    type: noop\n    # jobC depends on jobA and jobB\n    dependsOn:\n      - jobA\n      - jobB\n\n  - name: jobA\n    type: command\n    config:\n      command: echo \"This is an echoed text.\"\n\n  - name: jobB\n    type: command\n    config:\n      command: pwd\n  - name: jobD\n    type: command\n    config:\n      command: echo 'I am D job'\n  - name: jobE\n    type: command\n    dependsOn:\n      - jobC\n      - jobD\n    config:\n      command: echo 'I am E'\n```\n\n这样就配置三个任务,其中A,B,D任务互不干扰,同时运行,jobC必须要等任务A,B执行完了才执行,E任务需要任务C,D都执行完才执行,依赖关系\n\n![image-20200220192931508](/images/image/image-20200220192931508.png)\n\n> 注意:\n>\n> - 文件和yml文件格式类似,但是千万不要用tab,这个不识别\\t\n> - 不同的project,需要多次创建project,同一个project,后一次上传的会覆盖掉前一次上传的\n> - .flow文件的文件名就是project中显示的flow的name\n\n点击job进去,可以对参数进行设置\n\n![image-20200220193646623](/images/image/image-20200220193646623.png)\n\n### 定时调度\n\n点击Schedule可以设置定时任务的周期,![image-20200220194133237](/images/image/image-20200220194133237.png)\n\n![image-20200220194308982](/images/image/image-20200220194308982.png)\n\n这样就可在Scheduling下看到定时任务列表,等待下一个时钟来临就会执行\n\n![image-20200220194504136](/images/image/image-20200220194504136.png)\n\n如果需要移除,可以RemoveSchedule","tags":["Azkaban"],"categories":["Azkaban"]},{"title":"Hive的一些坑","url":"/2019/04/01/hive的一些坑/","content":"\n# Hive的一些坑\n\n1. specified datastore driver(\"com.mysql.jdbc.Driver\") was not found\n\n![image-20200126114217615](/images/image/image-20200126114217615.png)\n\n这个是因为驱动不对,下载了个新的就行了\n\n2. Unable to open a test connection to the given database. JDBC url = jdbc:mysql://hadoop001:3306/test?useSSL=true&serverTimezone=GMT%2B8, username = lrj. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception:\n\n![image-20200126114500594](/images/image/image-20200126114500594.png)\n\n这个需要把ssl禁用了,在jdbcUrl上指定useSSL=false\n\n3. MetaException(message:Version information not found in metastore. )\n\n![image-20200126114708126](/images/image/image-20200126114708126.png)\n\n这个需要将hive-site.xml中的hive.metastore.schema.verification设置为false\n\n4. Required table missing : \"`VERSION`\" in Catalog \"\" Schema \"\". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable \"datanucleus.autoCreateTables\"\n\n![image-20200126114930654](/images/image/image-20200126114930654.png)\n\n这个需要初始化一下schema,执行\n\nschematool -dbType mysql -initSchema\n\n----\n\n之后就可以启动metastore + hiveserver2服务\n\n```bash\nnohup hive --service  metastore > ~/metastore.log 2>&1 &\nnohup  hiveserver2  > ~/hiveserver2.log 2>&1 &\n```\n\n测试hiveserver2服务是否ok\n\n```bash\nbeeline\n```\n\n打印日志\n\n```\nwhich: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/mysql/bin:/home/lurongjiang/.local/bin:/home/lurongjiang/bin:/usr/software/hadoop-2.6.0-cdh5.16.2/bin:/usr/software/hadoop-2.6.0-cdh5.16.2/sbin:/usr/software/jdk1.8.0_231/bin:/usr/software/apache-maven-3.6.3/bin:/usr/software/scala-2.11.12/bin:/usr/software/hive-1.1.0-cdh5.16.2/bin)\nBeeline version 1.1.0-cdh5.16.2 by Apache Hive\n# 查看下数据库,此时发现没连接\nbeeline> show databases;\nNo current connection\n# 尝试连接数据库,只需要输入用户名就行,不需要密码\nbeeline> !connect jdbc:hive2://hadoop001:10000/default\nConnecting to jdbc:hive2://hadoop001:10000/default\nEnter username for jdbc:hive2://hadoop001:10000/default: lrj\nEnter password for jdbc:hive2://hadoop001:10000/default: \nError: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=lrj, access=EXECUTE, inode=\"/tmp\":lurongjiang:supergroup:drwx\n```\n\n5. java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=lrj, access=EXECUTE, inode=\"/tmp\":lurongjiang:supergroup:drwx------\n\n![image-20200126115801228](/images/image/image-20200126115801228.png)\n\n 这个是没权限\n\nhadoop fs -chmod -R 777  /tmp\n\n再次启动就ok了.\n\n","tags":["Hive"],"categories":["Hive"]},{"title":"MapReduce教程","url":"/2019/04/01/MapReduce Tutorial/","content":"\n\n# MapReduce Tutorial\n\n## Overview\n\n- Hadoop MapReduce是一个运行在集群上,并行处理大量数据(TB级别)的框架\n- MapReduce任务通常讲输入切分成多个独立的块,这些数据块被独立的map任务并行的处理\n- 该框架会对map输出进行排序,作为reduce任务的输入\n- 该框架负责调度任务,监视任务并重新执行失败的任务\n- 通常,计算的节点和数据存储节点是同一个节点,也就是说,MapReduce框架和HDFS都运行在同一些列节点中.这个约束使得框架在数据已经存在的节点上有效地调度任务,从而产生跨集群的非常高的聚合带宽\n- MapReduce框架由一个ResourceManager,集群每个节点的NodeManager和每个应用程序的MRAppMaster组成\n- 必须指定输入输出路径,实现指定的接口或者抽象类,覆写map和reduce方法\n- hadoop任务客户端提交任务和相关配置到ResouceManager,ResouceManager负责把任务/配置分发到其他的从节点,并调度和监控任务,给客户端提供任务的状态和诊断信息\n- hadoop stream允许用户使用任何可执行的程序来作为mapper/reducer任务\n- hadoop pipes工具可以使用C++ API来实现mapper/reducer\n\n## Inputs and Outputs\n\n- MapReduce框架只针对<Key,Value>键值对类型操作.也就是说,每个MapReduce任务的输入是<Key,Value>形式,输入也是<Key,Value>形式,输入输出类型可不相同\n- Key,Value的类型必须是可以被框架序列化的类型,因此他们必须实现Writable接口.\n- Key的类型除了实现Writable接口之外,还需要实现WritableComparable接口,这样才能被排序\n- (input) ` <k1,v1> ->` **map** `-> <k2,v2> ->` **combine** `->  <k2,v2>  ->` **reduce** `-> <k3,v3>  `  (output)\n\n**hadoop jar的一些参数**\n\n- -files 可以使用逗号分隔,指定多个文件\n- -libjars 可以添加jar包到map和reduce类路径下\n- -archives 可以使用逗号分隔传入多个压缩包路径\n\n## MapReduce - User Interfaces\n\n- 实现Mapper和Reducer接口吗,并提供map/reduce的实现是任务的核心\n\n#### Mapper\n\n- Mapper将输入的,Key/Value键值对类型映射成中间结果的Key/Value键值对类型\n- Maps是独立的任务,负责将输入转成中间结果\n- 中间结果的类型无需和输入的类型一样\n- 一个输入可能对应0,1,或者多个输出\n- 每个InputSplit(由InputFormat产生)都有一个map任务\n- 可以通过[Job.setMapperClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html) 来传入Mapper的实现.框架将对每个键值对形式的InputSplit调用[map(WritableComparable, Writable, Context)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Mapper.html) 方法.如果需要清理一些必要资源,可以覆写`cleanup(Context)`方法\n- map的输出可以通过调用context.write(WritableComparable, Writable)来收集\n- 所有的中间结果会被框架分组,然后传给Reducer.用户使用 [Job.setGroupingComparatorClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)指定比较器Comparator来控制分组\n- Mapper的输出会被排序(sort)和打散(partitioner)分发给每一个Reducer.partitioner数目和reduce任务的数量相同.用户可以实现Partitioner接口来自定义打散规则,控制不同的Key分到对应的reduce任务中\n- 用户可以使用[Job.setCombinerClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)对中间输出结果进行本地聚合,这可以减少从Mapper传到Reduce的传输量\n- 中间结果都是以简单的 (key-len, key, value-len, value) 形式存储,也可通过Configuration设置对中间结果进行压缩\n\n##### How Many Maps?\n\n- map任务的通常是由输入数据的大小来决定的,也就是输入文件的块数\n- 对于cpu轻量级任务来说,每个节点map的并行度可达300,但是一般情况下并行度在10-100之间.任务的启动需要一定的时间,所以map任务至少需要1min的执行时间\n\n#### Reducer\n\n- Reducer将相同key的中间结果集进行处理\n- reduce任务的个数是通过[Job.setNumReduceTasks(int)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)来设置的\n- 通过 [Job.setReducerClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)来设置Reducer的实现类.框架对每组<key, (list of values)>的输入进行调用[reduce(WritableComparable, Iterable, Context)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Reducer.html) 方法进行处理,需要清理资源可以覆写cleanup(Context)\n\n##### Shuffle\n\n- 传到Reducer的输入是经过排序后的mapper的输出.shuffle阶段,框架将通过http获取相关partition的mapper输出\n\n##### Sort\n\n- 排序阶段,框架将Reducer的输入进行按Key进行分组\n- shuffle和sort同时进行.在map输出被拉取时,他们进行合并\n\n##### Secondary Sort\n\n- 如果中间结果key的分组规则需要和进入reducer前的keys的分组规则不一样,那么可以通过[Job.setSortComparatorClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)来设置比较器.因为[Job.setSortComparatorClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)时用来控制中间结果的keys是怎么分组的,所以可以用这个来对值进行二次排序\n\n##### Reduce\n\n- reduce阶段,将对每一组<key, (list of values)>输入调用reduce(WritableComparable, Iterable\\<Writable>, Context)方法\n- reduce任务通过 Context.write(WritableComparable, Writable)将输出结果写入文件系统\n- 输出结果并不会进行排序\n\n##### How Many Reduces?\n\n- 比较合理的reduce任务的个数计算公式是:0.95(或1.75)×节点数(注意,不是每个节点的最大container数)\n- 0.95系数可以使得reduce任务在map任务的输出传输结束后同时开始运行\n- 1.75系数可以使得计算快的节点在一批reduce任务计算结束之后开始计算第二批 reduce任务,实现负载均衡\n- 增加reduce的数量虽然会增加负载，但是可以改善负载匀衡，降低任务失败带来的负面影响\n- 放缩系数要比整数略小是因为要给推测性任务和失败任务预留reduce位置\n\n##### Reducer NONE\n\n- 如果不需要reduce任务,将reduce任务个数设置为0是合法的\n- 这种情况下,map任务的输出会直接写入文件系统的指定输出路径[FileOutputFormat.setOutputPath(Job, Path)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html).在写入文件系统前,map的输出是进行排序的\n\n#### Partitioner\n\n- partitioner控制中间map输出的key的分区\n- 可以按照key(或者key的一部分)来产生分区,默认是使用hash进行分区\n- 分区数和reduce任务的个数相等\n- 控制发送给reduce的任务个数\n\n#### Counter\n\n- Counter是一个公共基础工具,用来报告MapReduce应用的统计信息\n- Mapper和Reducer实现类都可以使用Counter来报告统计\n\n### Job Configuration\n\n- Job就是MapReduce任务的job配置代表\n- 一般MapReduce框架会严格按照Job的配置执行,但是有几种情况例外\n  - 某些配置参数被标记为final类型,所以是修改配置是没法达到目的的,例如1.1比例\n  - 某些配置虽然可以直接配置,但是还需要配合其他的参数一起配置才能生效\n- Job通常会指定Mapper,combiner(有必要的话),Partitioner,Reducer,InputFormat,OutputFormat的实现类\n- 输入可以使用下列方式指定输入数据文件集\n  - ([FileInputFormat.setInputPaths(Job, Path…)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html)/ [FileInputFormat.addInputPath(Job, Path)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html))\n  - ([FileInputFormat.setInputPaths(Job, String…)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html)/ [FileInputFormat.addInputPaths(Job, String)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html)\n\n- 输出可以使用([FileOutputFormat.setOutputPath(Path)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html))来指定输出文件集\n- 其他配置都是可选的,如Caparator的使用,将文件放置到DistributeCache,是否中间结果或者最终输出结果需要压缩,是否允许推测模式,最大任务重试次数等\n- 可以通过[Configuration.set(String, String)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/conf/Configuration.html)/ [Configuration.get(String)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/conf/Configuration.html)来设置和获取任意需要的参数.但是对于大的只读数据集,还是要用DistributedCache\n\n### Task Execution & Environment\n\n- MRAppMaster在独立的JVM中执行每个Mapper/Reducer任务(任务进程级别)\n- 子任务继承了MRAppMaster的环境.\n- 用户可以通过 `mapreduce.{map|reduce}.java.opts` 给子任务添加额外的参数\n- 运行时非标准类库路径可以通过-Djava.library.path=<>指定\n- 如果mapreduce.{map|reduce}.java.opts参数配置包含了*@taskid@*则在运行时被替换成taskId\n- 显示JVM GC,JVM JMX无密代理(这样可以结合jconsole,查看内存,线程,线程垃圾回收),最大堆内存,添加其他路径到任务java.library.path的例子\n\n```xml\n<property>\n  <name>mapreduce.map.java.opts</name>\n  <value>\n  -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc\n  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false\n  </value>\n</property>\n\n<property>\n  <name>mapreduce.reduce.java.opts</name>\n  <value>\n  -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc\n  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false\n  </value>\n</property>\n```\n\n#### Memory Management\n\n- 用户可以通过`mapreduce.{map|reduce}.memory.mb`指定子任务的最大虚拟内存.注意这个设置是进程级别的\n- 注意这个参数不要大于-Xmx的参数,否则VM可能会无法启动\n- `mapreduce.{map|reduce}.java.opt`只能配置MRAppMaster的子任务.配置守护线程的需要参考 [Configuring the Environment of the Hadoop Daemons](https://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-common/ClusterSetup.html#Configuring_Environment_of_Hadoop_Daemons)\n\n- map/reduce任务的性能,可能会被并发数,写入磁盘的频率影响.检查文件系统的统计报告,尤其是从map进入reduce的字节数,这参数是非常宝贵的.\n\n#### Map Parameters\n\n- map输出的记录会被序列化到缓冲区,元数据存储在统计缓冲区\n- 当缓冲区或者元数据超过一定的阈值,缓冲区的内容会被排序然后存储和写入磁盘\n- 如果缓冲区一直是满状态的,map线程将被阻塞\n- map结束后,没有写入磁盘的map输出记录继续写入.\n- 磁盘上所有的map输出文件段会合并成单个文件\n- 减少写入磁盘的次数,可以减少map的次数,但是加大缓存区会压缩mapper的可用内存\n\n| Name                             | Type  | Description                                            |\n| :------------------------------- | :---- | :----------------------------------------------------- |\n| mapreduce.task.io.sort.mb        | int   | 序列化和map输出到缓冲区的记录预排序的累计大小,单位为MB |\n| mapreduce.map.sort.spill.percent | float | 序列化缓冲区spill阈值比例,超过会将缓冲区内容写入磁盘   |\n\n- spill之后,如果在写入磁盘过程中,map的输出没有超过spill阈值,则会继续收集到spill结束\n- 如果是spill设置为0.33,在spill到磁盘的过程,缓冲区继续会被map的输出填充,下一次spill的时候再将这期间填充的内容写到磁盘\n- 如果spill设置为0.66,则不会触发下一次spill.也就是说,spill可以触发,但是不会阻塞\n- 一条记录大于缓冲区的会先触发spill,而且会被spill到一个单独的文件.无论这条记录有没有定义combiner,它都会被combiner传输\n\n#### Shuffle/Reduce Parameters\n\n- reduce将partitioner通过http指派给自己的map输出加载到内存,并定期合并输出到磁盘.\n- 如果中间结果是压缩输出,那么输出也是被reduce压缩的读进内存中,减少了内存的压力\n\n| Name                                          | Type  | Description                                                  |\n| :-------------------------------------------- | :---- | :----------------------------------------------------------- |\n| mapreduce.task.io.soft.factor                 | int   | 每次合并磁盘上段的数目.如果超过这个设置会分多次进行合并      |\n| mapreduce.reduce.merge.inmem.thresholds       | int   | 在合并写入磁盘之前,将排序后的map输出加载到内存的map输出数目.这个值通常设置很大(1000)或者直接禁用(0),因为内存合并要比磁盘合并的代价小得多.这个阈值只影响shuffle期间内存中合并的频率 |\n| mapreduce.reduce.shuffle.merge.percent        | float | 在内存合并之前,读取map输出的内存阈值,代表着用于存储map输出在内存中的百分比.因为map的输出并不适合存储在内存,所以设置很高会知道使得获取和合并的并行度下降.相反,设置为1可以使得内存运行的reduce更快.这个参数只影响shuffle期间的内存内合并频率 |\n| mapreduce.reduce.shuffle.input.buffer.percent | float | 在shuffle期间,可以分配来存储map输出的内存百分比,相对于`mapreduce.reduce.java.opts`指定的最大堆内存.把这个值设的大一点可以存储更多的map输出,但是也应该为框架预留一些内存 |\n| mapreduce.reduce.input.buffer.percent         | float | 相当于reduce阶段,用于存储map输出的最大堆内存的内存百分比.reduce开始的时候,map的输出被合并到磁盘,知道map输出在一定的阈值之内.默认情况下,在reduce开始之前,map的输出都会被合并到磁盘,这样才能使得reduce充分的利用到内存.对于只要内存密集型的reduce任务,应该增加这个值,减少磁盘的的往返时间 |\n\n#### Configured Parameters\n\n这些参数都是局部的,每个任务的\n\n| Name                       | Type    | Description                                    |\n| :------------------------- | :------ | :--------------------------------------------- |\n| mapreduce.job.id           | String  | The job id                                     |\n| mapreduce.job.jar          | String  | job.jar location in job directory              |\n| mapreduce.job.local.dir    | String  | The job specific shared scratch space          |\n| mapreduce.task.id          | String  | The task id                                    |\n| mapreduce.task.attempt.id  | String  | The task attempt id                            |\n| mapreduce.task.is.map      | boolean | Is this a map task                             |\n| mapreduce.task.partition   | int     | The id of the task within the job              |\n| mapreduce.map.input.file   | String  | The filename that the map is reading from      |\n| mapreduce.map.input.start  | long    | The offset of the start of the map input split |\n| mapreduce.map.input.length | long    | The number of bytes in the map input split     |\n| mapreduce.task.output.dir  | String  | The task’s temporary output directory          |\n\n在流任务执行过程中,这些参数会被转化.点(.)会被转成下划线(_),所以要想在流任务的mapper/reducer中获得这些值,需要使用下划线形式.\n\n#### Distributing Libraries\n\n- [DistributedCache](https://hadoop.apache.org/docs/r2.8.5/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#DistributedCache)分布式缓存可以分发jars和本地类库给map/reduce任务使用.\n- child-jvm总将自己的工作目录添加到java.library.path和LD_LIBRARY_PATH\n- 缓存中的类库可以通过System.loadLibrary或者System.load\n\n### Job Submission and Monitoring\n\n- Job是用户任务和ResourceManager交互的主要接口\n- Job的提交流程包括\n  - 检查输入输出路径\n  - 计算任务的InputSplit\n  - 有必要的话,设置必要的分布式缓存\n  - 拷贝任务的jar和配置到MapReduce系统目录\n  - 提交任务到ResourceManager.监控任务状态是可选的\n  - 任务的执行记录历史存放在 `mapreduce.jobhistory.intermediate-done-dir` 和`mapreduce.jobhistory.done-dir`\n\n#### Job Control\n\n- 对于单个MapReduce任务无法完成的任务,用户可能需要执行MapReduce任务链,才能完成.这还是非常容易的,因为任务的输出一般是存储在分布式文件系统中,所以一个任务的输出可以作为另一个任务的输入.这也就使得判断任务是否完成,不管成功或者失败,都需要用户来控制.主要有两种控制手段\n  - Job.submit() 提交任务到集群中,立即返回\n  - Job.waitForCompletion(boolean) 提交任务到集群中,等待其完成\n\n### Job Input\n\n- InputFormat描述了MapReduce任务的输入规范\n- InputFormat的职责是:\n  - 校验输入是否合法\n  - 将输入逻辑切分成InputSplit实例,之后将它们发送到独立的Mapper\n  - RecordReader 实现了从符合框架逻辑的InputSplit实例收集输入的记录,提供给Mapper进行处理\n\n- 默认的InputFormat是基于输入文件的总字节大小,将输入文件切分成逻辑的InputSplit实例,例如FileInputFormat的子类.然而,文件系统的blocksize只是split的上限,下限需要通过`mapreduce.input.fileinputformat.split.minsize`来设置\n- 压缩文件并不一定可以被切分,如.gz文件会把完整的文件交给一个mapper来处理\n\n#### InputSplit\n\n- InputSplit代表了一个独立Mapper处理的输入数据\n- 通常InputSplit是面向字节的,把面向字节转为面向记录是RecordReader的职责\n\n- FileSplit是默认的InputSplit实现,它把输入设置成mapreduce.map.input.file 属性,用于进行逻辑分割\n\n#### RecordReader\n\n- RecordReader负责将InputSplit的面向字节的输入转换成面向记录,提供给Mapper实现去处理每一条记录.因此RecordReader承担了从记录中提取出键值对的任务\n\n### Job Output\n\n- OutputFormat描述了MapReduce输出的规范\n- OutputFormat的职责:\n  - 校验任务的输出,例如输出目录是否存在\n  - RecordWriter实现可以将任务的输出写入到文件,存储在文件系统中\n- TextOutputFormat是默认的OutputFormat实现\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Hadoop,MapReduce"],"categories":["Hadoop","MapReduce"]},{"title":"Hive UDF","url":"/2019/03/15/Hive UDF/","content":"\n# Hive UDF\n\nhive内置函数并不一定满足我们的业务要求,所以需要拓展,即用户自定义函数\n\n**UDF**\n\nUser Defined Function\n\n- UDF (one-to-one)\n- UDAF(many-to-one)\n- UDTF(one-to-many)\n\n## 创建UDF步骤\n\n- 添加依赖\n\n  ```xml\n      <dependency>\n        <groupId>org.apache.hive</groupId>\n        <artifactId>hive-exec</artifactId>\n        <version>${hive.cdh.version}</version>\n      </dependency>\n  ```\n\n  \n\n- 创建自定义类,继承UDF","tags":["hive","udf","user-defined-function"],"categories":["Hive"]},{"title":"Hadoop MapReduce编程核心","url":"/2019/03/12/Hadoop MapReduce编程核心/","content":"\n# Hadoop MapReduce编程核心\n\n## Partitioner 分区\n\n```java\n/** \n * Partitions the key space.\n * \n * <p><code>Partitioner</code> controls the partitioning of the keys of the \n * intermediate map-outputs. The key (or a subset of the key) is used to derive\n * the partition, typically by a hash function. The total number of partitions\n * is the same as the number of reduce tasks for the job. Hence this controls\n * which of the <code>m</code> reduce tasks the intermediate key (and hence the \n * record) is sent for reduction.</p>\n * partitioner是控制中间map阶段输出结果的key的分区.key通常被hash,分发到各个分区\n * 分区数一般和reduce job的个数相等,\n * @see Reducer\n */\n@InterfaceAudience.Public\n@InterfaceStability.Stable\npublic interface Partitioner<K2, V2> extends JobConfigurable {\n  \n  /** \n   * Get the paritition number for a given key (hence record) given the total \n   * number of partitions i.e. number of reduce-tasks for the job.\n   * \n   * <p>Typically a hash function on a all or a subset of the key.</p>\n   * 根据分区总数,例如reduce job个数,获取分区的编号.一般是对所有key或者key的一部分进行进行hash处理\n   * @param key the key to be paritioned.\n   * @param value the entry value.\n   * @param numPartitions the total number of partitions.\n   * @return the partition number for the <code>key</code>.\n   */\n  int getPartition(K2 key, V2 value, int numPartitions);\n}\n/**\n* hash分区的实现就是key取hashCode和reduce个数进行取模\n*/\npublic class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n\n  public void configure(JobConf job) {}\n\n  /** Use {@link Object#hashCode()} to partition. */\n  public int getPartition(K2 key, V2 value,\n                          int numReduceTasks) {\n    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n  }\n\n}\n```\n\n\n\n>  **需要注意的是**\n>\n> - 分区数一般和reduce job个数相等\n> - 如果分区数<reduce job个数,将导致输出有很多无用的空文件\n> - 如果分区数>reduce job个数,将导致有些map输出找不到hash路径,出现java.io.IOException: Illegal partition for xxx的异常\n\n## Combiner 局部汇总\n\nCombiner是hadoop对map阶段输出结果进行本地局部聚合,提高后面reduce的效率,避免大量数据进行网络传输.\n\n> **需要注意的是**\n>\n> - 并非所有的任务都适用于Combiner\n> - 求和等操作,局部聚合可以有效的提高后面reduce的效率\n> - 平均值等操作,这种并不适用,因为局部平均值和全局平均值还是有差异的","tags":["hadoop","MapReduce"],"categories":["Hadoop"]},{"title":"Hello World","url":"/2018/07/18/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","tags":["PlayStation","Games"],"categories":["TestNest","test1","nest1","nest2"]}]