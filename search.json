[{"title":"SpringCloud概述","url":"/2019/04/05/SpringCloud/","content":"# SpringCloud\n\n## SpringCloud概述\n\n### 官网\n\n- [官网](https://spring.io/projects/spring-cloud)\n\n### 主要功能\n![img](/images/image/image-20200207192330288.png)\n\n\n\n### 常用子项目\n\n![img](/images/image/image-20200207192553245.png)\n\n### 版本与兼容\n\n- SpringCloud的版本命名\n  - **版本命名**\n\n    SpringCloud的版本,前半部分(如Hoxton,Greenwich),意思是发布列车(ReleaseTrain),以伦敦地铁的站名命名,因为SpringCloud有很多的子项目,每个项目都有自己的版本管理,按照发布顺序以A,B,C等为首字母依次命名,已经发布的版本顺序为:\n\n  `Angel -> Brixton -> Camden -> Dalston -> Edgware -> Finchley -> Greenwich -> Hoxton`\n  ![img](/images/image/londontuberail-1.png)\n\n\n\n  后半部分(如SR,SR1,SR2),意思是服务发布(ServiceRelease),即重大Bug修复\n  - **版本发布流程**\n\n  `SNAPSHOT -> Mx -> RELEASE -> SRx`,其中x就是一些数字序号,例如M1,M2,SR1,SR2.SNAPSHOT为快照版本(开发版本),Mx为里程碑版本,此时并不是正式版本,但是已经接近正式版,经过多个版本迭代之后,发布第一个RELEASE版本,正式版本;在RELEASE版本之后如果有重大bug修复就会发布SR版本\n\n| Hoxton SR1 **CURRENT** **GA** | [ Reference Doc.](https://cloud.spring.io/spring-cloud-static/Hoxton.SR1/reference/html/spring-cloud.html) |\n| ----------------------------- | ------------------------------------------------------------ |\n| Hoxton **SNAPSHOT**           | [ Reference Doc.](https://spring.io/projects/spring-cloud)   |\n| Greenwich SR5 **GA**          | [ Reference Doc.](https://cloud.spring.io/spring-cloud-static/Greenwich.SR5/) |\n| Greenwich **SNAPSHOT**        | [ Reference Doc.](https://spring.io/projects/spring-cloud)   |\n\n----\n\n- SpringCloud的版本生命周期\n  - **版本发布规划**\n\n    https://github.com/spring-cloud/spring-cloud-release/milestones\n\n  - **版本发布记录**\n\n    https://github.com/spring-cloud/spring-cloud-release/releases\n\n  - **版本终止声明**\n\n    https://spring.io/projects/spring-cloud#overview\n\n- SpringBoot与SpringCloud的兼容性\n  - 版本兼容性非常重要\n    https://spring.io/projects/spring-cloud#overview\n\n| Release Train | Boot Version |\n| ------------- | ------------ |\n| Hoxton        | 2.2.x        |\n| Greenwich     | 2.1.x        |\n| Finchley      | 2.0.x        |\n| Edgware       | 1.5.x        |\n| Dalston       | 1.5.x        |\n\n- 生产环境如何选择选择\n  - 坚决不适用非稳定版本\n  - 坚决不适用end-of-life版本\n  - 尽量使用最新版本\n    - RELEASE版本可以观望/调研,因为是第一个正式版,并没有在生产上得以广泛应用\n    - SR2之后可以大规模使用\n\n### 版本选择\n\n- SpringCloud Hoxton SR1\n- SpringBoot 2.2.4.RELEASE\n\n```xml\n<parent>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>2.2.4.RELEASE</version>\n    <relativePath/> <!-- lookup parent from repository -->\n</parent> \n\n<dependencyManagement>\n    <dependencies>\n        <!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-dependencies -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-dependencies</artifactId>\n            <version>Hoxton.SR1</version>\n            <type>pom</type>\n            <scope>import</scope>\n        </dependency>\n    </dependencies>\n</dependencyManagement>\n```\n\n检查项目是否能运行\n\n```bash\nmvn clean install -U\n```\n\n## SpringCloud服务注册与发现\n\n- 使得服务消费者总能找到服务提供者\n\n### Consul单机版安装\n\n#### Consul下载\n\n- 下载Consoule https://releases.hashicorp.com/consul/1.6.3/consul_1.6.3_linux_amd64.zip\n\n#### 需要的端口\n\n| Use                                                          | Default Ports    |\n| :----------------------------------------------------------- | :--------------- |\n| DNS: The DNS server (TCP and UDP)                            | 8600             |\n| HTTP: The HTTP API (TCP Only)                                | 8500             |\n| HTTPS: The HTTPs API                                         | disabled (8501)* |\n| gRPC: The gRPC API                                           | disabled (8502)* |\n| LAN Serf: The Serf LAN port (TCP and UDP)                    | 8301             |\n| Wan Serf: The Serf WAN port TCP and UDP)                     | 8302             |\n| server: Server RPC address (TCP Only)                        | 8300             |\n| Sidecar Proxy Min: Inclusive min port number to use for automatically assigned sidecar service registrations. | 21000            |\n| Sidecar Proxy Max: Inclusive max port number to use for automatically assigned sidecar service registrations. | 21255            |\n\n检查端口是否被占用的方法\n\n```shell\nWindows:\n# 如果没有结果说明没有被占用\nnetstat -ano| findstr \"8500\"\n\nLinux:\n# 如果没有结果说明没有被占用\nnetstat -antp |grep 8500\n\nmacOS:\n# 如果没有结果说明没有被占用\nnetstat -ant | grep 8500\n或\nlsof -i:8500\n```\n\n#### 安装和启动\n\n- 解压\n\n```shell\n./consul agent -dev -client 0.0.0.0\n```\n\n- 严重是否成功\n\n```shell\n./consul -v\n```\n\n- 访问Consul首页`localhost:8500`\n\n**启动参数**\n\n- -ui 开启ui\n- -client 让consul拥有client功能,接受服务注册;0.0.0.0允许任意ip注册,不写只能使用localhost连接\n- -dev 以开发模式运行consul\n\n### 整合Consul\n\n- 添加依赖\n\n  ```xml\n  <dependency>\n      <groupId>org.springframework.cloud</groupId>\n      <artifactId>spring-cloud-starter-consul-discovery</artifactId>\n  </dependency>\n  ```\n\n  \n\n- 配置\n\n  ```yml\n  spring:\n   application:\n      # 指定注册到consul的服务名称,分隔符不能是下划线\n      # 如果服务发现组件是Consul,会强制转换成中划线,导致找不到服务\n      # 如果服务发现组件是Ribbon,则因为Ribbon的问题(把默认名称当初虚拟主机名,而虚拟主机名不能用下划线),会造成微服务之间无法调用\n      name: micro-service-user\n  \n    cloud:\n      consul:\n        host: 192.168.238.128\n        port: 8500\n  ```\n\n- 启动,检查consul ui的服务上线情况\n\n### Consul健康检查\n![img](/images/image/image-20200207231837317.png)\n\n\n- 添加健康检查依赖\n\n```xml\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n\n- 配置\n\n  ```yml\n  management:\n    endpoints:\n      web:\n        exposure:\n          include: '*'\n  ```\n\n  \n\n- 端点\n\n  http://localhost:8080/actuator 查看端点\n\n  http://localhost:8080/actuator/health 健康检查\n\n  添加详情配置,可以检查详细的健康情况\n\n  ```yml\n  management:\n      endpoint:\n        health:\n          show-details: always\n  ```\n  \n  ![img](/images/image/image-20200207233909996.png)\n\n\n\n\n- 简单研究一下健康检查的源码\n\n  以磁盘检查的为例\n\n  健康检查的类都继承了`AbstractHealthIndicator`抽象类,而`AbstractHealthIndicator`实现了`HealthIndicator`接口,所有健康检查实现类都必须实现`doHealthCheck(Health.Builder builder)`方法\n\n![img](/images/image/image-20200207235333557.png)\n\n\n```java\npublic class DiskSpaceHealthIndicator extends AbstractHealthIndicator {\n\n\tprivate static final Log logger = LogFactory.getLog(DiskSpaceHealthIndicator.class);\n\n\tprivate final File path;\n\n\tprivate final DataSize threshold;\n\n\t/**\n\t * Create a new {@code DiskSpaceHealthIndicator} instance.\n\t * @param path the Path used to compute the available disk space\n\t * @param threshold the minimum disk space that should be available\n\t */\n\tpublic DiskSpaceHealthIndicator(File path, DataSize threshold) {\n\t\tsuper(\"DiskSpace health check failed\");\n\t\tthis.path = path;\n\t\tthis.threshold = threshold;\n\t}\n\n\t@Override\n\tprotected void doHealthCheck(Health.Builder builder) throws Exception {\n        //获取可用的空间字节数\n\t\tlong diskFreeInBytes = this.path.getUsableSpace();\n        //如果可用的字节数大于预留字节数阈值则认为是健康的,设置status为UP\n\t\tif (diskFreeInBytes >= this.threshold.toBytes()) {\n\t\t\tbuilder.up();\n\t\t}\n\t\telse {\n            //否则任务是不健康的,设置status为DOWN\n\t\t\tlogger.warn(LogMessage.format(\"Free disk space below threshold. Available: %d bytes (threshold: %s)\",\n\t\t\t\t\tdiskFreeInBytes, this.threshold));\n\t\t\tbuilder.down();\n\t\t}\n        //输出总空间,可用空间和预留阈值\n\t\tbuilder.withDetail(\"total\", this.path.getTotalSpace()).withDetail(\"free\", diskFreeInBytes)\n\t\t\t\t.withDetail(\"threshold\", this.threshold.toBytes());\n\t}\n\n}\n\n//这个获取可用字节数还是挺好的,直接利用了File提供的方法\npublic long getUsableSpace() {\n    SecurityManager sm = System.getSecurityManager();\n    if (sm != null) {\n        sm.checkPermission(new RuntimePermission(\"getFileSystemAttributes\"));\n        sm.checkRead(path);\n    }\n    if (isInvalid()) {\n        return 0L;\n    }\n    //fs是默认的文件系统FileSystem fs = DefaultFileSystem.getFileSystem();\n    return fs.getSpace(this, FileSystem.SPACE_USABLE);\n}\n\n\n```\n\n健康检查使用了建造者模式,对于不同的健康指标非常方便,值得学习\n![img](/images/image/image-20200208000623044.png)\n\n\n- 整合Consul和SpringCloud的actuator\n\n  修改配置\n\n  ```yml\n  spring:\n      cloud:\n        consul:\n          host: 192.168.238.128\n          port: 8500\n          discovery:\n            health-check-path: /actuator/health\n  ```\n\n这样启动之后,再检查consul ui就可以发现没有红色的叉了\n![img](/images/image/image-20200208001532048.png)\n\n\n其他的健康检查配置\n![img](/images/image/image-20200208001745953.png)\n\n\n### 注册课程微服务到Consul\n\n- 添加依赖\n\n```xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-consul-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n\n- 配置\n\n```yml\nspring:\n  datasource:\n    url: jdbc:mysql://192.168.238.128:3306/ms?serverTimezone=GMT%2B8&characterEncoding=utf8&useSSL=false\n    hikari:\n      username: lrj\n      password: lu11221015\n      driver-class-name: com.mysql.cj.jdbc.Driver\n# JPA配置\n  jpa:\n    hibernate:\n      ddl-auto: update\n    show-sql: true\n  application:\n    name: micro-service-class\n  cloud:\n    consul:\n      host: 192.168.238.128\n      port: 8500\n      discovery:\n        health-check-path: /actuator/health\n# 暴露所有的actuator端点\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: '*'\n  # 开启健康检查详细信息\n  endpoint:\n    health:\n      show-details: always\nserver:\n  port: 8081\n```\n\n- 重构用户微服务\n\n```java\n@RestController\n@RequestMapping(\"user\")\npublic class UserController {\n    @Resource\n    private UserService userService;\n\n    @Resource\n    private DiscoveryClient discoveryClient;\n\n    @GetMapping(\"{id}\")\n    public Object findUserById(@PathVariable(\"id\") Integer id) {\n        return userService.findUserById(id);\n    }\n\n    @GetMapping(\"discoveryTest\")\n    public Object discoveryTest() {\n        return discoveryClient.getInstances(\"micro-service-class\");\n    }\n}\n\n```\n\n- 访问端点,可以发现不需要指定课程微服务的主机和端口就可以拿到相关信息,实现了服务发现\n![img](/images/image/image-20200208003543107.png)\n\n\n### 重构课程微服务\n\n- 原来\n\n  ```java\n  @Service\n  public class LessonServiceImpl implements LessonService {\n      @Resource\n      private LessonRepository lessonRepository;\n      @Resource\n      private LessonUserRepository lessonUserRepository;\n      @Resource\n      private RestTemplate restTemplate;\n  \n  \n      @Override\n      public Lesson buyById(Integer id) {\n          // 1. 根据课程id查询课程\n          Lesson lesson = lessonRepository.findById(id).orElseThrow(() -> new IllegalArgumentException(\"该课程不存在\"));\n          //根据课程查询是否已经购买过\n          LessonUser lessonUser = lessonUserRepository.findByLessonId(id);\n          if (lessonUser != null) {\n              return lesson;\n          }\n          //todo 2.登录之后获取userId\n          String userId = \"1\";\n          // 3. 如果没有购买过,查询用户余额\n          UserDTO userDTO = restTemplate.getForObject(\"http://localhost:8080/user/{userId}\", UserDTO.class, userId);\n          if (userDTO != null && userDTO.getMoney() != null &&\n                  userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() < 0) {\n              throw new IllegalArgumentException(\"余额不足\");\n          }\n          //4. 购买逻辑\n          //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录\n          return lesson;\n      }\n  \n  }\n  ```\n\n  这个写死了主机地址,无法动态获取微服务路径\n\n- 重构\n\n  ```java\n  public class LessonServiceImpl implements LessonService {\n      @Resource\n      private LessonRepository lessonRepository;\n      @Resource\n      private LessonUserRepository lessonUserRepository;\n      @Resource\n      private DiscoveryClient discoveryClient;\n      @Resource\n      private RestTemplate restTemplate;\n  \n      @Override\n      public Lesson buyById(Integer id) {\n          // 1. 根据课程id查询课程\n          Lesson lesson = lessonRepository.findById(id).orElseThrow(() -> new IllegalArgumentException(\"该课程不存在\"));\n          //根据课程查询是否已经购买过\n          LessonUser lessonUser = lessonUserRepository.findByLessonId(id);\n          if (lessonUser != null) {\n              return lesson;\n          }\n          //todo 2.登录之后获取userId\n          String userId = \"1\";\n          List<ServiceInstance> instances = discoveryClient.getInstances(\"micro-service-user\");\n          if (instances != null && !instances.isEmpty()) {\n              //todo 需要改进,如果存在多个实例,需要考虑负载均衡\n              ServiceInstance instance = instances.get(0);\n              URI uri = instance.getUri();\n              UserDTO userDTO = restTemplate.getForObject(uri + \"/user/{userId}\", UserDTO.class, userId);\n              if (userDTO != null && userDTO.getMoney() != null &&\n                      userDTO.getMoney().subtract(lesson.getPrice()).doubleValue() < 0) {\n                  throw new IllegalArgumentException(\"余额不足\");\n              }\n              //4. 购买逻辑\n              //todo 4.1.调用微服务金额扣减接口 4.2.向lesson_user表插入记录\n              return lesson;\n          }\n          throw new IllegalArgumentException(\"用户微服务异常,无法购买课程\");\n      }\n  \n  }\n  \n  ```\n\n  可以动态的获取到用户微服务的地址,请求正常\n  ![img](/images/image/image-20200208004838563.png)\n\n\n### 元数据\n\nConsul是没有元数据的概念的,所以SpringCloud做了个适配,在consul下设置tags作为元数据.\n\n元数据可以对微服务添加描述,标识,例如机房在哪里,这样可以进行就近判断,或者当就近机房不可用时才检查远程机房,当两者都不可用时才认为服务不可用等实现容灾或者跨机房\n\n- 配置\n\n  ```yml\n  spring:\n    cloud:\n      consul:\n        host: 192.168.238.128\n        port: 8500\n        discovery:\n          health-check-path: /actuator/health\n          tags: JiFang=Beijing,JiFang=Shanghai\n  ```\n\n- 实现机房选择\n\n  ```java\n  @GetMapping(\"discoveryTest\")\n  public Object discoveryTest() {\n      List<ServiceInstance> instances = discoveryClient.getInstances(\"micro-service-class\");\n      if (instances != null) {\n          List<ServiceInstance> shanghaiInstances = instances.stream()\n                  .filter(s -> s.getMetadata().containsKey(\"Shanghai\")).collect(Collectors.toList());\n          if (!shanghaiInstances.isEmpty()) {\n              return shanghaiInstances;\n          }\n      }\n      return instances;\n  }\n  ```\n\n\n\n","tags":["SpringCloud"],"categories":["SpringCloud"]},{"title":"MapReduce教程","url":"/2019/04/01/MapReduce Tutorial/","content":"\n\n# MapReduce Tutorial\n\n## Overview\n\n- Hadoop MapReduce是一个运行在集群上,并行处理大量数据(TB级别)的框架\n- MapReduce任务通常讲输入切分成多个独立的块,这些数据块被独立的map任务并行的处理\n- 该框架会对map输出进行排序,作为reduce任务的输入\n- 该框架负责调度任务,监视任务并重新执行失败的任务\n- 通常,计算的节点和数据存储节点是同一个节点,也就是说,MapReduce框架和HDFS都运行在同一些列节点中.这个约束使得框架在数据已经存在的节点上有效地调度任务,从而产生跨集群的非常高的聚合带宽\n- MapReduce框架由一个ResourceManager,集群每个节点的NodeManager和每个应用程序的MRAppMaster组成\n- 必须指定输入输出路径,实现指定的接口或者抽象类,覆写map和reduce方法\n- hadoop任务客户端提交任务和相关配置到ResouceManager,ResouceManager负责把任务/配置分发到其他的从节点,并调度和监控任务,给客户端提供任务的状态和诊断信息\n- hadoop stream允许用户使用任何可执行的程序来作为mapper/reducer任务\n- hadoop pipes工具可以使用C++ API来实现mapper/reducer\n\n## Inputs and Outputs\n\n- MapReduce框架只针对<Key,Value>键值对类型操作.也就是说,每个MapReduce任务的输入是<Key,Value>形式,输入也是<Key,Value>形式,输入输出类型可不相同\n- Key,Value的类型必须是可以被框架序列化的类型,因此他们必须实现Writable接口.\n- Key的类型除了实现Writable接口之外,还需要实现WritableComparable接口,这样才能被排序\n- (input) ` <k1,v1> ->` **map** `-> <k2,v2> ->` **combine** `->  <k2,v2>  ->` **reduce** `-> <k3,v3>  `  (output)\n\n**hadoop jar的一些参数**\n\n- -files 可以使用逗号分隔,指定多个文件\n- -libjars 可以添加jar包到map和reduce类路径下\n- -archives 可以使用逗号分隔传入多个压缩包路径\n\n## MapReduce - User Interfaces\n\n- 实现Mapper和Reducer接口吗,并提供map/reduce的实现是任务的核心\n\n#### Mapper\n\n- Mapper将输入的,Key/Value键值对类型映射成中间结果的Key/Value键值对类型\n- Maps是独立的任务,负责将输入转成中间结果\n- 中间结果的类型无需和输入的类型一样\n- 一个输入可能对应0,1,或者多个输出\n- 每个InputSplit(由InputFormat产生)都有一个map任务\n- 可以通过[Job.setMapperClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html) 来传入Mapper的实现.框架将对每个键值对形式的InputSplit调用[map(WritableComparable, Writable, Context)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Mapper.html) 方法.如果需要清理一些必要资源,可以覆写`cleanup(Context)`方法\n- map的输出可以通过调用context.write(WritableComparable, Writable)来收集\n- 所有的中间结果会被框架分组,然后传给Reducer.用户使用 [Job.setGroupingComparatorClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)指定比较器Comparator来控制分组\n- Mapper的输出会被排序(sort)和打散(partitioner)分发给每一个Reducer.partitioner数目和reduce任务的数量相同.用户可以实现Partitioner接口来自定义打散规则,控制不同的Key分到对应的reduce任务中\n- 用户可以使用[Job.setCombinerClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)对中间输出结果进行本地聚合,这可以减少从Mapper传到Reduce的传输量\n- 中间结果都是以简单的 (key-len, key, value-len, value) 形式存储,也可通过Configuration设置对中间结果进行压缩\n\n##### How Many Maps?\n\n- map任务的通常是由输入数据的大小来决定的,也就是输入文件的块数\n- 对于cpu轻量级任务来说,每个节点map的并行度可达300,但是一般情况下并行度在10-100之间.任务的启动需要一定的时间,所以map任务至少需要1min的执行时间\n\n#### Reducer\n\n- Reducer将相同key的中间结果集进行处理\n- reduce任务的个数是通过[Job.setNumReduceTasks(int)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)来设置的\n- 通过 [Job.setReducerClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)来设置Reducer的实现类.框架对每组<key, (list of values)>的输入进行调用[reduce(WritableComparable, Iterable, Context)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Reducer.html) 方法进行处理,需要清理资源可以覆写cleanup(Context)\n\n##### Shuffle\n\n- 传到Reducer的输入是经过排序后的mapper的输出.shuffle阶段,框架将通过http获取相关partition的mapper输出\n\n##### Sort\n\n- 排序阶段,框架将Reducer的输入进行按Key进行分组\n- shuffle和sort同时进行.在map输出被拉取时,他们进行合并\n\n##### Secondary Sort\n\n- 如果中间结果key的分组规则需要和进入reducer前的keys的分组规则不一样,那么可以通过[Job.setSortComparatorClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)来设置比较器.因为[Job.setSortComparatorClass(Class)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/Job.html)时用来控制中间结果的keys是怎么分组的,所以可以用这个来对值进行二次排序\n\n##### Reduce\n\n- reduce阶段,将对每一组<key, (list of values)>输入调用reduce(WritableComparable, Iterable\\<Writable>, Context)方法\n- reduce任务通过 Context.write(WritableComparable, Writable)将输出结果写入文件系统\n- 输出结果并不会进行排序\n\n##### How Many Reduces?\n\n- 比较合理的reduce任务的个数计算公式是:0.95(或1.75)×节点数(注意,不是每个节点的最大container数)\n- 0.95系数可以使得reduce任务在map任务的输出传输结束后同时开始运行\n- 1.75系数可以使得计算快的节点在一批reduce任务计算结束之后开始计算第二批 reduce任务,实现负载均衡\n- 增加reduce的数量虽然会增加负载，但是可以改善负载匀衡，降低任务失败带来的负面影响\n- 放缩系数要比整数略小是因为要给推测性任务和失败任务预留reduce位置\n\n##### Reducer NONE\n\n- 如果不需要reduce任务,将reduce任务个数设置为0是合法的\n- 这种情况下,map任务的输出会直接写入文件系统的指定输出路径[FileOutputFormat.setOutputPath(Job, Path)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html).在写入文件系统前,map的输出是进行排序的\n\n#### Partitioner\n\n- partitioner控制中间map输出的key的分区\n- 可以按照key(或者key的一部分)来产生分区,默认是使用hash进行分区\n- 分区数和reduce任务的个数相等\n- 控制发送给reduce的任务个数\n\n#### Counter\n\n- Counter是一个公共基础工具,用来报告MapReduce应用的统计信息\n- Mapper和Reducer实现类都可以使用Counter来报告统计\n\n### Job Configuration\n\n- Job就是MapReduce任务的job配置代表\n- 一般MapReduce框架会严格按照Job的配置执行,但是有几种情况例外\n  - 某些配置参数被标记为final类型,所以是修改配置是没法达到目的的,例如1.1比例\n  - 某些配置虽然可以直接配置,但是还需要配合其他的参数一起配置才能生效\n- Job通常会指定Mapper,combiner(有必要的话),Partitioner,Reducer,InputFormat,OutputFormat的实现类\n- 输入可以使用下列方式指定输入数据文件集\n  - ([FileInputFormat.setInputPaths(Job, Path…)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html)/ [FileInputFormat.addInputPath(Job, Path)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html))\n  - ([FileInputFormat.setInputPaths(Job, String…)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html)/ [FileInputFormat.addInputPaths(Job, String)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html)\n\n- 输出可以使用([FileOutputFormat.setOutputPath(Path)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html))来指定输出文件集\n- 其他配置都是可选的,如Caparator的使用,将文件放置到DistributeCache,是否中间结果或者最终输出结果需要压缩,是否允许推测模式,最大任务重试次数等\n- 可以通过[Configuration.set(String, String)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/conf/Configuration.html)/ [Configuration.get(String)](https://hadoop.apache.org/docs/r2.8.5/api/org/apache/hadoop/conf/Configuration.html)来设置和获取任意需要的参数.但是对于大的只读数据集,还是要用DistributedCache\n\n### Task Execution & Environment\n\n- MRAppMaster在独立的JVM中执行每个Mapper/Reducer任务(任务进程级别)\n- 子任务继承了MRAppMaster的环境.\n- 用户可以通过 `mapreduce.{map|reduce}.java.opts` 给子任务添加额外的参数\n- 运行时非标准类库路径可以通过-Djava.library.path=<>指定\n- 如果mapreduce.{map|reduce}.java.opts参数配置包含了*@taskid@*则在运行时被替换成taskId\n- 显示JVM GC,JVM JMX无密代理(这样可以结合jconsole,查看内存,线程,线程垃圾回收),最大堆内存,添加其他路径到任务java.library.path的例子\n\n```xml\n<property>\n  <name>mapreduce.map.java.opts</name>\n  <value>\n  -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc\n  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false\n  </value>\n</property>\n\n<property>\n  <name>mapreduce.reduce.java.opts</name>\n  <value>\n  -Xmx1024M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc\n  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false\n  </value>\n</property>\n```\n\n#### Memory Management\n\n- 用户可以通过`mapreduce.{map|reduce}.memory.mb`指定子任务的最大虚拟内存.注意这个设置是进程级别的\n- 注意这个参数不要大于-Xmx的参数,否则VM可能会无法启动\n- `mapreduce.{map|reduce}.java.opt`只能配置MRAppMaster的子任务.配置守护线程的需要参考 [Configuring the Environment of the Hadoop Daemons](https://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-common/ClusterSetup.html#Configuring_Environment_of_Hadoop_Daemons)\n\n- map/reduce任务的性能,可能会被并发数,写入磁盘的频率影响.检查文件系统的统计报告,尤其是从map进入reduce的字节数,这参数是非常宝贵的.\n\n#### Map Parameters\n\n- map输出的记录会被序列化到缓冲区,元数据存储在统计缓冲区\n- 当缓冲区或者元数据超过一定的阈值,缓冲区的内容会被排序然后存储和写入磁盘\n- 如果缓冲区一直是满状态的,map线程将被阻塞\n- map结束后,没有写入磁盘的map输出记录继续写入.\n- 磁盘上所有的map输出文件段会合并成单个文件\n- 减少写入磁盘的次数,可以减少map的次数,但是加大缓存区会压缩mapper的可用内存\n\n| Name                             | Type  | Description                                            |\n| :------------------------------- | :---- | :----------------------------------------------------- |\n| mapreduce.task.io.sort.mb        | int   | 序列化和map输出到缓冲区的记录预排序的累计大小,单位为MB |\n| mapreduce.map.sort.spill.percent | float | 序列化缓冲区spill阈值比例,超过会将缓冲区内容写入磁盘   |\n\n- spill之后,如果在写入磁盘过程中,map的输出没有超过spill阈值,则会继续收集到spill结束\n- 如果是spill设置为0.33,在spill到磁盘的过程,缓冲区继续会被map的输出填充,下一次spill的时候再将这期间填充的内容写到磁盘\n- 如果spill设置为0.66,则不会触发下一次spill.也就是说,spill可以触发,但是不会阻塞\n- 一条记录大于缓冲区的会先触发spill,而且会被spill到一个单独的文件.无论这条记录有没有定义combiner,它都会被combiner传输\n\n#### Shuffle/Reduce Parameters\n\n- reduce将partitioner通过http指派给自己的map输出加载到内存,并定期合并输出到磁盘.\n- 如果中间结果是压缩输出,那么输出也是被reduce压缩的读进内存中,减少了内存的压力\n\n| Name                                          | Type  | Description                                                  |\n| :-------------------------------------------- | :---- | :----------------------------------------------------------- |\n| mapreduce.task.io.soft.factor                 | int   | 每次合并磁盘上段的数目.如果超过这个设置会分多次进行合并      |\n| mapreduce.reduce.merge.inmem.thresholds       | int   | 在合并写入磁盘之前,将排序后的map输出加载到内存的map输出数目.这个值通常设置很大(1000)或者直接禁用(0),因为内存合并要比磁盘合并的代价小得多.这个阈值只影响shuffle期间内存中合并的频率 |\n| mapreduce.reduce.shuffle.merge.percent        | float | 在内存合并之前,读取map输出的内存阈值,代表着用于存储map输出在内存中的百分比.因为map的输出并不适合存储在内存,所以设置很高会知道使得获取和合并的并行度下降.相反,设置为1可以使得内存运行的reduce更快.这个参数只影响shuffle期间的内存内合并频率 |\n| mapreduce.reduce.shuffle.input.buffer.percent | float | 在shuffle期间,可以分配来存储map输出的内存百分比,相对于`mapreduce.reduce.java.opts`指定的最大堆内存.把这个值设的大一点可以存储更多的map输出,但是也应该为框架预留一些内存 |\n| mapreduce.reduce.input.buffer.percent         | float | 相当于reduce阶段,用于存储map输出的最大堆内存的内存百分比.reduce开始的时候,map的输出被合并到磁盘,知道map输出在一定的阈值之内.默认情况下,在reduce开始之前,map的输出都会被合并到磁盘,这样才能使得reduce充分的利用到内存.对于只要内存密集型的reduce任务,应该增加这个值,减少磁盘的的往返时间 |\n\n#### Configured Parameters\n\n这些参数都是局部的,每个任务的\n\n| Name                       | Type    | Description                                    |\n| :------------------------- | :------ | :--------------------------------------------- |\n| mapreduce.job.id           | String  | The job id                                     |\n| mapreduce.job.jar          | String  | job.jar location in job directory              |\n| mapreduce.job.local.dir    | String  | The job specific shared scratch space          |\n| mapreduce.task.id          | String  | The task id                                    |\n| mapreduce.task.attempt.id  | String  | The task attempt id                            |\n| mapreduce.task.is.map      | boolean | Is this a map task                             |\n| mapreduce.task.partition   | int     | The id of the task within the job              |\n| mapreduce.map.input.file   | String  | The filename that the map is reading from      |\n| mapreduce.map.input.start  | long    | The offset of the start of the map input split |\n| mapreduce.map.input.length | long    | The number of bytes in the map input split     |\n| mapreduce.task.output.dir  | String  | The task’s temporary output directory          |\n\n在流任务执行过程中,这些参数会被转化.点(.)会被转成下划线(_),所以要想在流任务的mapper/reducer中获得这些值,需要使用下划线形式.\n\n#### Distributing Libraries\n\n- [DistributedCache](https://hadoop.apache.org/docs/r2.8.5/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#DistributedCache)分布式缓存可以分发jars和本地类库给map/reduce任务使用.\n- child-jvm总将自己的工作目录添加到java.library.path和LD_LIBRARY_PATH\n- 缓存中的类库可以通过System.loadLibrary或者System.load\n\n### Job Submission and Monitoring\n\n- Job是用户任务和ResourceManager交互的主要接口\n- Job的提交流程包括\n  - 检查输入输出路径\n  - 计算任务的InputSplit\n  - 有必要的话,设置必要的分布式缓存\n  - 拷贝任务的jar和配置到MapReduce系统目录\n  - 提交任务到ResourceManager.监控任务状态是可选的\n  - 任务的执行记录历史存放在 `mapreduce.jobhistory.intermediate-done-dir` 和`mapreduce.jobhistory.done-dir`\n\n#### Job Control\n\n- 对于单个MapReduce任务无法完成的任务,用户可能需要执行MapReduce任务链,才能完成.这还是非常容易的,因为任务的输出一般是存储在分布式文件系统中,所以一个任务的输出可以作为另一个任务的输入.这也就使得判断任务是否完成,不管成功或者失败,都需要用户来控制.主要有两种控制手段\n  - Job.submit() 提交任务到集群中,立即返回\n  - Job.waitForCompletion(boolean) 提交任务到集群中,等待其完成\n\n### Job Input\n\n- InputFormat描述了MapReduce任务的输入规范\n- InputFormat的职责是:\n  - 校验输入是否合法\n  - 将输入逻辑切分成InputSplit实例,之后将它们发送到独立的Mapper\n  - RecordReader 实现了从符合框架逻辑的InputSplit实例收集输入的记录,提供给Mapper进行处理\n\n- 默认的InputFormat是基于输入文件的总字节大小,将输入文件切分成逻辑的InputSplit实例,例如FileInputFormat的子类.然而,文件系统的blocksize只是split的上限,下限需要通过`mapreduce.input.fileinputformat.split.minsize`来设置\n- 压缩文件并不一定可以被切分,如.gz文件会把完整的文件交给一个mapper来处理\n\n#### InputSplit\n\n- InputSplit代表了一个独立Mapper处理的输入数据\n- 通常InputSplit是面向字节的,把面向字节转为面向记录是RecordReader的职责\n\n- FileSplit是默认的InputSplit实现,它把输入设置成mapreduce.map.input.file 属性,用于进行逻辑分割\n\n#### RecordReader\n\n- RecordReader负责将InputSplit的面向字节的输入转换成面向记录,提供给Mapper实现去处理每一条记录.因此RecordReader承担了从记录中提取出键值对的任务\n\n### Job Output\n\n- OutputFormat描述了MapReduce输出的规范\n- OutputFormat的职责:\n  - 校验任务的输出,例如输出目录是否存在\n  - RecordWriter实现可以将任务的输出写入到文件,存储在文件系统中\n- TextOutputFormat是默认的OutputFormat实现\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Hadoop,MapReduce"],"categories":["Hadoop","MapReduce"]},{"title":"Hive的一些坑","url":"/2019/04/01/hive的一些坑/","content":"\n# Hive的一些坑\n\n1. specified datastore driver(\"com.mysql.jdbc.Driver\") was not found\n\n![image-20200126114217615](/images/image/image-20200126114217615.png)\n\n这个是因为驱动不对,下载了个新的就行了\n\n2. Unable to open a test connection to the given database. JDBC url = jdbc:mysql://hadoop001:3306/test?useSSL=true&serverTimezone=GMT%2B8, username = lrj. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception:\n\n![image-20200126114500594](/images/image/image-20200126114500594.png)\n\n这个需要把ssl禁用了,在jdbcUrl上指定useSSL=false\n\n3. MetaException(message:Version information not found in metastore. )\n\n![image-20200126114708126](/images/image/image-20200126114708126.png)\n\n这个需要将hive-site.xml中的hive.metastore.schema.verification设置为false\n\n4. Required table missing : \"`VERSION`\" in Catalog \"\" Schema \"\". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable \"datanucleus.autoCreateTables\"\n\n![image-20200126114930654](/images/image/image-20200126114930654.png)\n\n这个需要初始化一下schema,执行\n\nschematool -dbType mysql -initSchema\n\n----\n\n之后就可以启动metastore + hiveserver2服务\n\n```bash\nnohup hive --service  metastore > ~/metastore.log 2>&1 &\nnohup  hiveserver2  > ~/hiveserver2.log 2>&1 &\n```\n\n测试hiveserver2服务是否ok\n\n```bash\nbeeline\n```\n\n打印日志\n\n```\nwhich: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/mysql/bin:/home/lurongjiang/.local/bin:/home/lurongjiang/bin:/usr/software/hadoop-2.6.0-cdh5.16.2/bin:/usr/software/hadoop-2.6.0-cdh5.16.2/sbin:/usr/software/jdk1.8.0_231/bin:/usr/software/apache-maven-3.6.3/bin:/usr/software/scala-2.11.12/bin:/usr/software/hive-1.1.0-cdh5.16.2/bin)\nBeeline version 1.1.0-cdh5.16.2 by Apache Hive\n# 查看下数据库,此时发现没连接\nbeeline> show databases;\nNo current connection\n# 尝试连接数据库,只需要输入用户名就行,不需要密码\nbeeline> !connect jdbc:hive2://hadoop001:10000/default\nConnecting to jdbc:hive2://hadoop001:10000/default\nEnter username for jdbc:hive2://hadoop001:10000/default: lrj\nEnter password for jdbc:hive2://hadoop001:10000/default: \nError: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=lrj, access=EXECUTE, inode=\"/tmp\":lurongjiang:supergroup:drwx\n```\n\n5. java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.security.AccessControlException: Permission denied: user=lrj, access=EXECUTE, inode=\"/tmp\":lurongjiang:supergroup:drwx------\n\n![image-20200126115801228](/images/image/image-20200126115801228.png)\n\n 这个是没权限\n\nhadoop fs -chmod -R 777  /tmp\n\n再次启动就ok了.\n\n","tags":["Hive"],"categories":["Hive"]},{"title":"Hive UDF","url":"/2019/03/15/Hive UDF/","content":"\n# Hive UDF\n\nhive内置函数并不一定满足我们的业务要求,所以需要拓展,即用户自定义函数\n\n**UDF**\n\nUser Defined Function\n\n- UDF (one-to-one)\n- UDAF(many-to-one)\n- UDTF(one-to-many)\n\n## 创建UDF步骤\n\n- 添加依赖\n\n  ```xml\n      <dependency>\n        <groupId>org.apache.hive</groupId>\n        <artifactId>hive-exec</artifactId>\n        <version>${hive.cdh.version}</version>\n      </dependency>\n  ```\n\n  \n\n- 创建自定义类,继承UDF","tags":["hive","udf","user-defined-function"],"categories":["Hive"]},{"title":"Hadoop MapReduce编程核心","url":"/2019/03/12/Hadoop MapReduce编程核心/","content":"\n# Hadoop MapReduce编程核心\n\n## Partitioner 分区\n\n```java\n/** \n * Partitions the key space.\n * \n * <p><code>Partitioner</code> controls the partitioning of the keys of the \n * intermediate map-outputs. The key (or a subset of the key) is used to derive\n * the partition, typically by a hash function. The total number of partitions\n * is the same as the number of reduce tasks for the job. Hence this controls\n * which of the <code>m</code> reduce tasks the intermediate key (and hence the \n * record) is sent for reduction.</p>\n * partitioner是控制中间map阶段输出结果的key的分区.key通常被hash,分发到各个分区\n * 分区数一般和reduce job的个数相等,\n * @see Reducer\n */\n@InterfaceAudience.Public\n@InterfaceStability.Stable\npublic interface Partitioner<K2, V2> extends JobConfigurable {\n  \n  /** \n   * Get the paritition number for a given key (hence record) given the total \n   * number of partitions i.e. number of reduce-tasks for the job.\n   * \n   * <p>Typically a hash function on a all or a subset of the key.</p>\n   * 根据分区总数,例如reduce job个数,获取分区的编号.一般是对所有key或者key的一部分进行进行hash处理\n   * @param key the key to be paritioned.\n   * @param value the entry value.\n   * @param numPartitions the total number of partitions.\n   * @return the partition number for the <code>key</code>.\n   */\n  int getPartition(K2 key, V2 value, int numPartitions);\n}\n/**\n* hash分区的实现就是key取hashCode和reduce个数进行取模\n*/\npublic class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n\n  public void configure(JobConf job) {}\n\n  /** Use {@link Object#hashCode()} to partition. */\n  public int getPartition(K2 key, V2 value,\n                          int numReduceTasks) {\n    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n  }\n\n}\n```\n\n\n\n>  **需要注意的是**\n>\n> - 分区数一般和reduce job个数相等\n> - 如果分区数<reduce job个数,将导致输出有很多无用的空文件\n> - 如果分区数>reduce job个数,将导致有些map输出找不到hash路径,出现java.io.IOException: Illegal partition for xxx的异常\n\n## Combiner 局部汇总\n\nCombiner是hadoop对map阶段输出结果进行本地局部聚合,提高后面reduce的效率,避免大量数据进行网络传输.\n\n> **需要注意的是**\n>\n> - 并非所有的任务都适用于Combiner\n> - 求和等操作,局部聚合可以有效的提高后面reduce的效率\n> - 平均值等操作,这种并不适用,因为局部平均值和全局平均值还是有差异的","tags":["hadoop","MapReduce"],"categories":["Hadoop"]},{"title":"Hello World","url":"/2018/07/18/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","tags":["PlayStation","Games"],"categories":["TestNest","test1","nest1","nest2"]}]